{
  "hash": "98850fe9eb0c1b6c38661b94adc1e64c",
  "result": {
    "markdown": "---\ntitle: \"8: MCMC\"\nauthor: \"Derek Sollberger\"\ndate: \"2024-02-22\"\n# execute:\n#   cache: true\n# format:\n#   revealjs:\n#     scrollable: true\nformat:\n  html:\n    toc: true\nparams:\n  heavy_chunks: \"true\"\n  # heavy_chunks: \"false\"\n---\n\n\n\\newcommand{\\ds}{\\displaystyle}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"bayesplot\")\nlibrary(\"ggtext\")\nlibrary(\"rstan\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n# A Good Example\n\n:::: {.panel-tabset}\n\n## Define Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 1: DEFINE the model\nbb_model <- \"\n  data {\n    int<lower = 0, upper = 9> Y;\n  }\n  parameters {\n    real<lower = 0, upper = 1> pi;\n  }\n  model {\n    Y ~ binomial(9, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n```\n:::\n\n\n## Simulate Posterior\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart_time <- Sys.time()\n\n# STEP 2: SIMULATE the posterior\ngood_simulation <- stan(model_code = bb_model, data = list(Y = 4), \n                        chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 1:                0.029 seconds (Sampling)\nChain 1:                0.059 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 2:                0.03 seconds (Sampling)\nChain 2:                0.058 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 3:                0.029 seconds (Sampling)\nChain 3:                0.057 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 4:                0.029 seconds (Sampling)\nChain 4:                0.057 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nend_time <- Sys.time()\nprint(round(end_time- start_time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 37 secs\n```\n:::\n:::\n\n\n## Histogram\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(good_simulation, pars = \"pi\")\n```\n\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Density\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_dens(good_simulation, pars = \"pi\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  labs(title = \"MCMC: <span style='color:#619CFF'>simulation</span> versus <span style='color:#E77500'>theoretical</span>\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Trace\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(good_simulation, pars = \"pi\") + \n  labs(title = \"MCMC Trace\",\n         subtitle = \"Good Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n::::\n\n\n# A Bad Example\n\n:::: {.panel-tabset}\n\n## Define Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 1: DEFINE the model\nbb_model <- \"\n  data {\n    int<lower = 0, upper = 9> Y;\n  }\n  parameters {\n    real<lower = 0, upper = 1> pi;\n  }\n  model {\n    Y ~ binomial(9, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n```\n:::\n\n\n## Simulate Posterior\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart_time <- Sys.time()\n\n# STEP 2: SIMULATE the posterior\nbad_simulation <- stan(model_code = bb_model, data = list(Y = 4), \n                        chains = 4, iter = 50*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: WARNING: There aren't enough warmup iterations to fit the\nChain 1:          three stages of adaptation as currently configured.\nChain 1:          Reducing each adaptation stage to 15%/75%/10% of\nChain 1:          the given number of warmup iterations:\nChain 1:            init_buffer = 7\nChain 1:            adapt_window = 38\nChain 1:            term_buffer = 5\nChain 1: \nChain 1: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 1: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 1: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 1: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 1: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 1: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 1: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 1: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 1: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 1: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 1: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 1: Iteration: 100 / 100 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0 seconds (Warm-up)\nChain 1:                0 seconds (Sampling)\nChain 1:                0 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: WARNING: There aren't enough warmup iterations to fit the\nChain 2:          three stages of adaptation as currently configured.\nChain 2:          Reducing each adaptation stage to 15%/75%/10% of\nChain 2:          the given number of warmup iterations:\nChain 2:            init_buffer = 7\nChain 2:            adapt_window = 38\nChain 2:            term_buffer = 5\nChain 2: \nChain 2: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 2: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 2: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 2: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 2: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 2: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 2: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 2: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 2: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 2: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 2: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 2: Iteration: 100 / 100 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0 seconds (Warm-up)\nChain 2:                0 seconds (Sampling)\nChain 2:                0 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: WARNING: There aren't enough warmup iterations to fit the\nChain 3:          three stages of adaptation as currently configured.\nChain 3:          Reducing each adaptation stage to 15%/75%/10% of\nChain 3:          the given number of warmup iterations:\nChain 3:            init_buffer = 7\nChain 3:            adapt_window = 38\nChain 3:            term_buffer = 5\nChain 3: \nChain 3: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 3: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 3: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 3: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 3: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 3: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 3: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 3: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 3: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 3: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 3: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 3: Iteration: 100 / 100 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0 seconds (Warm-up)\nChain 3:                0 seconds (Sampling)\nChain 3:                0 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: WARNING: There aren't enough warmup iterations to fit the\nChain 4:          three stages of adaptation as currently configured.\nChain 4:          Reducing each adaptation stage to 15%/75%/10% of\nChain 4:          the given number of warmup iterations:\nChain 4:            init_buffer = 7\nChain 4:            adapt_window = 38\nChain 4:            term_buffer = 5\nChain 4: \nChain 4: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 4: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 4: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 4: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 4: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 4: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 4: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 4: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 4: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 4: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 4: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 4: Iteration: 100 / 100 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0 seconds (Warm-up)\nChain 4:                0 seconds (Sampling)\nChain 4:                0 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nend_time <- Sys.time()\nprint(round(end_time- start_time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 0 secs\n```\n:::\n:::\n\n\n## Histogram\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(bad_simulation, pars = \"pi\")\n```\n\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Density\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_dens(bad_simulation, pars = \"pi\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  labs(title = \"MCMC: <span style='color:#619CFF'>simulation</span> versus <span style='color:#E77500'>theoretical</span>\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Trace\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(bad_simulation, pars = \"pi\") + \n  labs(title = \"MCMC Trace\",\n         subtitle = \"Bad Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n::::\n\n\n# A Thin Example\n\n:::: {.panel-tabset}\n\n## Define Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 1: DEFINE the model\nbb_model <- \"\n  data {\n    int<lower = 0, upper = 9> Y;\n  }\n  parameters {\n    real<lower = 0, upper = 1> pi;\n  }\n  model {\n    Y ~ binomial(9, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n```\n:::\n\n\n## Simulate Posterior\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart_time <- Sys.time()\n\n# STEP 2: SIMULATE the posterior\nthin_simulation <- stan(model_code = bb_model, data = list(Y = 4), \n                        chains = 4, iter = 5000*2, seed = 84735,\n                        thin = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.023 seconds (Warm-up)\nChain 1:                0.024 seconds (Sampling)\nChain 1:                0.047 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.023 seconds (Warm-up)\nChain 2:                0.026 seconds (Sampling)\nChain 2:                0.049 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.023 seconds (Warm-up)\nChain 3:                0.025 seconds (Sampling)\nChain 3:                0.048 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.023 seconds (Warm-up)\nChain 4:                0.025 seconds (Sampling)\nChain 4:                0.048 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nend_time <- Sys.time()\nprint(round(end_time- start_time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 0 secs\n```\n:::\n:::\n\n\n## Histogram\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(thin_simulation, pars = \"pi\")\n```\n\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## Density\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_dens(thin_simulation, pars = \"pi\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  labs(title = \"MCMC: <span style='color:#619CFF'>simulation</span> versus <span style='color:#E77500'>theoretical</span>\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## Trace\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(thin_simulation, pars = \"pi\") + \n  labs(title = \"MCMC Trace\",\n         subtitle = \"Thin Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Commentary\n\nThe notion of \"thinning\" an MCMC chain (or other stochastic process) might have been desirable years ago with less computer power, but nearly all of the current textbooks and software documentation advise against thinning the chains.\n\n::::\n\n\n# Metrics\n\nHow do we know if an MCMC is reliable?\n\n## Autocorrelation\n\n:::: {.panel-tabset}\n\n## Markov Property\n\nIf we are assuming the Markov property\n\n$$f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(i)}, y \\right) = f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(i)}, y \\right)$$\n\nthen autocorrelation measurements should only be \"large\" with a lag of one.\n\n## Plots\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](08_mcmc_2_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- bayesplot::mcmc_acf(good_simulation, pars = \"pi\") +\n  labs(title = \"Good Simulation\",\n       subtitle = \"10000 iterations\",\n       caption = \"SML 320\")\n\np2 <- bayesplot::mcmc_acf(bad_simulation, pars = \"pi\") +\n  labs(title = \"Bad Simulation\",\n       subtitle = \"100 iterations\",\n       caption = \"SML 320\")\n\np3 <- bayesplot::mcmc_acf(thin_simulation, pars = \"pi\") +\n  labs(title = \"Thinned Simulation\",\n       subtitle = \"Retained every 10 values\",\n       caption = \"SML 320\")\n\n# patchwork\np1 + p2 + p3\n```\n:::\n\n\n::::\n\n\n## Split R Metric\n\n:::: {.panel-tabset}\n\n## Analysis of Variance\n\n![Bayes Rules! Figure 6.19](rhat-ch6-1.png)\n\n## Definition\n\n::: {.callout-note collapse=\"true\"}\n## R-Hat\n\nConsider a Markov chain simulation of parameter $\\theta$ which utilizes four parallel chains. Let $\\text{Var}_{\\text{combined}}$ denote the variability in $\\theta$ across all four chains combined and $\\text{Var}_{\\text{within}}$ denote the typical variability within any individual chain. The R-hat metric calculates the ratio between these two sources of variability:\n\n$$\\text{R-hat} \\approx \\sqrt{\\ds\\frac{\\text{Var}_{\\text{combined}}}{\\text{Var}_{\\text{within}}}}$$\n\n:::\n\n## Guidance\n\n* Ideally, $\\text{R-hat}\\approx 1$, reflecting stability across the parallel chains. \n* In contrast, $\\text{R-hat} > 1$ indicates instability, with the variability in the combined chains exceeding that within the chains. \n* Though no golden rule exists, an R-hat ratio greater than 1.05 raises some red flags about the stability of the simulation.\n\n\n## Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::rhat(good_simulation, pars = \"pi\") |> round(digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.0003\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::rhat(bad_simulation, pars = \"pi\") |> round(digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.052\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::rhat(thin_simulation, pars = \"pi\") |> round(digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9983\n```\n:::\n:::\n\n\n::::\n\n\n## Effective Sample Size\n\n:::: {.panel-tabset}\n\n## Motivation\n\nLoosely speaking, how many independent sample values would it take to produce an equivalently accurate posterior approximation?\n\n## Description\n\n::: {.callout-note collapse=\"true\"}\n## Effective Sample Size Ratio\n\nLet $N$ denote the actual sample size or length of a dependent Markov chain. The effective sample size of this chain, $N_{\\text{eff}$, quantifies the number of independent samples it would take to produce an equivalently accurate posterior approximation. The greater the $N_{\\text{eff}$ the better, yet it’s typically true that the accuracy of a Markov chain approximation is only as good as that of a smaller independent sample. That is, it’s typically true that $N_{\\text{eff} < N$, thus the effective sample size ratio is less than 1:\n$$\\ds\\frac{N_{\\text{eff}}{N} < 1 \\quad\\text{(usually)}$$\n:::\n\n## Guidance\n\nThere’s no magic rule for interpreting this ratio, and it should be utilized alongside other diagnostics such as the trace plot. That said, we might be suspicious of a Markov chain for which the effective sample size ratio is less than 0.1, i.e., the effective sample size $N_{\\text{eff}$ is less than 10% of the actual sample size N.\n\n## Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::neff_ratio(good_simulation, pars = \"pi\") |> round(digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3873\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::neff_ratio(bad_simulation, pars = \"pi\") |> round(digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3434\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::neff_ratio(thin_simulation, pars = \"pi\") |> round(digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.0543\n```\n:::\n:::\n\n\n::::\n\n# MCMC Optimized\n\n## Metropolis Algorithm\n\nIf we have a *symmetric* proposal model, the probability of accepting a move from a current location $\\mu$ to a proposed location $\\mu^{′}$ comes down to a comparison of their posterior plausibility: $f(\\mu^{'}|y)$ versus $f(\\mu|y)$. There are two possible scenarios here:\n\n* Scenario 1: $f(\\mu^{'}|y) \\geq f(\\mu|y)$.  When the posterior plausibility of $\\mu^{′}$ is at least as great as that of $\\mu$, $\\alpha=1$. Thus, we’ll *definitely* move there.\n* Scenario 2: $f(\\mu^{'}|y) < f(\\mu|y)$.  If the posterior plausibility of $\\mu^{′}$ is less than that of $\\mu$, then\n\n$$α=\\ds\\frac{f(\\mu^{′}|y)}{f(\\mu|y)}<1$$\n\nThus, we *might* move there.\n\n## Metropolis-Hastings Algorithm\n\nRemoving the assumption of a symmetric proposal model, we have to convey $q(\\mu^{'}|\\mu)$, the probability density function of the proposal model.\n\nConditioned on data $y$, let parameter $\\mu$ have posterior pdf\n$$f(\\mu|y)\\propto f(\\mu) \\cdot L(\\mu|y)$$ \nA Metropolis-Hastings Markov chain for $f(\\mu|y)$, $\\{\\mu^{(1)},\\mu^{(2)},...,\\mu^{(N)}\\}$, evolves as follows. Let $\\mu^{(i)}=\\mu$ be the chain’s location at iteration $i\\in\\{1,2,...,N−1\\}$ and identify the next location $\\mu^{(i+1)}$ through a two-step process:\n\n* Step 1: *Propose a new location.* Conditioned on the current location $\\mu$, draw a location $\\mu^{′}$ from a proposal model with pdf $q(\\mu^{′}|\\mu)$.\n* Step 2: *Decide whether or not to go there.*\n\n    * Calculate the **acceptance probability** (i.e., the probability of accepting the proposal $\\mu^{′}$):\n    $$\\alpha = \\text{min}\\left\\{1, \\ds\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y)}{f(\\mu) \\cdot L(\\mu|y)} \\cdot \\ds\\frac{q(\\mu^{′}|\\mu)}{q(\\mu|\\mu^{′})} \\right\\}$$\n    * Figuratively, flip a weighted coin. If it’s Heads, with probability $\\alpha$, go to the proposed location $\\mu^{′}$. If it’s Tails, with probability $1−\\alpha$, stay at $\\mu$:\n    $$\\mu^{(i+1)} = \\begin{cases}\n      \\mu^{'} & \\text{with probability } \\alpha \\\\\n      \\mu & \\text{with probability } 1-\\alpha \\\\\n    \\end{cases}$$\n\n## Motivation\n\nWhy generalize to a non-symmetric proposal model?\n\n* flexibility to estimate a variety of parameters, such as standard deviations (or other nonnegative values)\n* leads to more clever searches\n\n## Gibbs Sampling\n\n* adaptive algorithm (especially with conjugate pairs)\n* described in 1984 by Stuart Geman and Donald Geman\n* named after Josiah Willard Gibbs (for Gibbs' work in statistical physics)\n* good for conditional distributions and marginal distributions\n\n## Interlude: Mountains of Laos\n\n![Vang Pao](Vang_Pao.png)\n\n## Hamiltonian Monte Carlo\n\nToward simulating a posterior distribution, **Hamiltonian Monte Carlo** (HMC) uses the topology by seeking out the gradient of maximum ascent\n\n$$\\ds\\text{max}_{\\vec{h}} \\lim_{|h| \\to 0} \\ds\\frac{f(\\vec{x} + \\vec{h}) - f(\\vec{x})}{|h|}$$\n\n\"Path of least resistance\"\n\n\n# Activity\n\n## App\n\nTry out some MCMC simulations!\n\n* link: [MCMC Demo](https://chi-feng.github.io/mcmc-demo/app.html) by Chi Feng\n\n* algorithms: \n\n    * RandomWalkMH\n    * GibbsSampling\n    * HamiltonianMC\n    * EfficientNUTS\n    \n* Target distributions:\n\n    * standard\n    * banana\n    * donut\n    * multimodal\n\n## Tuning\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngood_simulation <- stan(model_code = bb_model, data = list(Y = 4), \n                        chains = 4, iter = 5000*2, seed = 84735)\n```\n:::\n\n\nWhere did the simulation parameters, such as half-width and step-size, go?\n\n\n# Footnotes\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2    forcats_1.0.0      stringr_1.5.0      dplyr_1.1.3       \n [5] purrr_1.0.2        readr_2.1.4        tidyr_1.3.0        tibble_3.2.1      \n [9] ggplot2_3.4.3      tidyverse_2.0.0    patchwork_1.1.2    rstan_2.32.5      \n[13] StanHeaders_2.32.5 ggtext_0.1.2       bayesplot_1.10.0  \n\nloaded via a namespace (and not attached):\n [1] tensorA_0.36.2       gtable_0.3.4         xfun_0.40           \n [4] QuickJSR_1.1.3       htmlwidgets_1.6.2    processx_3.8.1      \n [7] inline_0.3.19        callr_3.7.3          tzdb_0.4.0          \n[10] vctrs_0.6.3          tools_4.3.0          ps_1.7.5            \n[13] generics_0.1.3       stats4_4.3.0         curl_5.0.2          \n[16] parallel_4.3.0       fansi_1.0.4          pkgconfig_2.0.3     \n[19] checkmate_2.2.0      distributional_0.3.2 RcppParallel_5.1.7  \n[22] lifecycle_1.0.4      farver_2.1.1         compiler_4.3.0      \n[25] munsell_0.5.0        codetools_0.2-19     htmltools_0.5.6     \n[28] yaml_2.3.7           pillar_1.9.0         crayon_1.5.2        \n[31] abind_1.4-5          posterior_1.4.1      commonmark_1.9.0    \n[34] tidyselect_1.2.0     digest_0.6.33        stringi_1.7.12      \n[37] reshape2_1.4.4       labeling_0.4.3       fastmap_1.1.1       \n[40] grid_4.3.0           colorspace_2.1-0     cli_3.6.1           \n[43] magrittr_2.0.3       loo_2.6.0            pkgbuild_1.4.0      \n[46] utf8_1.2.3           withr_2.5.2          backports_1.4.1     \n[49] prettyunits_1.1.1    scales_1.2.1         timechange_0.2.0    \n[52] rmarkdown_2.24       matrixStats_1.0.0    gridExtra_2.3       \n[55] hms_1.1.3            evaluate_0.21        knitr_1.43          \n[58] V8_4.3.0             markdown_1.8         rlang_1.1.1         \n[61] gridtext_0.1.5       Rcpp_1.0.11          glue_1.6.2          \n[64] xml2_1.3.5           rstudioapi_0.15.0    jsonlite_1.8.7      \n[67] plyr_1.8.8           R6_2.5.1            \n```\n:::\n:::\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n:::\n\n::::\n\n:::: {.panel-tabset}\n\n\n\n::::",
    "supporting": [
      "08_mcmc_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}