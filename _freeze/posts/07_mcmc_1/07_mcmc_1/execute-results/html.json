{
  "hash": "3bb293cd66053b22f9c1582cff8eaca2",
  "result": {
    "markdown": "---\ntitle: \"7: MCMC\"\nauthor: \"Derek Sollberger\"\ndate: \"2024-02-20\"\n# execute:\n#   cache: true\n# format:\n#   revealjs:\n#     scrollable: true\nformat:\n  html:\n    toc: true\nparams:\n  heavy_chunks: \"true\"\n  # heavy_chunks: \"false\"\n---\n\n\n\\newcommand{\\ds}{\\displaystyle}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"bayesplot\")\nlibrary(\"ggtext\")\nlibrary(\"rstan\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# tips_df <- readr::read_csv(\"tips.csv\")\n```\n:::\n\n\n# A Good Example\n\n:::: {.panel-tabset}\n\n## Define Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 1: DEFINE the model\nbb_model <- \"\n  data {\n    int<lower = 0, upper = 9> Y;\n  }\n  parameters {\n    real<lower = 0, upper = 1> pi;\n  }\n  model {\n    Y ~ binomial(9, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n```\n:::\n\n\n## Simulate Posterior\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart_time <- Sys.time()\n\n# STEP 2: SIMULATE the posterior\ngood_simulation <- stan(model_code = bb_model, data = list(Y = 4), \n                        chains = 4, iter = 5000*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 1:                0.029 seconds (Sampling)\nChain 1:                0.057 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 2:                0.033 seconds (Sampling)\nChain 2:                0.062 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 3:                0.031 seconds (Sampling)\nChain 3:                0.06 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 4:                0.031 seconds (Sampling)\nChain 4:                0.06 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nend_time <- Sys.time()\nprint(round(end_time- start_time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 39 secs\n```\n:::\n:::\n\n\n## Histogram\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(good_simulation, pars = \"pi\")\n```\n\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Density\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_dens(good_simulation, pars = \"pi\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  labs(title = \"MCMC: <span style='color:#619CFF'>simulation</span> versus <span style='color:#E77500'>theoretical</span>\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Trace\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(good_simulation, pars = \"pi\") + \n  labs(title = \"MCMC Trace\",\n         subtitle = \"Good Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n::::\n\n\n# A Bad Example\n\n:::: {.panel-tabset}\n\n## Define Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# STEP 1: DEFINE the model\nbb_model <- \"\n  data {\n    int<lower = 0, upper = 9> Y;\n  }\n  parameters {\n    real<lower = 0, upper = 1> pi;\n  }\n  model {\n    Y ~ binomial(9, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n```\n:::\n\n\n## Simulate Posterior\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart_time <- Sys.time()\n\n# STEP 2: SIMULATE the posterior\nbad_simulation <- stan(model_code = bb_model, data = list(Y = 4), \n                        chains = 4, iter = 50*2, seed = 84735)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: WARNING: There aren't enough warmup iterations to fit the\nChain 1:          three stages of adaptation as currently configured.\nChain 1:          Reducing each adaptation stage to 15%/75%/10% of\nChain 1:          the given number of warmup iterations:\nChain 1:            init_buffer = 7\nChain 1:            adapt_window = 38\nChain 1:            term_buffer = 5\nChain 1: \nChain 1: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 1: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 1: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 1: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 1: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 1: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 1: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 1: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 1: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 1: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 1: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 1: Iteration: 100 / 100 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0 seconds (Warm-up)\nChain 1:                0 seconds (Sampling)\nChain 1:                0 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: WARNING: There aren't enough warmup iterations to fit the\nChain 2:          three stages of adaptation as currently configured.\nChain 2:          Reducing each adaptation stage to 15%/75%/10% of\nChain 2:          the given number of warmup iterations:\nChain 2:            init_buffer = 7\nChain 2:            adapt_window = 38\nChain 2:            term_buffer = 5\nChain 2: \nChain 2: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 2: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 2: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 2: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 2: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 2: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 2: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 2: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 2: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 2: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 2: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 2: Iteration: 100 / 100 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0 seconds (Warm-up)\nChain 2:                0 seconds (Sampling)\nChain 2:                0 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: WARNING: There aren't enough warmup iterations to fit the\nChain 3:          three stages of adaptation as currently configured.\nChain 3:          Reducing each adaptation stage to 15%/75%/10% of\nChain 3:          the given number of warmup iterations:\nChain 3:            init_buffer = 7\nChain 3:            adapt_window = 38\nChain 3:            term_buffer = 5\nChain 3: \nChain 3: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 3: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 3: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 3: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 3: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 3: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 3: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 3: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 3: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 3: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 3: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 3: Iteration: 100 / 100 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0 seconds (Warm-up)\nChain 3:                0 seconds (Sampling)\nChain 3:                0 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: WARNING: There aren't enough warmup iterations to fit the\nChain 4:          three stages of adaptation as currently configured.\nChain 4:          Reducing each adaptation stage to 15%/75%/10% of\nChain 4:          the given number of warmup iterations:\nChain 4:            init_buffer = 7\nChain 4:            adapt_window = 38\nChain 4:            term_buffer = 5\nChain 4: \nChain 4: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 4: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 4: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 4: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 4: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 4: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 4: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 4: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 4: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 4: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 4: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 4: Iteration: 100 / 100 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0 seconds (Warm-up)\nChain 4:                0 seconds (Sampling)\nChain 4:                0 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nend_time <- Sys.time()\nprint(round(end_time- start_time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 0 secs\n```\n:::\n:::\n\n\n## Histogram\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(bad_simulation, pars = \"pi\")\n```\n\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Density\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_dens(bad_simulation, pars = \"pi\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  labs(title = \"MCMC: <span style='color:#619CFF'>simulation</span> versus <span style='color:#E77500'>theoretical</span>\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Trace\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(bad_simulation, pars = \"pi\") + \n  labs(title = \"MCMC Trace\",\n         subtitle = \"Bad Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n```\n\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n::::\n\n\n# Coins\n\n:::: {.panel-tabset}\n\n## Jakob Bernoulli\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n* German mathematician\n* 1654 - 1705\n* *Ars Conjectandi* published 1713\n* brother: Daniel Bernoulli\n:::\n\n::: {.column width=\"40%\"}\n![Jakob Bernoulli](Jakob_Bernoulli.png)\n\nImage source: [Mathematical Association of America](https://maa.org/press/periodicals/convergence/euler-and-the-bernoullis-learning-by-teaching-jakob-bernoulli)\n:::\n\n::::\n\n## Flipping Coins\n\n![coin flip](coin_flip.png)\n\nImage source: [Liberty Coin and Currency](https://libertycoinandcurrency.com/blog/the-history-of-the-coin-flip/)\n\n## One Coin\n\n$$P(H) = 0.50$$\n$$P(T) = 0.50$$\n\n![](onecoin.png)\n\n## Codes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 0:2\nf_k <- dbinom(k, 2, 0.5)\ntwo_coin_plot <- data.frame(k, f_k) |>\n  ggplot(aes(x = k, y = f_k)) +\n  geom_bar(color = \"black\", fill = \"#E77500\", stat = \"identity\") +\n  labs(title = \"2 Coins\", subtitle = \"fair coin\", caption = \"SML 320\", x = \"heads\", y = \"probability\") +\n  theme_minimal()\n\nk <- 0:3\nf_k <- dbinom(k, 3, 0.5)\nthree_coin_plot <- data.frame(k, f_k) |>\n  ggplot(aes(x = k, y = f_k)) +\n  geom_bar(color = \"black\", fill = \"#E77500\", stat = \"identity\") +\n  labs(title = \"3 Coins\", subtitle = \"fair coin\", caption = \"SML 320\", x = \"heads\", y = \"probability\") +\n  theme_minimal()\n\nk <- 0:4\nf_k <- dbinom(k, 4, 0.5)\nfour_coin_plot <- data.frame(k, f_k) |>\n  ggplot(aes(x = k, y = f_k)) +\n  geom_bar(color = \"black\", fill = \"#E77500\", stat = \"identity\") +\n  labs(title = \"4 Coins\", subtitle = \"fair coin\", caption = \"SML 320\", x = \"heads\", y = \"probability\") +\n  theme_minimal()\n\nk <- 0:5\nf_k <- dbinom(k, 5, 0.5)\nfive_coin_plot <- data.frame(k, f_k) |>\n  ggplot(aes(x = k, y = f_k)) +\n  geom_bar(color = \"black\", fill = \"#E77500\", stat = \"identity\") +\n  labs(title = \"5 Coins\", subtitle = \"fair coin\", caption = \"SML 320\", x = \"heads\", y = \"probability\") +\n  theme_minimal()\n```\n:::\n\n\n## 2 Coins\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## 3 Coins\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## 4 Coins\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n## 5 Coins\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Binomial Distribution\n\nLet random variable $Y$ be the number of successes in a fixed number of trials $n$. Assume that the trials are independent and that the probability of success in each trial is $\\pi$. Then the conditional dependence of $Y$ on $\\pi$ can be modeled by the Binomial model with parameters $n$ and $\\pi$. In mathematical notation:\n\n$$Y|\\pi \\sim \\text{Bin}(n,\\pi)$$\nwhere $\\sim$ can be read as \"modeled by\".  Correspondingly, the binomial model is specified by the conditional pmf\n\n$$f(y|\\pi) = \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y} \\text{ for } y \\in \\{0, 1, 2, ..., n\\}$$\nwhere $\\binom{n}{y} = \\ds\\frac{n!}{y!(n-y)!}$\n\n::::\n\n# Random Walks\n\n:::: {.panel-tabset}\n\n## Definition\n\nA **random walk** is a *random process* where we start at $x = 0$ on a one-dimensional number line, and then\n\n* go left: $f(X = -1) = 0.50$\n* go right: $f(X = 1) = 0.50$\n\n(and then repeat the \"step\" many times)\n\n## 1D Distribution\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n## 2 Dimensions\n\nA **random walk** is a *random process* where we start at $x = 0$ on a two-dimensional Cartesian plane, and then\n\n* go left: $f(X = -1, Y = 0) = 0.25$\n* go right: $f(X = 1, Y = 0) = 0.25$\n* go up: $f(X = 0, Y = 1) = 0.25$\n* go down: $f(X = 0, Y = -1) 0.25$\n\n(and then repeat the \"step\" many times)\n\n## 2D Traces\n\n![trajectories of 6 particles](2D_traces.png)\n\nImage Source: [Research Gate](https://www.researchgate.net/figure/Two-dimensional-random-walk-simulation-demonstrates-that-a-particle-can-reach-all-parts_fig4_327566938)\n\n## 3D Traces\n\n![tracjectories of 4 random walks](3D_traces.png)\n\nImage Source: [The Koga](http://www.thekoga.com/research-and-academics/random-walks/)\n\n::::\n\n\n# Going on Tour\n\n## Metropolis-Hastings\n\nUsing the analogy from the *Bayes Rules!* textbook, \"As tour manager, you can automate the tour route using the **Metropolis-Hastings algorithm**. This algorithm iterates through a two-step process. Assuming the Markov chain is at location $\\mu(i)=\\mu$ at iteration or “tour stop” $i$, the next tour stop $\\mu(i+1)$ is selected as follows:\n\n* Step 1: Propose a random location, $\\mu^{′}$, for the next tour stop.\n* Step 2: Decide whether to \n\n    * to to the proposed location\n    $$\\mu(i+1)=\\mu^{′}$$\n    * or to stay at the current location for another iteration\n    $$\\mu(i+1)=\\mu$$\n\n## Monte Carlo\n\nIf we know the posterior distribution, this special case of the Metropolis-Hastings algorithm has a special name\n\n::: {.callout-note collapse=\"true\"}\n## Monte Carlo algorithm\n\nTo construct an *independent* Monte Carlo sample *directly* from posterior pdf $f(\\mu|y)$, \n$$\\{\\mu^{(1)},\\mu^{(2)},...,\\mu^{(N)}\\}$$ \n\nselect each tour stop $\\mu^{(i)}=\\mu$\n\nas follows:\n\n* Step 1: Propose a location. Draw a location μ from the posterior model with pdf $f(\\mu|y)$\n* Step 2: Go there.\n:::\n\n## Generalizing\n\n* we only need MCMC to approximate a Bayesian posterior when that posterior is too complicated to specify\n* if a posterior is too complicated to specify, it’s typically too complicated to directly sample or draw from as we did in our Monte Carlo tour above\n* Metropolis-Hastings relies on the fact that, even if we don’t know the posterior model, we do know that the posterior pdf is proportional to the product of the known prior pdf and likelihood function\n\n$$f(\\mu|y) \\propto f(\\mu) \\cdot L(\\mu|y)$$\n\n\n# Metropolis-Hastings Algorithm\n\n## Nicholas Metropolis\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n* PhD: University of Chicago\n* Recruited by Oppenheimer\n* Team included von Neumann and Ulam\n:::\n\n::: {.column width=\"50%\"}\n![Nicholas Metropolis](Nicholas_Metropolis.png)\n\nImage Source: [Nuclear Museum](https://ahf.nuclearmuseum.org/voices/oral-histories/nicholas-metropolis-interview/)\n:::\n\n::::\n\n:::: {.columns}\n\n\n## Wilfred Hastings\n\n::: {.column width=\"50%\"}\n* PhD: University of Toronto\n* Bell Labs (New Jersey)\n* generalized Metropolis algorithm and MCMC\n:::\n\n::: {.column width=\"50%\"}\n![Wilfred Hastings](Wilfred_Hastings.png)\n\nImage Source: [McCall Gardens](https://www.mccallgardens.com/obituaries/wilfred-keith-hastings)\n:::\n\n::::\n\n:::: {.columns}\n\n## Rosenbluths\n\n\n::: {.column width=\"50%\"}\n### Arianna Rosenbluth\n\n* PhD: Harvard University\n* qualified for Olympics (fencing)\n* wrote first MCMC algorithm\n\n### Marshall Rosenbluth\n\n* PhD: University of Chicago\n* WWII veteran (navy)\n:::\n\n\n::: {.column width=\"50%\"}\n![Marshall and Arianna Rosenbluth](Rosenbluths.png)\n\nImage Source: [Los Alamos National Laboratories](https://discover.lanl.gov/publications/the-vault/the-vault-2022/metropolis/)\n:::\n\n::::\n\n\n\n\n\n\n\n## Uniform proposal model\n\nLet $\\mu^{(i)} = \\mu$ denote the current tour location with $w$ being a *half-width* distance:\n\n* random draw: $\\mu^{'}|\\mu \\sim \\text{Unif}(\\mu - w, \\mu + w)$\n* pdf: $q(\\mu^{'}|\\mu) = \\ds\\frac{1}{2w} \\text{ for } \\mu^{'} \\in [(\\mu - w, \\mu + w]$\n\n## Rejected Ideas\n\n* *Never* accept the proposed location.\n* *Always* accept the proposed location.\n* Only accept the proposed location *if* its (unnormalized) posterior plausibility is greater than that of the current location.\n\n## Main Idea\n\n* Step 1: Propose a location, $\\mu^{'}$, for the next tour stop by taking a draw from a proposal model.\n* Step 2: Decide whether to go to the proposed location ($μ^{(i+1)}=\\mu^{'}$) or to stay at the current location for another iteration ($μ^{(i+1)}=\\mu$) as follows.\n\n    * If the (unnormalized) posterior plausibility of the proposed location $\\mu^{'}$ is greater than that of the current location $\\mu$, $$f(\\mu^{'})L(\\mu^{'}|y)>f(\\mu)L(\\mu|y)$$\n*definitely* go there.\n    * Otherwise, *maybe* go there.\n\n## Definition\n\n::: {.callout-note collapse=\"true\"}\n## Metropolis-Hastings algorithm\n\nConditioned on data $y$, let parameter $\\mu$ have posterior pdf\n$$f(\\mu|y)\\propto f(\\mu) \\cdot L(\\mu|y)$$ \nA Metropolis-Hastings Markov chain for $f(\\mu|y)$, $\\{\\mu^{(1)},\\mu^{(2)},...,\\mu^{(N)}\\}$, evolves as follows. Let $\\mu^{(i)}=\\mu$ be the chain’s location at iteration $i\\in\\{1,2,...,N−1\\}$ and identify the next location $\\mu^{(i+1)}$ through a two-step process:\n\n* Step 1: *Propose a new location.* Conditioned on the current location $\\mu$, draw a location $\\mu^{′}$ from a proposal model with pdf $q(\\mu^{′}|\\mu)$.\n* Step 2: *Decide whether or not to go there.*\n\n    * Calculate the **acceptance probability** (i.e., the probability of accepting the proposal $\\mu^{′}$):\n    $$\\alpha = \\text{min}\\left\\{1, \\ds\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y)}{f(\\mu) \\cdot L(\\mu|y)} \\cdot \\ds\\frac{q(\\mu^{′}|\\mu)}{q(\\mu|\\mu^{′})} \\right\\}$$\n    * Figuratively, flip a weighted coin. If it’s Heads, with probability $\\alpha$, go to the proposed location $\\mu^{′}$. If it’s Tails, with probability $1−\\alpha$, stay at $\\mu$:\n    $$\\mu^{(i+1)} = \\begin{cases}\n      \\mu^{'} & \\text{with probability } \\alpha \\\\\n      \\mu & \\text{with probability } 1-\\alpha \\\\\n    \\end{cases}$$\n:::\n\n## Symmetric Proposal\n\nWhat happens if we have a symmetric proposal model?  For instance, \n\n$$\\mu^{'}|\\mu \\sim \\text{Unif}(\\mu - w, \\mu + w)$$\n\nleads to the probability density function\n\n$$q(\\mu^{′}|\\mu) = q(\\mu|\\mu^{′}) = \\begin{cases}\n  \\frac{1}{2w}, & |\\mu - \\mu^{'}| < w \\\\\n  0, & \\text{otherwise} \\\\\n\\end{cases}$$\n\n## Metropolis Algorithm\n\n::: {.callout-note collapse=\"true\"}\n## Metropolis Algorithm\n\nThe Metropolis algorithm is a special case of the Metropolis-Hastings in which the proposal model is symmetric. That is, the chance of proposing a move to $\\mu^{′}$ from $\\mu$ is equal to that of proposing a move to $\\mu$ from $\\mu^{′}$: \n$$q(\\mu^{′}|\\mu) = q(\\mu|\\mu^{′})$$\n\nThe acceptance probability simplifies to\n$$\\alpha = \\text{min}\\left\\{1, \\ds\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y)}{f(\\mu) \\cdot L(\\mu|y)} \\right\\}$$\n:::\n\n## Bayesian Ratio\n\nBy dividing both the numerator and denominator by $f(y)$\n\n$$\\alpha = \\text{min}\\left\\{1, \\ds\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y) / f(y)}{f(\\mu)  \\cdot L(\\mu|y) / f(y)} \\right\\} = \\text{min}\\left\\{1, \\ds\\frac{f(\\mu^{'}|y)}{f(\\mu|y)} \\right\\}$$\n\nThis rewrite emphasizes that, though we can’t calculate the posterior pdfs of $\\mu^{′}$ and $\\mu$, $f(\\mu^{'}|y)$ and $f(\\mu|y)$, their ratio is equivalent to that of the *unnormalized posterior* pdfs (which we can calculate)\n\n\n## Tour Decisions\n\nThus, the probability of accepting a move from a current location $\\mu$ to a proposed location $\\mu^{′}$ comes down to a comparison of their posterior plausibility: $f(\\mu^{'}|y)$ versus $f(\\mu|y)$. There are two possible scenarios here:\n\n* Scenario 1: $f(\\mu^{'}|y) \\geq f(\\mu|y)$.  When the posterior plausibility of $\\mu^{′}$ is at least as great as that of $\\mu$, $\\alpha=1$. Thus, we’ll *definitely* move there.\n* Scenario 2: $f(\\mu^{'}|y) < f(\\mu|y)$.  If the posterior plausibility of $\\mu^{′}$ is less than that of $\\mu$, then\n\n$$α=\\ds\\frac{f(\\mu^{′}|y)}{f(\\mu|y)}<1$$\n\nThus, we *might* move there.\n\n::: {.callout-tip collapse=\"true\"}\n## Near\n\nFurther, $\\alpha$ approaches 1 as $f(\\mu^{'}|y)$ nears $f(\\mu|y)$. That is, the probability of accepting the proposal increases with the plausibility of $\\mu^{′}$ relative to $\\mu$.\n:::\n\n\n# Metropolis-Hastings Implementation\n\n## Detailed Account\n\n* start location: $\\mu^{(0)} = 3$\n* half-width: $w = 0.5$\n* normal-normal model:\n\n    * prior: $\\mu \\sim \\text{N}(3, 0.50^2)$\n    * likelihood: $Y|\\mu \\sim \\text{N}(\\mu, 0.75^2)$\n    \n* observed value: $y = 3.20$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20240220)\ncurrent  <- 3\nproposal <- runif(1, min = current - 1, max = current + 1)\nprint(paste0(\"The proposed value is: \", proposal))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The proposed value is: 2.8043764475733\"\n```\n:::\n\n```{.r .cell-code}\nproposal_plausibility <- dnorm(proposal, 3, 0.5) * dnorm(3.20, proposal, 0.75)\nprint(paste0(\"The proposed plausibility is: \", proposal_plausibility))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The proposed plausibility is: 0.342079515048734\"\n```\n:::\n\n```{.r .cell-code}\ncurrent_plausibility <- dnorm(current, 3, 0.5) * dnorm(3.20, current, 0.75)\nprint(paste0(\"The current plausibility is: \", current_plausibility))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The current plausibility is: 0.409588054724166\"\n```\n:::\n\n```{.r .cell-code}\nalpha <- min(1, proposal_plausibility / current_plausibility)\nprint(paste0(\"The acceptance probability is: \", alpha))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The acceptance probability is: 0.835179422600849\"\n```\n:::\n:::\n\n\n\n## Helper Function\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_mh_iteration <- function(current, w, obs_value, tau, sigma){\n  # Step 1: propose next location in chain\n  proposal <- runif(1, min = current - w, current + w)\n  \n  # Step 2: decide whether or not to go there\n  proposal_plausibility <- dnorm(proposal, 3, tau) * dnorm(obs_value, proposal, sigma)\n  current_plausibility <- dnorm(current, 3, tau) * dnorm(obs_value, current, sigma)\n  alpha <- min(1, proposal_plausibility / current_plausibility)\n  next_stop <- sample(c(proposal, current), \n                      size = 1, prob = c(alpha, 1 - alpha))\n  \n  # Return the results as a data frame\n  return(data.frame(proposal, alpha, next_stop))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\none_mh_iteration(current = 3, w = 0.5, \n                 obs_value = 3.20, tau = 0.75, sigma = 0.50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  proposal     alpha next_stop\n1 2.765509 0.7071998  2.765509\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\none_mh_iteration(current = 3, w = 0.5, \n                 obs_value = 3.20, tau = 0.75, sigma = 0.50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  proposal alpha next_stop\n1   3.0858     1    3.0858\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5)\none_mh_iteration(current = 3, w = 0.5, \n                 obs_value = 3.20, tau = 0.75, sigma = 0.50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  proposal     alpha next_stop\n1 2.700214 0.6068602         3\n```\n:::\n:::\n\n\n\n## For Loop\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmh_tour <- function(N, current, w, obs_value, tau, sigma){\n  # N: chain length\n  # initialize vector\n  mu <- rep(0, N)\n  \n  # simulate N Markov chain stops\n  for(i in 1:N){\n    # simulate one iteration\n    this_iteration <- one_mh_iteration(current, w, obs_value, tau, sigma)\n    \n    # record next location\n    mu[i] <- this_iteration$next_stop\n    \n    # update current location\n    current <- this_iteration$next_stop\n  }\n  \n  # return the chain locations\n  return(data.frame(iteration = c(1:N), mu))\n}\n```\n:::\n\n\n## Go on Tour!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nour_mh_tour <- mh_tour(N = 5000, current = 3, w = 1, \n                 obs_value = 3.20, tau = 0.50, sigma = 0.75)\n```\n:::\n\n\n\n# Metrics\n\nHow do we know if our MCMC chain is reliable?\n\n## Traces\n\n### Our Metropolis-Hastings Tour\n\nFrom our knowledge of the Normal-Normal model, \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesrules::summarize_normal_normal(\n  # from prior\n  mean = 3, sd = 0.50,\n  \n  # from observations\n  y_bar = 3.20, sigma = 0.75, n = 1\n) |>\n  mutate_if(is.numeric, round, digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      model   mean   mode    var    sd\n1     prior 3.0000 3.0000 0.2500 0.500\n2 posterior 3.0615 3.0615 0.1731 0.416\n```\n:::\n:::\n\n\n\nwe should have a $\\text{N}(3.0615, 0.4160^2)$ posterior distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot(our_mh_tour, aes(x = iteration, y = mu)) + \n  geom_line() +\n  labs(title = \"Our Metropolis-Hastings Tour\",\n       subtitle = \"Posterior: N(3.0615, 0.4160^2)\",\n       caption = \"SML 320\") +\n  theme_minimal()\n\np2 <- ggplot(our_mh_tour, aes(x = mu)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.1,\n                 color = \"black\", fill = \"gray50\") + \n  stat_function(fun = dnorm, args = list(3.0615, 0.4160), \n                color = \"#E77500\",\n                linewidth = 2) +\n  theme_minimal()\n\n# patchwork\np1 + p2\n```\n\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n## Bad Examples\n\n![bad examples](bad-trace-1.png)\n\nImage Source: [Bayes Rules!](https://www.bayesrulesbook.com/chapter-6#diagnostics\n\n## Density Overlay\n\n:::: {.panel-tabset}\n\n## Plots\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07_mcmc_1_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- mcmc_dens_overlay(bad_simulation, pars = \"pi\") + \n  labs(title = \"Density Overlay\",\n       subtitle = \"bad simulation\",\n       caption = \"SML 320\",\n       y = \"density\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np2 <- mcmc_dens_overlay(good_simulation, pars = \"pi\") + \n  labs(title = \"Density Overlay\",\n       subtitle = \"good simulation\",\n       caption = \"SML 320\",\n       y = \"density\") +\n  theme_minimal()\n\n# patchwork\np1 + p2\n```\n:::\n\n\n::::\n\n\n\n# Footnotes\n\n## Legacy\n\nThe Metropolis-Hastings algorithm appeared in a [top-ten list](https://www.andrew.cmu.edu/course/15-355/misc/Top%20Ten%20Algorithms.html) called \"\"the greatest influence on the development and practice of science and engineering in the 20th century\".\n\n## Music Recommendation\n\n[Should I Stay or Should I Go](https://www.youtube.com/watch?v=xMaE6toi4mk) by The Clash\n\n::: {.callout-note collapse=\"true\"}\n## Session Info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2    forcats_1.0.0      stringr_1.5.0      dplyr_1.1.3       \n [5] purrr_1.0.2        readr_2.1.4        tidyr_1.3.0        tibble_3.2.1      \n [9] ggplot2_3.4.3      tidyverse_2.0.0    patchwork_1.1.2    rstan_2.32.5      \n[13] StanHeaders_2.32.5 ggtext_0.1.2       bayesplot_1.10.0  \n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3        inline_0.3.19        rlang_1.1.1         \n  [4] magrittr_2.0.3       snakecase_0.11.0     matrixStats_1.0.0   \n  [7] e1071_1.7-13         compiler_4.3.0       loo_2.6.0           \n [10] callr_3.7.3          vctrs_0.6.3          reshape2_1.4.4      \n [13] pkgconfig_2.0.3      crayon_1.5.2         fastmap_1.1.1       \n [16] backports_1.4.1      ellipsis_0.3.2       labeling_0.4.3      \n [19] utf8_1.2.3           threejs_0.3.3        promises_1.2.1      \n [22] rmarkdown_2.24       markdown_1.8         tzdb_0.4.0          \n [25] nloptr_2.0.3         ps_1.7.5             xfun_0.40           \n [28] jsonlite_1.8.7       later_1.3.1          parallel_4.3.0      \n [31] prettyunits_1.1.1    R6_2.5.1             dygraphs_1.1.1.6    \n [34] stringi_1.7.12       boot_1.3-28.1        Rcpp_1.0.11         \n [37] knitr_1.43           zoo_1.8-12           base64enc_0.1-3     \n [40] splines_4.3.0        Matrix_1.5-4         igraph_1.4.3        \n [43] httpuv_1.6.11        timechange_0.2.0     tidyselect_1.2.0    \n [46] rstudioapi_0.15.0    abind_1.4-5          yaml_2.3.7          \n [49] miniUI_0.1.1.1       codetools_0.2-19     curl_5.0.2          \n [52] processx_3.8.1       pkgbuild_1.4.0       lattice_0.21-8      \n [55] plyr_1.8.8           shiny_1.7.5          withr_2.5.2         \n [58] groupdata2_2.0.2     posterior_1.4.1      evaluate_0.21       \n [61] survival_3.5-5       proxy_0.4-27         RcppParallel_5.1.7  \n [64] xts_0.13.1           xml2_1.3.5           pillar_1.9.0        \n [67] tensorA_0.36.2       DT_0.28              checkmate_2.2.0     \n [70] stats4_4.3.0         shinyjs_2.1.0        distributional_0.3.2\n [73] generics_0.1.3       hms_1.1.3            rstantools_2.3.1    \n [76] munsell_0.5.0        commonmark_1.9.0     scales_1.2.1        \n [79] minqa_1.2.5          gtools_3.9.4         xtable_1.8-4        \n [82] class_7.3-21         glue_1.6.2           janitor_2.2.0       \n [85] tools_4.3.0          shinystan_2.6.0      lme4_1.1-33         \n [88] colourpicker_1.2.0   bayesrules_0.0.2     grid_4.3.0          \n [91] crosstalk_1.2.0      QuickJSR_1.1.3       colorspace_2.1-0    \n [94] nlme_3.1-162         cli_3.6.1            fansi_1.0.4         \n [97] V8_4.3.0             gtable_0.3.4         digest_0.6.33       \n[100] htmlwidgets_1.6.2    farver_2.1.1         htmltools_0.5.6     \n[103] lifecycle_1.0.4      mime_0.12            rstanarm_2.21.4     \n[106] MASS_7.3-58.4        shinythemes_1.2.0    gridtext_0.1.5      \n```\n:::\n:::\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\t\n:::\n\n::: {.column width=\"50%\"}\n\n:::\n\n::::\n\n:::: {.panel-tabset}\n\n\n\n::::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}