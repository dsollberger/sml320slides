[
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html",
    "href": "posts/01_conditional_probability/01_conditional_probability.html",
    "title": "1: Conditional Probability",
    "section": "",
    "text": "Spring 2024\nTuTh, 11 AM to 1220 PM\nBendheim House 103\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides an introduction to Bayesian analysis—a powerful statistical framework for making inferences and modeling uncertainty in a wide range of applications. Students will explore the fundamental principles of Bayesian statistics, probability theory, Bayesian inference, and practical applications of Bayesian modeling. The course will cover both the theory and hands-on implementation using data science software and the R programming language."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SML 320: Bayesian Analysis",
    "section": "",
    "text": "1: Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n2: Bayes’ Rule\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html",
    "href": "posts/02_bayes_rule/02_bayes_rule.html",
    "title": "2: Bayes’ Rule",
    "section": "",
    "text": "In the previous section, we studied conditional probability \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] and we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is \\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\] This is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is \\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] Bayes’ Rule combines the ideas of conditioned probability and total probability as \\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#bayes-rule",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#bayes-rule",
    "title": "2: Bayes’ Rule",
    "section": "",
    "text": "In the previous section, we studied conditional probability \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] and we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is \\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\] This is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is \\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] Bayes’ Rule combines the ideas of conditioned probability and total probability as \\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#a-deep-dive",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#a-deep-dive",
    "title": "2: Bayes’ Rule",
    "section": "A Deep Dive",
    "text": "A Deep Dive\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T)\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#more-practice",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#more-practice",
    "title": "2: Bayes’ Rule",
    "section": "More Practice",
    "text": "More Practice\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T^{c})\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-spam-filtering",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-spam-filtering",
    "title": "2: Bayes’ Rule",
    "section": "Example: Spam Filtering",
    "text": "Example: Spam Filtering\nIn 2002, Paul Graham used Bayes’ Rule as part of his algorithms to greatly decrease false positive rates of unwanted e-mails (“spam”). Let \\(H^{c}\\) be the event that an e-mail is “spam”. Let \\(W\\) be the event that an e-mail contains a trigger word such as “watches”. Suppose that\n\nthe probability that an e-mail contains that word given that it is spam is 17%\nthe probability that an e-mail contains that word given that it is not spam is 9%\nthe probability that a randomly selected e-mail message is spam is 80%\n\nFind the probability that an e-mail message is spam, given that the trigger word appears."
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-quality-control",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-quality-control",
    "title": "2: Bayes’ Rule",
    "section": "Example: Quality Control",
    "text": "Example: Quality Control\nA manufacturing process produces integrated circuit chips. Over the long run the fraction of bad chips produced by the process is around 20%. Thoroughly testing a chip to determine whether it is good or bad is rather expensive, so a cheap test is tried. All good chips will pass the cheap test, but so will 10% of the bad chips. Given that a chip passes the test, what is the probability that the chip was defective?"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-dui-checkpoint",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-dui-checkpoint",
    "title": "2: Bayes’ Rule",
    "section": "Example: DUI Checkpoint",
    "text": "Example: DUI Checkpoint\nA breath analyzer, used by the police to test whether drivers exceed the legal limit set for the blood alcohol percentage while driving, is known to satisfy\n\\[P(A|B) = P(A^{c}|B^{c}) = x\\]\nwhere \\(A\\) is the event “breath analyzer indicates that legal limit is exceeded” and \\(B\\) “driver’s blood alcohol percentage exceeds legal limit.” On Saturday nights, about 4% of the drivers are known to exceed the limit.\n\nDescribe in words the meaning of \\(P(B|A)\\)\nDetermine \\(P(B|A)\\) if \\(x = 0.90\\)\nHow big should \\(x\\) be so that \\(P(B|A) \\geq 0.95\\)?"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-monty-hall-problem",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-monty-hall-problem",
    "title": "2: Bayes’ Rule",
    "section": "Example: Monty Hall Problem",
    "text": "Example: Monty Hall Problem\n\n\n\n\nMonty Hall asks you to choose one of three doors. One of the doors hides a prize and the other two doors have no prize. You state out loud which door you pick, but you don’t open it right away.\n“Monty opens one of the other two doors, and there is no prize behind it.\n“At this moment, there are two closed doors, one of which you picked. The prize is behind one of the closed doors, but you don’t know which one. Monty asks you, ‘Do you want to switch doors?’”\n\nswitch doors\ndo not switch doors"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#generalized-bayes-rule",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#generalized-bayes-rule",
    "title": "2: Bayes’ Rule",
    "section": "Generalized Bayes’ Rule",
    "text": "Generalized Bayes’ Rule\nIf we are conditioning \\(B\\) on an event \\(A\\), where the latter can be partitioned into several subsets,\n\\[A = \\{ A_{1}, A_{2}, ..., A_{j} \\}\\]\nthen the total probability is\n\\[P(B) = P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n})\\]\nand Bayes Rule for computing the probability of \\(A_{i}\\) given \\(B\\) becomes\n\\[P(A_{i}|B) = \\displaystyle\\frac{ P(B|A_{i}) \\cdot P(A_{i}) }{ P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n}) }\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#bayesian-odds",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#bayesian-odds",
    "title": "2: Bayes’ Rule",
    "section": "Bayesian Odds",
    "text": "Bayesian Odds\n\n\n\n\n\n\nNote\n\n\n\nThe Bayesian odds of event \\(A\\) to event \\(B\\) given that event \\(C\\) has already taken place is \\[\\displaystyle\\frac{ P(A|C) }{ P(B|C) }\\]"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-analysis",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-analysis",
    "title": "1: Conditional Probability",
    "section": "",
    "text": "Spring 2024\nTuTh, 11 AM to 1220 PM\nBendheim House 103\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides an introduction to Bayesian analysis—a powerful statistical framework for making inferences and modeling uncertainty in a wide range of applications. Students will explore the fundamental principles of Bayesian statistics, probability theory, Bayesian inference, and practical applications of Bayesian modeling. The course will cover both the theory and hands-on implementation using data science software and the R programming language."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#lecturer",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#lecturer",
    "title": "1: Conditional Probability",
    "section": "Lecturer",
    "text": "Lecturer"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#current-research-in-pedagogy",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#current-research-in-pedagogy",
    "title": "1: Conditional Probability",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\nflipped classrooms"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#identity-statement",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#identity-statement",
    "title": "1: Conditional Probability",
    "section": "Identity Statement",
    "text": "Identity Statement\n\n\n\nOriginally from Los Angeles\nMath: easier to understand through graphs\nComputer Programming: years of experience with R, Python, MATLAB, PHP, HTML, etc.\nLearning: drawn to puzzles and manageable tasks\nPersonality: shy, introvert"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#textbook",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#textbook",
    "title": "1: Conditional Probability",
    "section": "Textbook",
    "text": "Textbook\n\n\nThis course will closely follow the Bayes Rules! textbook by Alicia A Johnson, Miles Q Ott, and Mine Dogucu. It is, in my opinion, the best blend of Bayesian thought, mathematical background, computer processes, and relevant applications. The authors have made the materials of their textbook available online at https://www.bayesrulesbook.com/\n\n\n\n\nBayes Rules"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#additional-reading",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#additional-reading",
    "title": "1: Conditional Probability",
    "section": "Additional Reading",
    "text": "Additional Reading\nThe following list of books is optional for student studies, but the instructor may use some materials to add depth and interest to the course.\n\n\n\n\n\n\nAdditional Reading\n\n\n\n\n\n\nStatistical Rethinking by Richard McElreath is the premier body of work in the field of Bayesian analysis. This resource is great for people who want to build a strong foundation in philosophy and theory in this branch of mathematics.\nBayesian Data Analysis by Andrew Gelman, et al., is the classic textbook (available online) in this field that is used in several university courses. The authors’ approach work well for people looking to quickly add Bayesian approaches to their research skills.\nBayesian Statistics the Fun Way by Will Kurt brings Bayesian notions to a broad audience and its presentation blends will with an introductory course in statistics.\nBayesian Thinking in Biostatistics by Gary L Rosner, et al., provides rigorous applications in bioinformatics along with strong software use."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#cooperative-classroom",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#cooperative-classroom",
    "title": "1: Conditional Probability",
    "section": "Cooperative Classroom",
    "text": "Cooperative Classroom\nLearning in a cooperative environment should be stimulating, demanding, and fair. Because this approach to learning is different from the competitive classroom structure that many other courses used to be based on, it is important for us to be clear about mutual expectations. Below are my expectations for students in this class. This set of expectations is intended to maximize debate and exchange of ideas in an atmosphere of mutual respect while preserving individual ownership of ideas and written words. If you feel you do not understand or cannot agree to these expectations, you should discuss this with your instructor and classmates.\n\nStudents are expected to work cooperatively with other members of the class and show respect for the ideas and contributions of other people.\nWhen working as part of a group, students should strive to be good contributors to the group, listen to others, not dominate, and recognize the contributions of others. Students should try to ensure that everyone in the group is welcome to contribute and recognize that everyone contributes in different ways to a group process.\nStudents should explore data, make observations, and develop inferences as part of a group. If you use material from published sources, you must provide appropriate attribution.\n\n\n\n(Students will be asked to acknowledge this document in an online form.)\nThis document has been adapted from Scientific Teaching by Jo Handelsman, Sarah Miller, and Christine Pfund\n\n\n\n\nScientific Teaching"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#pep-talk",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#pep-talk",
    "title": "1: Conditional Probability",
    "section": "Pep Talk",
    "text": "Pep Talk\nLearning R can be difficult at first—it is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you will be using like ggplot2—made this wise observation:\n\n\n\n\n\n\nWisdom from Hadley Wickham\n\n\n\n\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\n\n\nIf you are finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, ask questions … e-mail [Derek], etc. I promise you can do this.\n—Andrew Heiss, Georgia State University"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#inclusion-statement",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#inclusion-statement",
    "title": "1: Conditional Probability",
    "section": "Inclusion Statement",
    "text": "Inclusion Statement\nI value all students regardless of their background, country of origin, race, religion, ethnicity, gender, sexual orientation, disability status, etc. and am committed to providing a climate of excellence and inclusiveness within all aspects of the course. If there are aspects of your culture or identity that you would like to share with me as they relate to your success in this class, I am happy to meet to discuss. Likewise, if you have any concerns in this area or facing any special issues or challenges, you are encouraged to discuss the matter with me (set up a meeting by e-mail) with an assurance of full confidentiality (only exception being mandatory reporting of academic integrity code violations or sexual harassment)."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#setting",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#setting",
    "title": "1: Conditional Probability",
    "section": "Setting",
    "text": "Setting\n\n\nLet us visit the lands of Faerûn. To grossly simplify and introduce notions from Dungeons and Dragons, let us define the following random variables:\n\n\\(H\\): human\n\\(E\\): evil\n\nso that \\(H^{c}\\) is “non-human” and \\(E^c\\) is “not evil”.\n\n\n\n\nBaldur’s Gate 3\n\n\n\n\n\n\n\n\n\n\nUnicode Characters\n\n\n\n\n\nDerek wanted to make a note to himself here that the way to make accented letters in a markdown environment is to use unicode characters."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nContingency TableCode\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(E^{c}|H)\\) and \\(P(H^{c}|E)\\)\n\n\n\nalignment &lt;- c(\"evil\", \"not evil\")\nhuman &lt;- c(\"24\", \"30\")\nnon_human &lt;- c(\"88\", \"178\")\n\nbg3_df &lt;- data.frame(alignment, human, non_human)\n\nbg3_gt_table &lt;- bg3_df |&gt;\n  gt(rowname_col = \"alignment\") |&gt;\n  cols_align(align = \"right\", columns = alignment) |&gt;\n  cols_align(align = \"center\",  columns = c(human, non_human)) |&gt;\n  tab_style(locations = cells_body(columns = c(human, non_human)),\n            style = list(cell_fill(color = \"yellow\")))\n\nbg3_gt_table #display table"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#practice",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#practice",
    "title": "1: Conditional Probability",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(H|E)\\) and \\(P(E|H)\\). What do you observe about the results?"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#setting-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#setting-1",
    "title": "1: Conditional Probability",
    "section": "Setting",
    "text": "Setting\n\n\nDuring the Winter of 2024, Kaggle had a competition where programmers were asked to “create an energy prediction model of prosumers to reduce energy imbalance costs” based on data that included property information, historical weather, and forecasted weather.\nFor now, let us pretend to classify the results into “high energy” and “low energy” usage, where “positive” results correspond to the “high energy” prosumers.\n\n\nThis Kaggle competition was called “Enefit”, and it was created by Eesti Energia to model Estonian energy customers."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-1",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nConfusion MatrixCodeMetricsFormulas\n\n\nSuppose that a team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\ntrue positives: 59\ntrue negatives: 97\nfalse positives: 14\nfalse negatives: 28\n\n\n\n\nenergy_levels &lt;- c(\"high\", \"low\")\nhigh &lt;- c(59, 14)\nlow &lt;- c(28, 97)\n\nenefit_df &lt;- data.frame(energy_levels, high, low)\n\nenefit_gt &lt;- enefit_df |&gt;\n  gt(rowname_col = \"energy_levels\") |&gt;\n  cols_align(align = \"right\", columns = energy_levels) |&gt;\n  cols_align(align = \"center\",  columns = c(high, low)) |&gt;\n  tab_spanner(columns = c(high, low),\n              label = \"model predictions\") |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 1),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 2),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 2),\n            style = list(cell_fill(color = \"#FFB3B2\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 1),\n            style = list(cell_fill(color = \"#FFB3B2\")))\n\nenefit_gt #display table\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\n\n\\[\\text{accuracy } = \\frac{TP + TN}{TP + FN + FP + TN}\\] \\[\\text{sensitivity } = \\frac{TP}{TP + FN}\\] \\[\\text{specificity } = \\frac{TN}{FP + TN}\\] \\[\\text{F-score } = \\frac{2*TP}{2*TP + FN + FP}\\]\nSource: Wikipedia page on sensitivity and specificity\n\n\n\n\n\n\n\n\n\nRow Span\n\n\n\n\n\nWhen these lecture notes were written, there might not have been a function to have a label span multiple rows in the gt package. The left side of the confusion matrix should say “ground truth”"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#practice-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#practice-1",
    "title": "1: Conditional Probability",
    "section": "Practice",
    "text": "Practice\nSuppose that another team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n94\n80\n    low\n23\n939\n  \n  \n  \n\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\nProsecutor’s Fallacy\n\n\n\n\n\nFor events \\(A\\) and \\(B\\), the inverse conditional probabilities are almost never equal to each other\n\\[P(A|B) \\neq P(B|A)\\]\n\n\n\n\n\n\n\n\n\n(optional) Additional Resources\n\n\n\n\n\n\nClassical vs Frequentist vs Bayesian Probability by Trefor Bazett"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#classical-probability",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#classical-probability",
    "title": "1: Conditional Probability",
    "section": "Classical Probability",
    "text": "Classical Probability\n\nDefinitionExample\n\n\n\\[P(A) = \\frac{\\text{number of outcomes that are } A}{\\text{number of outcomes total}}\\]\n\n\nOverly simplistic example\nIt snowed during 2 of the 30 days of January in 2024. We can claim that it snows \\(\\frac{2}{30}\\) of the days in Princeton."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#frequentist-probablity",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#frequentist-probablity",
    "title": "1: Conditional Probability",
    "section": "Frequentist Probablity",
    "text": "Frequentist Probablity\n\nDefinitionExample\n\n\n\\[P(A) = \\lim_{n \\to \\infty} \\frac{\\text{number of outcomes that are } A}{\\text{number of outcomes total}}\\]\n\n\nOverly simplistic example\nIf we can model many months of weather in Princeton, we expect it to snow in about 3 percent of the days."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-probability",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-probability",
    "title": "1: Conditional Probability",
    "section": "Bayesian Probability",
    "text": "Bayesian Probability\n\nDefinitionExample\n\n\n\nprior belief: \\(P(A)\\)\nupdated belief: \\(P(A|B)\\)\n\n\n\nOverly simplistic example\nIt snowed during 2 of the 30 days of January in 2024. Could we update our probability calculations for snow?"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-3",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-3",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nContingency TableCode\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(E^{c}|H)\\) and \\(P(H^{c}|E)\\)\n\n\n\nalignment &lt;- c(\"evil\", \"not evil\")\nhuman &lt;- c(\"24\", \"30\")\nnon_human &lt;- c(\"88\", \"178\")\n\nbg3_df &lt;- data.frame(alignment, human, non_human)\n\nbg3_gt_table &lt;- bg3_df |&gt;\n  gt(rowname_col = \"alignment\") |&gt;\n  cols_align(align = \"right\", columns = alignment) |&gt;\n  cols_align(align = \"center\",  columns = c(human, non_human)) |&gt;\n  tab_style(locations = cells_body(columns = c(human, non_human)),\n            style = list(cell_fill(color = \"yellow\")))\n\nbg3_gt_table #display table"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-4",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-4",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nConfusion MatrixCodeMetricsFormulas\n\n\nSuppose that a team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\ntrue positives: 59\ntrue negatives: 97\nfalse positives: 14\nfalse negatives: 28\n\n\n\n\nenergy_levels &lt;- c(\"high\", \"low\")\nhigh &lt;- c(59, 14)\nlow &lt;- c(28, 97)\n\nenefit_df &lt;- data.frame(energy_levels, high, low)\n\nenefit_gt &lt;- enefit_df |&gt;\n  gt(rowname_col = \"energy_levels\") |&gt;\n  cols_align(align = \"right\", columns = energy_levels) |&gt;\n  cols_align(align = \"center\",  columns = c(high, low)) |&gt;\n  tab_spanner(columns = c(high, low),\n              label = \"model predictions\") |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 1),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 2),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 2),\n            style = list(cell_fill(color = \"#FFB3B2\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 1),\n            style = list(cell_fill(color = \"#FFB3B2\")))\n\nenefit_gt #display table\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\n\n\\[\\text{accuracy } = \\frac{TP + TN}{TP + FN + FP + TN}\\] \\[\\text{sensitivity } = \\frac{TP}{TP + FN}\\] \\[\\text{specificity } = \\frac{TN}{FP + TN}\\] \\[\\text{F-score } = \\frac{2*TP}{2*TP + FN + FP}\\]\nSource: Wikipedia page on sensitivity and specificity\n\n\n\n\n\n\n\n\n\nRow Span\n\n\n\n\n\nWhen these lecture notes were written, there might not have been a function to have a label span multiple rows in the gt package. The left side of the confusion matrix should say “ground truth”"
  }
]