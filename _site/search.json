[
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html",
    "href": "posts/01_conditional_probability/01_conditional_probability.html",
    "title": "1: Conditional Probability",
    "section": "",
    "text": "Goal: Motivate the Bayesian inversion of events\nObjective: Review conditional probability and metrics\n\n\n\n\n\nBayes Rules\n\n\n\n\n\n\n\n\nSpring 2024\nTuTh, 11 AM to 1220 PM\nBendheim House 103\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides an introduction to Bayesian analysis—a powerful statistical framework for making inferences and modeling uncertainty in a wide range of applications. Students will explore the fundamental principles of Bayesian statistics, probability theory, Bayesian inference, and practical applications of Bayesian modeling. The course will cover both the theory and hands-on implementation using data science software and the R programming language."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SML 320: Bayesian Analysis",
    "section": "",
    "text": "17: Hierarchical Models\n\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n16: Naive Bayes Classification\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n15: Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n14: Poisson Regression Models\n\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n13: Extending Regression Models\n\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n12: Evaluating Regression Models\n\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n11: Normal Regression\n\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n10: Posterior Prediction\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n9: Posterior Inference\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n8: MCMC\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n7: MCMC\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n6: Approximating the Posterior\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n5: Conjugate Families\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n4: Balance and Sequentiality\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n3: Beta-Binomial Models\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n2: Bayes’ Rule\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n1: Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html",
    "href": "posts/02_bayes_rule/02_bayes_rule.html",
    "title": "2: Bayes’ Rule",
    "section": "",
    "text": "In the previous section, we studied conditional probability \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] and we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is \\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\] This is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is \\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] Bayes’ Rule combines the ideas of conditioned probability and total probability as \\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#bayes-rule",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#bayes-rule",
    "title": "2: Bayes’ Rule",
    "section": "",
    "text": "In the previous section, we studied conditional probability \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] and we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is \\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\] This is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is \\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] Bayes’ Rule combines the ideas of conditioned probability and total probability as \\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#a-deep-dive",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#a-deep-dive",
    "title": "2: Bayes’ Rule",
    "section": "A Deep Dive",
    "text": "A Deep Dive\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T)\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#more-practice",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#more-practice",
    "title": "2: Bayes’ Rule",
    "section": "More Practice",
    "text": "More Practice\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T^{c})\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-spam-filtering",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-spam-filtering",
    "title": "2: Bayes’ Rule",
    "section": "Example: Spam Filtering",
    "text": "Example: Spam Filtering\nIn 2002, Paul Graham used Bayes’ Rule as part of his algorithms to greatly decrease false positive rates of unwanted e-mails (“spam”). Let \\(H^{c}\\) be the event that an e-mail is “spam”. Let \\(W\\) be the event that an e-mail contains a trigger word such as “watches”. Suppose that\n\nthe probability that an e-mail contains that word given that it is spam is 17%\nthe probability that an e-mail contains that word given that it is not spam is 9%\nthe probability that a randomly selected e-mail message is spam is 80%\n\nFind the probability that an e-mail message is spam, given that the trigger word appears."
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-quality-control",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-quality-control",
    "title": "2: Bayes’ Rule",
    "section": "Example: Quality Control",
    "text": "Example: Quality Control\nA manufacturing process produces integrated circuit chips. Over the long run the fraction of bad chips produced by the process is around 20%. Thoroughly testing a chip to determine whether it is good or bad is rather expensive, so a cheap test is tried. All good chips will pass the cheap test, but so will 10% of the bad chips. Given that a chip passes the test, what is the probability that the chip was defective?"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-dui-checkpoint",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-dui-checkpoint",
    "title": "2: Bayes’ Rule",
    "section": "Example: DUI Checkpoint",
    "text": "Example: DUI Checkpoint\nA breath analyzer, used by the police to test whether drivers exceed the legal limit set for the blood alcohol percentage while driving, is known to satisfy\n\\[P(A|B) = P(A^{c}|B^{c}) = x\\]\nwhere \\(A\\) is the event “breath analyzer indicates that legal limit is exceeded” and \\(B\\) “driver’s blood alcohol percentage exceeds legal limit.” On Saturday nights, about 4% of the drivers are known to exceed the limit.\n\nDescribe in words the meaning of \\(P(B|A)\\)\nDetermine \\(P(B|A)\\) if \\(x = 0.90\\)\nHow big should \\(x\\) be so that \\(P(B|A) \\geq 0.95\\)?"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-monty-hall-problem",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-monty-hall-problem",
    "title": "2: Bayes’ Rule",
    "section": "Example: Monty Hall Problem",
    "text": "Example: Monty Hall Problem\n\n\n\n\nMonty Hall asks you to choose one of three doors. One of the doors hides a prize and the other two doors have no prize. You state out loud which door you pick, but you don’t open it right away.\n“Monty opens one of the other two doors, and there is no prize behind it.\n“At this moment, there are two closed doors, one of which you picked. The prize is behind one of the closed doors, but you don’t know which one. Monty asks you, ‘Do you want to switch doors?’”\n\nswitch doors\ndo not switch doors"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#generalized-bayes-rule",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#generalized-bayes-rule",
    "title": "2: Bayes’ Rule",
    "section": "Generalized Bayes’ Rule",
    "text": "Generalized Bayes’ Rule\nIf we are conditioning \\(B\\) on an event \\(A\\), where the latter can be partitioned into several subsets,\n\\[A = \\{ A_{1}, A_{2}, ..., A_{j} \\}\\]\nthen the total probability is\n\\[P(B) = P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n})\\]\nand Bayes Rule for computing the probability of \\(A_{i}\\) given \\(B\\) becomes\n\\[P(A_{i}|B) = \\displaystyle\\frac{ P(B|A_{i}) \\cdot P(A_{i}) }{ P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n}) }\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#bayesian-odds",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#bayesian-odds",
    "title": "2: Bayes’ Rule",
    "section": "Bayesian Odds",
    "text": "Bayesian Odds\n\n\n\n\n\n\nNote\n\n\n\nThe Bayesian odds of event \\(A\\) to event \\(B\\) given that event \\(C\\) has already taken place is \\[\\displaystyle\\frac{ P(A|C) }{ P(B|C) }\\]"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-analysis",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-analysis",
    "title": "1: Conditional Probability",
    "section": "",
    "text": "Spring 2024\nTuTh, 11 AM to 1220 PM\nBendheim House 103\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides an introduction to Bayesian analysis—a powerful statistical framework for making inferences and modeling uncertainty in a wide range of applications. Students will explore the fundamental principles of Bayesian statistics, probability theory, Bayesian inference, and practical applications of Bayesian modeling. The course will cover both the theory and hands-on implementation using data science software and the R programming language."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#lecturer",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#lecturer",
    "title": "1: Conditional Probability",
    "section": "Lecturer",
    "text": "Lecturer"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#current-research-in-pedagogy",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#current-research-in-pedagogy",
    "title": "1: Conditional Probability",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\nflipped classrooms"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#identity-statement",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#identity-statement",
    "title": "1: Conditional Probability",
    "section": "Identity Statement",
    "text": "Identity Statement\n\n\n\nOriginally from Los Angeles\nMath: easier to understand through graphs\nComputer Programming: years of experience with R, Python, MATLAB, PHP, HTML, etc.\nLearning: drawn to puzzles and manageable tasks\nPersonality: shy, introvert"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#textbook",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#textbook",
    "title": "1: Conditional Probability",
    "section": "Textbook",
    "text": "Textbook\n\n\nThis course will closely follow the Bayes Rules! textbook by Alicia A Johnson, Miles Q Ott, and Mine Dogucu. It is, in my opinion, the best blend of Bayesian thought, mathematical background, computer processes, and relevant applications. The authors have made the materials of their textbook available online at https://www.bayesrulesbook.com/\n\n\n\n\nBayes Rules"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#additional-reading",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#additional-reading",
    "title": "1: Conditional Probability",
    "section": "Additional Reading",
    "text": "Additional Reading\nThe following list of books is optional for student studies, but the instructor may use some materials to add depth and interest to the course.\n\n\n\n\n\n\nAdditional Reading\n\n\n\n\n\n\nStatistical Rethinking by Richard McElreath is the premier body of work in the field of Bayesian analysis. This resource is great for people who want to build a strong foundation in philosophy and theory in this branch of mathematics.\nBayesian Data Analysis by Andrew Gelman, et al., is the classic textbook (available online) in this field that is used in several university courses. The authors’ approach work well for people looking to quickly add Bayesian approaches to their research skills.\nBayesian Statistics the Fun Way by Will Kurt brings Bayesian notions to a broad audience and its presentation blends will with an introductory course in statistics.\nBayesian Thinking in Biostatistics by Gary L Rosner, et al., provides rigorous applications in bioinformatics along with strong software use."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#cooperative-classroom",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#cooperative-classroom",
    "title": "1: Conditional Probability",
    "section": "Cooperative Classroom",
    "text": "Cooperative Classroom\nLearning in a cooperative environment should be stimulating, demanding, and fair. Because this approach to learning is different from the competitive classroom structure that many other courses used to be based on, it is important for us to be clear about mutual expectations. Below are my expectations for students in this class. This set of expectations is intended to maximize debate and exchange of ideas in an atmosphere of mutual respect while preserving individual ownership of ideas and written words. If you feel you do not understand or cannot agree to these expectations, you should discuss this with your instructor and classmates.\n\nStudents are expected to work cooperatively with other members of the class and show respect for the ideas and contributions of other people.\nWhen working as part of a group, students should strive to be good contributors to the group, listen to others, not dominate, and recognize the contributions of others. Students should try to ensure that everyone in the group is welcome to contribute and recognize that everyone contributes in different ways to a group process.\nStudents should explore data, make observations, and develop inferences as part of a group. If you use material from published sources, you must provide appropriate attribution.\n\n\n\n(Students will be asked to acknowledge this document in an online form.)\nThis document has been adapted from Scientific Teaching by Jo Handelsman, Sarah Miller, and Christine Pfund\n\n\n\n\nScientific Teaching"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#pep-talk",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#pep-talk",
    "title": "1: Conditional Probability",
    "section": "Pep Talk",
    "text": "Pep Talk\nLearning R can be difficult at first—it is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you will be using like ggplot2—made this wise observation:\n\n\n\n\n\n\nWisdom from Hadley Wickham\n\n\n\n\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\n\n\nIf you are finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, ask questions … e-mail [Derek], etc. I promise you can do this.\n—Andrew Heiss, Georgia State University"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#inclusion-statement",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#inclusion-statement",
    "title": "1: Conditional Probability",
    "section": "Inclusion Statement",
    "text": "Inclusion Statement\nI value all students regardless of their background, country of origin, race, religion, ethnicity, gender, sexual orientation, disability status, etc. and am committed to providing a climate of excellence and inclusiveness within all aspects of the course. If there are aspects of your culture or identity that you would like to share with me as they relate to your success in this class, I am happy to meet to discuss. Likewise, if you have any concerns in this area or facing any special issues or challenges, you are encouraged to discuss the matter with me (set up a meeting by e-mail) with an assurance of full confidentiality (only exception being mandatory reporting of academic integrity code violations or sexual harassment)."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#setting",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#setting",
    "title": "1: Conditional Probability",
    "section": "Setting",
    "text": "Setting\n\n\nLet us visit the lands of Faerûn. To grossly simplify and introduce notions from Dungeons and Dragons, let us define the following random variables:\n\n\\(H\\): human\n\\(E\\): evil\n\nso that \\(H^{c}\\) is “non-human” and \\(E^c\\) is “not evil”.\n\n\n\n\nBaldur’s Gate 3\n\n\n\n\n\n\n\n\n\n\nUnicode Characters\n\n\n\n\n\nDerek wanted to make a note to himself here that the way to make accented letters in a markdown environment is to use unicode characters."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nContingency TableCode\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(E^{c}|H)\\) and \\(P(H^{c}|E)\\)\n\n\n\nalignment &lt;- c(\"evil\", \"not evil\")\nhuman &lt;- c(\"24\", \"30\")\nnon_human &lt;- c(\"88\", \"178\")\n\nbg3_df &lt;- data.frame(alignment, human, non_human)\n\nbg3_gt_table &lt;- bg3_df |&gt;\n  gt(rowname_col = \"alignment\") |&gt;\n  cols_align(align = \"right\", columns = alignment) |&gt;\n  cols_align(align = \"center\",  columns = c(human, non_human)) |&gt;\n  tab_style(locations = cells_body(columns = c(human, non_human)),\n            style = list(cell_fill(color = \"yellow\")))\n\nbg3_gt_table #display table"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#practice",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#practice",
    "title": "1: Conditional Probability",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(H|E)\\) and \\(P(E|H)\\). What do you observe about the results?"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#setting-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#setting-1",
    "title": "1: Conditional Probability",
    "section": "Setting",
    "text": "Setting\n\n\nDuring the Winter of 2024, Kaggle had a competition where programmers were asked to “create an energy prediction model of prosumers to reduce energy imbalance costs” based on data that included property information, historical weather, and forecasted weather.\nFor now, let us pretend to classify the results into “high energy” and “low energy” usage, where “positive” results correspond to the “high energy” prosumers.\n\n\nThis Kaggle competition was called “Enefit”, and it was created by Eesti Energia to model Estonian energy customers."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-1",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nConfusion MatrixCodeMetricsFormulas\n\n\nSuppose that a team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\ntrue positives: 59\ntrue negatives: 97\nfalse positives: 14\nfalse negatives: 28\n\n\n\n\nenergy_levels &lt;- c(\"high\", \"low\")\nhigh &lt;- c(59, 14)\nlow &lt;- c(28, 97)\n\nenefit_df &lt;- data.frame(energy_levels, high, low)\n\nenefit_gt &lt;- enefit_df |&gt;\n  gt(rowname_col = \"energy_levels\") |&gt;\n  cols_align(align = \"right\", columns = energy_levels) |&gt;\n  cols_align(align = \"center\",  columns = c(high, low)) |&gt;\n  tab_spanner(columns = c(high, low),\n              label = \"model predictions\") |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 1),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 2),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 2),\n            style = list(cell_fill(color = \"#FFB3B2\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 1),\n            style = list(cell_fill(color = \"#FFB3B2\")))\n\nenefit_gt #display table\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\n\n\\[\\text{accuracy } = \\frac{TP + TN}{TP + FN + FP + TN}\\] \\[\\text{sensitivity } = \\frac{TP}{TP + FN}\\] \\[\\text{specificity } = \\frac{TN}{FP + TN}\\] \\[\\text{F-score } = \\frac{2*TP}{2*TP + FN + FP}\\]\nSource: Wikipedia page on sensitivity and specificity\n\n\n\n\n\n\n\n\n\nRow Span\n\n\n\n\n\nWhen these lecture notes were written, there might not have been a function to have a label span multiple rows in the gt package. The left side of the confusion matrix should say “ground truth”"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#practice-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#practice-1",
    "title": "1: Conditional Probability",
    "section": "Practice",
    "text": "Practice\nSuppose that another team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n94\n80\n    low\n23\n939\n  \n  \n  \n\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\nProsecutor’s Fallacy\n\n\n\n\n\nFor events \\(A\\) and \\(B\\), the inverse conditional probabilities are almost never equal to each other\n\\[P(A|B) \\neq P(B|A)\\]"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#classical-probability",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#classical-probability",
    "title": "1: Conditional Probability",
    "section": "Classical Probability",
    "text": "Classical Probability\n\nDefinitionExample\n\n\n\\[P(A) = \\frac{\\text{number of outcomes that are } A}{\\text{number of outcomes total}}\\]\n\n\nOverly simplistic example\nIt snowed during 2 of the 30 days of January in 2024. We can claim that it snows \\(\\frac{2}{30}\\) of the days in Princeton."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#frequentist-probablity",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#frequentist-probablity",
    "title": "1: Conditional Probability",
    "section": "Frequentist Probablity",
    "text": "Frequentist Probablity\n\nDefinitionExample\n\n\n\\[P(A) = \\lim_{n \\to \\infty} \\frac{\\text{number of outcomes that are } A}{\\text{number of outcomes total}}\\]\n\n\nOverly simplistic example\nIf we can model many months of weather in Princeton, we expect it to snow in about 3 percent of the days."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-probability",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-probability",
    "title": "1: Conditional Probability",
    "section": "Bayesian Probability",
    "text": "Bayesian Probability\n\nDefinitionExample\n\n\n\nprior belief: \\(P(A)\\)\nupdated belief: \\(P(A|B)\\)\n\n\n\nOverly simplistic example\nIt snowed during 2 of the 30 days of January in 2024. Could we update our probability calculations for snow?"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-3",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-3",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nContingency TableCode\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(E^{c}|H)\\) and \\(P(H^{c}|E)\\)\n\n\n\nalignment &lt;- c(\"evil\", \"not evil\")\nhuman &lt;- c(\"24\", \"30\")\nnon_human &lt;- c(\"88\", \"178\")\n\nbg3_df &lt;- data.frame(alignment, human, non_human)\n\nbg3_gt_table &lt;- bg3_df |&gt;\n  gt(rowname_col = \"alignment\") |&gt;\n  cols_align(align = \"right\", columns = alignment) |&gt;\n  cols_align(align = \"center\",  columns = c(human, non_human)) |&gt;\n  tab_style(locations = cells_body(columns = c(human, non_human)),\n            style = list(cell_fill(color = \"yellow\")))\n\nbg3_gt_table #display table"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-4",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-4",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nConfusion MatrixCodeMetricsFormulas\n\n\nSuppose that a team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\ntrue positives: 59\ntrue negatives: 97\nfalse positives: 14\nfalse negatives: 28\n\n\n\n\nenergy_levels &lt;- c(\"high\", \"low\")\nhigh &lt;- c(59, 14)\nlow &lt;- c(28, 97)\n\nenefit_df &lt;- data.frame(energy_levels, high, low)\n\nenefit_gt &lt;- enefit_df |&gt;\n  gt(rowname_col = \"energy_levels\") |&gt;\n  cols_align(align = \"right\", columns = energy_levels) |&gt;\n  cols_align(align = \"center\",  columns = c(high, low)) |&gt;\n  tab_spanner(columns = c(high, low),\n              label = \"model predictions\") |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 1),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 2),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 2),\n            style = list(cell_fill(color = \"#FFB3B2\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 1),\n            style = list(cell_fill(color = \"#FFB3B2\")))\n\nenefit_gt #display table\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\n\n\\[\\text{accuracy } = \\frac{TP + TN}{TP + FN + FP + TN}\\] \\[\\text{sensitivity } = \\frac{TP}{TP + FN}\\] \\[\\text{specificity } = \\frac{TN}{FP + TN}\\] \\[\\text{F-score } = \\frac{2*TP}{2*TP + FN + FP}\\]\nSource: Wikipedia page on sensitivity and specificity\n\n\n\n\n\n\n\n\n\nRow Span\n\n\n\n\n\nWhen these lecture notes were written, there might not have been a function to have a label span multiple rows in the gt package. The left side of the confusion matrix should say “ground truth”"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html",
    "href": "posts/03_beta_binomial/03_beta_binomial.html",
    "title": "3: Beta-Binomial Models",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"gt\")\nlibrary(\"janitor\")\nlibrary(\"patchwork\")\nlibrary(\"skimr\")\nlibrary(\"tidyverse\")\n\ntips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#tips-data-set",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#tips-data-set",
    "title": "3: Beta-Binomial Models",
    "section": "Tips Data Set",
    "text": "Tips Data Set\n\nDescriptionGlanceStructureSkim\n\n\n\nsource: Kaggle\n\n“The data was reported in a collection of case studies for business statistics. Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing\n\ncontext: “One waiter recorded information about each tip he received over a period of a few months working in one restaurant. In all he recorded 244 tips.”\n\n\n\n\nhead(tips_df)\n\n# A tibble: 6 × 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n3       21.0  3.5  Male   No     Sun   Dinner     3\n4       23.7  3.31 Male   No     Sun   Dinner     2\n5       24.6  3.61 Female No     Sun   Dinner     4\n6       25.3  4.71 Male   No     Sun   Dinner     4\n\n\n\n\n\nstr(tips_df, give.attr = FALSE)\n\nspc_tbl_ [244 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ total_bill: num [1:244] 17 10.3 21 23.7 24.6 ...\n $ tip       : num [1:244] 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ...\n $ sex       : chr [1:244] \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ smoker    : chr [1:244] \"No\" \"No\" \"No\" \"No\" ...\n $ day       : chr [1:244] \"Sun\" \"Sun\" \"Sun\" \"Sun\" ...\n $ time      : chr [1:244] \"Dinner\" \"Dinner\" \"Dinner\" \"Dinner\" ...\n $ size      : num [1:244] 2 3 3 2 4 4 2 4 2 2 ...\n\n\n\n\n\nskimr::skim(tips_df)\n\n\nData summary\n\n\nName\ntips_df\n\n\nNumber of rows\n244\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\nsmoker\n0\n1\n2\n3\n0\n2\n0\n\n\nday\n0\n1\n3\n4\n0\n4\n0\n\n\ntime\n0\n1\n5\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntotal_bill\n0\n1\n19.79\n8.90\n3.07\n13.35\n17.8\n24.13\n50.81\n▃▇▃▁▁\n\n\ntip\n0\n1\n3.00\n1.38\n1.00\n2.00\n2.9\n3.56\n10.00\n▇▆▂▁▁\n\n\nsize\n0\n1\n2.57\n0.95\n1.00\n2.00\n2.0\n3.00\n6.00\n▇▂▂▁▁"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#prior-model",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#prior-model",
    "title": "3: Beta-Binomial Models",
    "section": "Prior Model",
    "text": "Prior Model\n\n\n\n\\(\\pi\\)\n0.25\n0.50\n0.75\ntotal\n\n\n\n\n\\(f(\\pi)\\)\n1/3\n1/3\n1/3\n1\n\n\n\n\nuniform prior\ne.g. guessing the probability that the percentage of customers that smoked was 75% was \\(\\frac{1}{3}\\)\n\n\n\n\n\n\n\nDiscrete Probability Model\n\n\n\n\n\nLet \\(Y\\) be a discrete random variable. The probability model of \\(Y\\) is specified by a probability mass function (pmf) \\(f(y)\\). This pmf defines the probability of any given outcome \\(y\\),\n\\[f(y) = P(Y = y)\\]\n\n\\(0 \\leq f(y) \\leq 1\\)\n\\(\\sum f(y) = 1\\)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#observed-data",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#observed-data",
    "title": "3: Beta-Binomial Models",
    "section": "Observed Data",
    "text": "Observed Data\n\nObserved SampleCode\n\n\nLooking at the last 9 observations in the data set, 4 of the customers were smokers.\n\n\n\n\n\n\n  \n    \n    \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    10.07\n1.25\nMale\nNo\nSat\nDinner\n2\n    12.60\n1.00\nMale\nYes\nSat\nDinner\n2\n    32.83\n1.17\nMale\nYes\nSat\nDinner\n2\n    35.83\n4.67\nFemale\nNo\nSat\nDinner\n3\n    29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n    27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n    22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n    17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n    18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n  \n  \n  \n\n\n\n\n\n\n\ntail(tips_df, 9) |&gt;\n  gt() |&gt;\n  tab_style(locations = cells_body(columns = smoker),\n            style = list(cell_fill(color = \"gray80\"))) |&gt;\n  tab_style(locations = cells_body(columns = smoker,\n                                   rows = smoker == \"Yes\"),\n            style = list(cell_text(color = \"red\")))"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#binomial-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#binomial-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\n\nLet random variable \\(Y\\) be the number of successes in a fixed number of trials \\(n\\). Assume that the trials are independent and that the probability of success in each trial is \\(\\pi\\). Then the conditional dependence of \\(Y\\) on \\(\\pi\\) can be modeled by the Binomial model with parameters \\(n\\) and \\(\\pi\\). In mathematical notation:\n\\[Y|\\pi \\sim \\text{Bin}(n,\\pi)\\] where \\(\\sim\\) can be read as “modeled by”. Correspondingly, the binomial model is specified by the conditional pmf\n\\[f(y|\\pi) = \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y} \\text{ for } y \\in \\{0, 1, 2, ..., n\\}\\] where \\(\\binom{n}{y} = \\displaystyle\\frac{n!}{y!(n-y)!}\\)\n\n\n\nIn this example of \\(Y\\) smokers in \\(n=9\\) customers with probability \\(\\pi\\) of smokers,\n\\[Y|\\pi \\sim \\text{Bin}(9,\\pi)\\] \\[f(y|\\pi) = \\binom{9}{y}\\pi^{y}(1-\\pi)^{9-y} \\text{ for } y \\in \\{0, 1, 2, ..., 9\\}\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#conditional-pmfs",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#conditional-pmfs",
    "title": "3: Beta-Binomial Models",
    "section": "Conditional PMFs",
    "text": "Conditional PMFs\n\nBased on Observed DataCode\n\n\n\n\n\n\n\n\n\n\nhighlight_col &lt;- 0:9 == 4\ndf_25 &lt;- data.frame(k = 0:9, f_y_pi = dbinom(0:9, 9, 0.25), highlight_col)\ndf_50 &lt;- data.frame(k = 0:9, f_y_pi = dbinom(0:9, 9, 0.50), highlight_col)\ndf_75 &lt;- data.frame(k = 0:9, f_y_pi = dbinom(0:9, 9, 0.75), highlight_col)\n\nplot_25 &lt;- df_25 |&gt;\n  ggplot(aes(x = k, y = f_y_pi, fill = highlight_col)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Bin(9,0.25)\") +\n  scale_x_continuous(name = \"customers\", \n                   breaks = 0:9, \n                   labels = as.character(0:9)) +\n  theme_minimal()\n\nplot_50 &lt;- df_50 |&gt;\n  ggplot(aes(x = k, y = f_y_pi, fill = highlight_col)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Bin(9,0.50)\") +\n  scale_x_continuous(name = \"customers\", \n                   breaks = 0:9, \n                   labels = as.character(0:9)) +\n  theme_minimal()\n\nplot_75 &lt;- df_75 |&gt;\n  ggplot(aes(x = k, y = f_y_pi, fill = highlight_col)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Bin(9,0.75)\") +\n  scale_x_continuous(name = \"customers\", \n                   breaks = 0:9, \n                   labels = as.character(0:9)) +\n  theme_minimal()\n\n# patchwork\nplot_25 + plot_50 + plot_75"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#likelihoods",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#likelihoods",
    "title": "3: Beta-Binomial Models",
    "section": "Likelihoods",
    "text": "Likelihoods\nWith the observed data \\(Y = 4\\) out of \\(n = 9\\) customers, for \\(\\pi = \\{0.25, 0.50, 0.75\\}\\), \\[L(\\pi|y = 4) = f(y = 4|\\pi) = \\binom{9}{4}\\pi^{4}(1-\\pi)^{5}\\]\n\\[L(\\pi = 0.25|y = 4) = \\binom{9}{4}(0.25)^{4}(1-0.25)^{5} \\approx 0.1168\\] \\[L(\\pi = 0.50|y = 4) = \\binom{9}{4}(0.50)^{4}(1-0.50)^{5} \\approx 0.2461\\] \\[L(\\pi = 0.75|y = 4) = \\binom{9}{4}(0.75)^{4}(1-0.75)^{5} \\approx 0.0389\\]\n\n\n\n\\(\\pi\\)\n0.25\n0.50\n0.75\ntotal\n\n\n\n\n\\(f(\\pi)\\)\n1/3\n1/3\n1/3\n1\n\n\n\\(L(\\pi|y=4)\\)\n0.1168\n0.2461\n0.0389\n0.4018"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#bayesian-concepts",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#bayesian-concepts",
    "title": "3: Beta-Binomial Models",
    "section": "Bayesian Concepts",
    "text": "Bayesian Concepts\n\\[\\text{posterior} = \\frac{\\text{prior} * \\text{likelihood}}{\\text{normalizing constant}}\\]\nFor observations \\(\\vec{y}\\) and probabilities \\(\\vec{\\pi}\\),\n\\[f(\\pi|y) = \\frac{f(\\pi)L(\\pi|y)}{f(y)} \\propto f(\\pi)L(\\pi|y)\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#normalizing-constant",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#normalizing-constant",
    "title": "3: Beta-Binomial Models",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\\[f(y = 4) = \\displaystyle\\sum_{\\pi\\in\\{0.25, 0.50, 0.75\\}} L(\\pi|y=4) \\cdot f(\\pi)\\]\n\\[f(y = 4) = \\displaystyle\\frac{0.1168}{3} + \\displaystyle\\frac{0.2461}{3} + \\displaystyle\\frac{0.0389}{3} \\approx 0.1339\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#posterior-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#posterior-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\\[f(\\pi|y=4) = \\displaystyle\\frac{f(\\pi)L(\\pi|y=4)}{f(y=4)} \\text{ for } \\pi \\in \\{0.25, 0.50, 0.75 \\}\\] \\[f(\\pi=0.25|y=4) = \\displaystyle\\frac{(1/3)(0.1168)}{0.1339} \\approx 0.2907\\] \\[f(\\pi=0.50|y=4) = \\displaystyle\\frac{(1/3)(0.2461)}{0.1339} \\approx 0.6126\\] \\[f(\\pi=0.75|y=4) = \\displaystyle\\frac{(1/3)(0.0389)}{0.1339} \\approx 0.0968\\]\n\n\n\n\\(\\pi\\)\n0.25\n0.50\n0.75\ntotal\n\n\n\n\n\\(f(\\pi)\\)\n1/3\n1/3\n1/3\n1\n\n\n\\(L(\\pi|y=4)\\)\n0.1168\n0.2461\n0.0389\n0.4018\n\n\n\\(f(\\pi|y=4)\\)\n0.2907\n0.6126\n0.0968\n1"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#computer-simulation",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#computer-simulation",
    "title": "3: Beta-Binomial Models",
    "section": "Computer Simulation",
    "text": "Computer Simulation\n\nSimulation SamplesVerify PriorPMFsPosterior Distribution\n\n\n\n# define possible smoker proportions\nsmokers &lt;- data.frame(pi = c(0.25, 0.50, 0.75))\n\n# define prior model\nprior &lt;- c(1/3, 1/3, 1/3)\n\n# simulate 10000 values of pi from the prior\nset.seed(320)\nsmoker_sim &lt;- sample_n(smokers, size = 10000, weight = prior, replace = TRUE)\n\n# simulate 10000 samples of customers\nsmoker_sim &lt;- smoker_sim |&gt;\n  mutate(y = rbinom(10000, size = 9, prob = pi))\n\nSo far, the simulation yields a data frame that looks like\n\nhead(smoker_sim)\n\n    pi y\n1 0.50 5\n2 0.25 2\n3 0.50 6\n4 0.25 3\n5 0.50 4\n6 0.50 4\n\n\n\n\n\n# summarize the prior\nsmoker_sim |&gt;\n  tabyl(pi) |&gt;\n  adorn_totals(\"row\")\n\n    pi     n percent\n  0.25  3283  0.3283\n   0.5  3345  0.3345\n  0.75  3372  0.3372\n Total 10000  1.0000\n\n\n\n\n\n# plot y by pi\nggplot(smoker_sim, aes(x = y)) + \n  stat_count(aes(y = after_stat(prop))) + \n  facet_wrap(~ pi)\n\n\n\n\n\n\n\n# focus on simulations with y = 4\nfour_smokers &lt;- smoker_sim %&gt;% \n  filter(y == 4)\n\n# summarize the posterior approximation\nfour_smokers %&gt;% \n  tabyl(pi) %&gt;% \n  adorn_totals(\"row\")\n\n    pi    n   percent\n  0.25  408 0.3000000\n   0.5  813 0.5977941\n  0.75  139 0.1022059\n Total 1360 1.0000000\n\n\n\n# plot the posterior approximation\nggplot(four_smokers, aes(x = pi)) + \n  geom_bar()"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#beta-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#beta-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Beta Distribution",
    "text": "Beta Distribution\n\n\n\n\n\n\nBeta Distribution\n\n\n\n\n\nLet \\(\\pi \\in [0,1]\\), then the variability in \\(\\pi\\) may be modeled by a Beta distribution with shape hyperparameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\)\n\\[\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\] with probability density function\n\\[f(\\pi) = \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\]\nwhere the gamma function\n\n\\(\\Gamma(z) = \\displaystyle\\int_{0}^{\\infty} \\! x^{z-1}e^{-x} \\, dx\\)\n\\(\\Gamma(z + 1) = z\\Gamma(z)\\)\n\n\n\n\n\n\n\n\n\n\nCorollary\n\n\n\n\n\nWhen \\(z\\) is a positive integer, then \\[\\Gamma(z) = (z-1)!\\] That is, the gamma function is a generalization of the factorial.\n\n\n\n\n\n\n\n\n\nHyperparameters\n\n\n\n\n\nA hyperparameter is a parameter used in a prior model.\n\n\n\n\n\n\n\n\n\nExplore!\n\n\n\n\n\nMatt Bognar at the University of Iowa created this great webapp to explore the beta distribution.\n\n\n\n\n\n\n\n\n\nUniform Distribution\n\n\n\n\n\nWhen it is equally plausible for \\(\\pi\\) to take on any value between zero and one, we can model \\(\\pi\\) by the standard uniform distribution\n\\[\\pi \\sim \\text{Unif}(0,1)\\]\nwith pdf \\(f(\\pi) = 1\\) for \\(\\pi \\in [0,1]\\). The \\(\\text{Unif}(0,1)\\) distribution is a special case of the beta distribution when \\(\\alpha = 1\\) and \\(\\beta = 1\\)\n\\[\\text{Unif}(0,1) = \\text{Beta}(1,1)\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#sample-statistics",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#sample-statistics",
    "title": "3: Beta-Binomial Models",
    "section": "Sample Statistics",
    "text": "Sample Statistics\nFor a beta distribution, \\(\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\)\n\nexpected value: \\(\\text{E}(\\pi) = \\displaystyle\\frac{\\alpha}{\\alpha + \\beta}\\)\nvariance: \\(\\text{Var}(\\pi) = \\displaystyle\\frac{\\alpha\\beta}{(\\alpha + \\beta)^{2}(\\alpha + \\beta + 1)}\\)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#tuning-the-beta-prior",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#tuning-the-beta-prior",
    "title": "3: Beta-Binomial Models",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\nHere, let us use that sample of observations where 4 out of the 9 customers where smokers. We might then try to align this sample proportion \\(\\frac{4}{9}\\) with the expected value\n\\[\\displaystyle\\frac{\\alpha}{\\alpha + \\beta} = \\displaystyle\\frac{4}{9} \\quad\\rightarrow\\quad \\alpha = 4, \\quad \\beta = 5\\]\nto create a beta model \\(\\pi \\sim \\text{Beta}(4, 5)\\)\n\nbayesrules::plot_beta(4,5)\n\n\n\n\nWe can compute the variance\n\\[\\text{Var}(\\pi) = \\displaystyle\\frac{\\alpha\\beta}{(\\alpha + \\beta)^{2}(\\alpha + \\beta + 1)} = \\displaystyle\\frac{4 \\cdot 5}{(4 + 5)^{2}(4 + 5 + 1)} \\approx 0.0247\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#binomial-data-model",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#binomial-data-model",
    "title": "3: Beta-Binomial Models",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\nSuppose that we obtain a larger sample of observations with \\(n = 25\\) customers. The number of smokers, denoted by random variable \\(Y\\), may have a binomial model conditional on probability \\(\\pi\\),\n\\[Y|\\pi \\sim \\text{Bin}(25, \\pi)\\]\nwith conditional pmf over \\(y \\in \\{0, 1, ..., 25\\}\\), \\[f(y|\\pi) = P(Y = y|\\pi) = \\binom{25}{y}\\pi^{y}(1-\\pi)^{25-y}\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#likelihood",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#likelihood",
    "title": "3: Beta-Binomial Models",
    "section": "Likelihood",
    "text": "Likelihood\n\nFunctionPlotCode\n\n\nSuppose that in that sample of \\(n = 25\\) customers, we observe that \\(y = 7\\) of those customers were smokers. Our likelihood function is then\n\\[L(\\pi|y = 7) = \\binom{25}{7}\\pi^{7}(1-\\pi)^{18}\\]\n\n\n\n\n\n\n\n\n\n\npi &lt;- seq(0, 1, 0.01)\nL_pi_y &lt;- dbinom(7, 25, pi)\n\ndf_for_graph &lt;- data.frame(pi, L_pi_y)\n\ndf_for_graph |&gt;\n  ggplot(aes(x = pi, y = L_pi_y)) +\n  geom_line() +\n  labs(title = \"Likelihood function\",\n       subtitle = \"y = 7, n = 25\",\n       caption = \"SML 320\")"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#beta-binomial-model",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#beta-binomial-model",
    "title": "3: Beta-Binomial Models",
    "section": "Beta-Binomial Model",
    "text": "Beta-Binomial Model\nClaim: With probability \\(\\pi \\in [0,1]\\) and random variable \\(Y\\) representing the number of “successes” in \\(n\\) trials, if the behavior is modeled with prior distribution and likelihood\n\\[\\begin{array}{rcl}\n  \\pi & \\sim & \\text{Beta}(\\alpha, \\beta) \\\\\n  Y|\\pi & \\sim & \\text{Bin}(n,\\pi) \\\\\n\\end{array}\\]\nthen the posterior distribution can be modeled with an updated beta distribution\n\\[\\pi|(Y=y) \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\]\nwith sample statistics\n\\[\\begin{array}{rcl}\n  \\text{E}(\\pi|Y=y) & = & \\displaystyle\\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\n  \\text{Var}(\\pi|Y=y) & = & \\displaystyle\\frac{(\\alpha  +y)(\\beta + n - y)}{(\\alpha + \\beta + n)^{2}(\\alpha + \\beta + n + 1)} \\\\\n\\end{array}\\]\n\n\n\n\n\n\nPartial Proof\n\n\n\n\n\nWith the conditional pmf\n\\[f(\\pi) = \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\] and likelihood function\n\\[L(\\pi|y) = \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y}\\]\nit follows from Bayes’ Rule that the posterior distribution\n\\[\\begin{array}{rcl}\n  f(\\pi|y) & \\propto & f(\\pi)L(\\pi|y) \\\\\n  ~ & = & \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\cdot \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y} \\\\\n  ~ & \\propto & \\pi^{(\\alpha + y)-1}(1-\\pi)^{(\\beta+n-y)-1} \\\\\n\\end{array}\\]\nwhere that last expression is the unnormalized posterior pdf. We observe that it has the same structure of the normalized \\(\\text{Beta}(\\alpha + y, \\beta + n - y)\\) pdf\n\\[f(\\pi|y) = \\displaystyle\\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+y)\\Gamma(\\beta+n-y)} \\pi^{(\\alpha + y)-1}(1-\\pi)^{(\\beta+n-y)-1}\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#beta-posterior",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#beta-posterior",
    "title": "3: Beta-Binomial Models",
    "section": "Beta Posterior",
    "text": "Beta Posterior\n\nUpdated DistributionPlotBoth\n\n\nBy the above theory, having started with a \\(\\text{Beta}{(4,5)}\\) prior, and then observing \\(y = 7\\) smokers among \\(n = 25\\) customers\n\\[\\alpha = 4, \\quad \\beta = 5, \\quad y = 7, \\quad n = 25\\]\nour posterior distribution can be modeled with\n\\[\\pi|(Y=y) \\sim \\text{Beta}(\\alpha + y, \\beta + n - y) = \\text{Beta}(11, 23)\\]\n\n\n\nbayesrules::plot_beta(11,23)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#putting-it-all-together",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#putting-it-all-together",
    "title": "3: Beta-Binomial Models",
    "section": "Putting it All Together",
    "text": "Putting it All Together\n\nHelper FunctionsTablePlot\n\n\nThe bayesrules package (from the textbook authors) provide additonal helper functions for this procedure of modeling with a beta-binomial model.\nbayesrules::summarize_beta_binomial(alpha, beta, y, n)\nbayesrules::plot_beta_binomial(alpha, beta, y, n)\n\n\n\nsummarize_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    5 0.4444 0.4286 0.0247 0.1571\n2 posterior    11   23 0.3235 0.3125 0.0063 0.0791\n\n\n\n\n\nplot_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html",
    "title": "3: Beta-Binomial Models",
    "section": "",
    "text": "Today, let’s see if the following R code runs on your computer.\nRemember to install packages as needed.\nlibrary(\"bayesrules\")\nlibrary(\"gt\")\nlibrary(\"janitor\")\nlibrary(\"patchwork\")\nlibrary(\"skimr\")\nlibrary(\"tidyverse\")\nAlso, place the tips.csv file in the same directory as this script.\ntips_df &lt;- readr::read_csv(\"tips.csv\")\n\nRows: 244 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): sex, smoker, day, time\ndbl (3): total_bill, tip, size\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#simulation-samples",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#simulation-samples",
    "title": "3: Beta-Binomial Models",
    "section": "Simulation Samples",
    "text": "Simulation Samples\n\n# define possible smoker proportions\nsmokers &lt;- data.frame(pi = c(0.25, 0.50, 0.75))\n\n# define prior model\nprior &lt;- c(1/3, 1/3, 1/3)\n\n# simulate 10000 values of pi from the prior\nset.seed(320)\nsmoker_sim &lt;- sample_n(smokers, size = 10000, weight = prior, replace = TRUE)\n\n# simulate 10000 samples of customers\nsmoker_sim &lt;- smoker_sim |&gt;\n  mutate(y = rbinom(10000, size = 9, prob = pi))"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#pmfs",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#pmfs",
    "title": "3: Beta-Binomial Models",
    "section": "PMFs",
    "text": "PMFs\n\n# plot y by pi\nggplot(smoker_sim, aes(x = y)) + \n  stat_count(aes(y = after_stat(prop))) + \n  facet_wrap(~ pi)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#posterior-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#posterior-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\n# focus on simulations with y = 4\nfour_smokers &lt;- smoker_sim %&gt;% \n  filter(y == 4)\n\n# summarize the posterior approximation\nfour_smokers %&gt;% \n  tabyl(pi) %&gt;% \n  adorn_totals(\"row\")\n\n    pi    n   percent\n  0.25  408 0.3000000\n   0.5  813 0.5977941\n  0.75  139 0.1022059\n Total 1360 1.0000000\n\n\n\n# plot the posterior approximation\nggplot(four_smokers, aes(x = pi)) + \n  geom_bar()"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#table",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#table",
    "title": "3: Beta-Binomial Models",
    "section": "Table",
    "text": "Table\n\nsummarize_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    5 0.4444 0.4286 0.0247 0.1571\n2 posterior    11   23 0.3235 0.3125 0.0063 0.0791"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#plot",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#plot",
    "title": "3: Beta-Binomial Models",
    "section": "Plot",
    "text": "Plot\n\nplot_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html",
    "title": "4: Balance and Sequentiality",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\ntips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#journey-so-far",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#journey-so-far",
    "title": "4: Balance and Sequentiality",
    "section": "Journey so far",
    "text": "Journey so far\nWith hopes of learning more about a target probability \\[\\pi \\in [0,1]\\] we have been applying beta-binomial models\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Beta}(\\alpha, \\beta) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Bin}(y, \\pi) \\\\\n  \\text{posterior: } & \\pi|(Y = y) & \\sim & \\text{Beta}(\\alpha + y, \\beta + n - y) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta",
    "title": "4: Balance and Sequentiality",
    "section": "Using plot_beta",
    "text": "Using plot_beta\n\nInclinationsCode\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesrules::plot_beta(4,8) +\n  labs(title = \"Beta(4,8), E(pi) = 1/3\")\np2 &lt;- bayesrules::plot_beta(6,6) +\n  labs(title = \"Beta(6,6), E(pi) = 1/2\")\np3 &lt;- bayesrules::plot_beta(8,4) +\n  labs(title = \"Beta(8,4), E(pi) = 2/3\")\n\n# patchwork\np1 / p2 / p3"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-thursday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-thursday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Thursday Customers",
    "text": "Subset: Thursday Customers\n\nThursday &lt;- tips_df |&gt;\n  filter(day == \"Thur\")\n\nn_Thursday &lt;- nrow(Thursday)\ny_Thursday &lt;- Thursday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta_binomial",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta_binomial",
    "title": "4: Balance and Sequentiality",
    "section": "Using plot_beta_binomial",
    "text": "Using plot_beta_binomial\n\nCodePlotsStatistics\n\n\n\np1 &lt;- bayesrules::plot_beta_binomial(4, 8, y_Thursday, n_Thursday) +\n  labs(title = \"Beta(4,8) prior\")\np2 &lt;- bayesrules::plot_beta_binomial(6, 6, y_Thursday, n_Thursday) +\n  labs(title = \"Beta(6,6) prior\")\np3 &lt;- bayesrules::plot_beta_binomial(8, 4, y_Thursday, n_Thursday) +\n  labs(title = \"Beta(8,4) prior\")\n\n# patchwork\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_beta_binomial(4, 8, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    8 0.3333 0.3000 0.0171 0.1307\n2 posterior    21   53 0.2838 0.2778 0.0027 0.0521\n\n\n\nbayesrules::summarize_beta_binomial(6, 6, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     6    6 0.5000 0.5000 0.0192 0.1387\n2 posterior    23   51 0.3108 0.3056 0.0029 0.0534\n\n\n\nbayesrules::summarize_beta_binomial(8, 4, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     8    4 0.6667 0.7000 0.0171 0.1307\n2 posterior    25   49 0.3378 0.3333 0.0030 0.0546"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#certainty",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#certainty",
    "title": "4: Balance and Sequentiality",
    "section": "Certainty",
    "text": "Certainty\n\nInclinationsCode\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesrules::plot_beta(1,1) +\n  labs(title = \"Uniform Prior: Beta(1,1), var(pi) = 0.0833\")\np2 &lt;- bayesrules::plot_beta(4,4) +\n  labs(title = \"Vague Prior: Beta(4,4), var(pi) = 0.0278\")\np3 &lt;- bayesrules::plot_beta(16,16) +\n  labs(title = \"Informative Prior: Beta(16,16), var(pi) = 0.0076\")\n\n# patchwork\np1 / p2 / p3"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-friday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-friday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Friday Customers",
    "text": "Subset: Friday Customers\n\nFriday &lt;- tips_df |&gt;\n  filter(day == \"Fri\")\n\nn_Friday &lt;- nrow(Friday)\ny_Friday &lt;- Friday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#applying-the-friday-crowd",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#applying-the-friday-crowd",
    "title": "4: Balance and Sequentiality",
    "section": "Applying the Friday Crowd",
    "text": "Applying the Friday Crowd\n\nCodePlotsStatistics\n\n\n\np1 &lt;- bayesrules::plot_beta_binomial(1, 1, y_Friday, n_Friday) +\n  labs(title = \"Uniform Prior: Beta(1,1)\")\np2 &lt;- bayesrules::plot_beta_binomial(4, 4, y_Friday, n_Friday) +\n  labs(title = \"Vague Prior: Beta(4,4)\")\np3 &lt;- bayesrules::plot_beta_binomial(16, 16, y_Friday, n_Friday) +\n  labs(title = \"Informative Prior: Beta(16,16)\")\n\n# patchwork\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_beta_binomial(1, 1, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1    1 0.5000    NaN 0.0833 0.2887\n2 posterior    16    5 0.7619 0.7895 0.0082 0.0908\n\n\n\nbayesrules::summarize_beta_binomial(4, 4, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean mode    var     sd\n1     prior     4    4 0.5000 0.50 0.0278 0.1667\n2 posterior    19    8 0.7037 0.72 0.0074 0.0863\n\n\n\nbayesrules::summarize_beta_binomial(16, 16, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    16   16 0.5000 0.5000 0.0076 0.0870\n2 posterior    31   20 0.6078 0.6122 0.0046 0.0677"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#skewed-prior",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#skewed-prior",
    "title": "4: Balance and Sequentiality",
    "section": "Skewed Prior",
    "text": "Skewed Prior\n\nFixed PriorCode\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_beta(1,32) +\n  labs(title = \"Skewed Prior: Beta(1,32), var(pi) = 0.0009\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-saturday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-saturday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Saturday Customers",
    "text": "Subset: Saturday Customers\n\nSaturday &lt;- tips_df |&gt;\n  filter(day == \"Sat\")\n\nn_Saturday &lt;- nrow(Saturday)\ny_Saturday &lt;- Saturday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#friday-versus-saturday",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#friday-versus-saturday",
    "title": "4: Balance and Sequentiality",
    "section": "Friday versus Saturday",
    "text": "Friday versus Saturday\n\nCodePlotsStatistics\n\n\n\np1 &lt;- bayesrules::plot_beta_binomial(1, 32, y_Friday, n_Friday) +\n  labs(title = \"Friday Customers\")\np2 &lt;- bayesrules::plot_beta_binomial(1, 32, y_Saturday, n_Saturday) +\n  labs(title = \"Saturday Customers\")\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_beta_binomial(1, 32, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean mode   var     sd\n1     prior     1   32 0.0303  0.0 9e-04 0.0294\n2 posterior    16   36 0.3077  0.3 4e-03 0.0634\n\n\n\nbayesrules::summarize_beta_binomial(1, 32, y_Saturday, n_Saturday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1   32 0.0303 0.0000 0.0009 0.0294\n2 posterior    43   77 0.3583 0.3559 0.0019 0.0436"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#vague-prior-revisited",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#vague-prior-revisited",
    "title": "4: Balance and Sequentiality",
    "section": "Vague Prior Revisited",
    "text": "Vague Prior Revisited\n\nVague PriorCode\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_beta(4,4) +\n  labs(title = \"Vague Prior: Beta(4,4), var(pi) = 0.0278\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subsets-lunch-and-dinner",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subsets-lunch-and-dinner",
    "title": "4: Balance and Sequentiality",
    "section": "Subsets: Lunch and Dinner",
    "text": "Subsets: Lunch and Dinner\n\n\n\nLunch &lt;- tips_df |&gt;\n  filter(time == \"Lunch\")\n\nn_Lunch &lt;- nrow(Lunch)\ny_Lunch &lt;- Lunch |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()\n\n\n\nDinner &lt;- tips_df |&gt;\n  filter(time == \"Dinner\")\n\nn_Dinner &lt;- nrow(Dinner)\ny_Dinner &lt;- Dinner |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#lunch-first-then-dinner",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#lunch-first-then-dinner",
    "title": "4: Balance and Sequentiality",
    "section": "Lunch First, then Dinner",
    "text": "Lunch First, then Dinner\n\nCodePlotsStatistics\n\n\n\nalpha_1 &lt;- 4\nbeta_1  &lt;- 4\np1 &lt;- bayesrules::plot_beta_binomial(alpha_1, beta_1, y_Lunch, n_Lunch) +\n  labs(title = \"Lunch First\")\n\nalpha_2 &lt;- alpha_1 + y_Lunch\nbeta_2  &lt;- beta_1 + n_Lunch - y_Lunch\np2 &lt;- bayesrules::plot_beta_binomial(alpha_2, beta_2, y_Dinner, n_Dinner) +\n  labs(title = \"then Dinner\")\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nLunch First\n\nbayesrules::summarize_beta_binomial(alpha_1, beta_1, y_Lunch, n_Lunch) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    4 0.5000 0.5000 0.0278 0.1667\n2 posterior    27   49 0.3553 0.3514 0.0030 0.0545\n\n\n\n\nthen Dinner\n\nbayesrules::summarize_beta_binomial(alpha_2, beta_2, y_Dinner, n_Dinner) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode   var     sd\n1     prior    27   49 0.3553 0.3514 3e-03 0.0545\n2 posterior    97  155 0.3849 0.3840 9e-04 0.0306"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#dinner-first-then-lunch",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#dinner-first-then-lunch",
    "title": "4: Balance and Sequentiality",
    "section": "Dinner First, then Lunch",
    "text": "Dinner First, then Lunch\n\nCodePlotsStatistics\n\n\n\nalpha_1 &lt;- 4\nbeta_1  &lt;- 4\np1 &lt;- bayesrules::plot_beta_binomial(alpha_1, beta_1, y_Dinner, n_Dinner) +\n  labs(title = \"Dinner First\")\n\nalpha_2 &lt;- alpha_1 + y_Dinner\nbeta_2  &lt;- beta_1 + n_Dinner - y_Dinner\np2 &lt;- bayesrules::plot_beta_binomial(alpha_2, beta_2, y_Lunch, n_Lunch) +\n  labs(title = \"then Lunch\")\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nDinner First\n\nbayesrules::summarize_beta_binomial(alpha_1, beta_1, y_Dinner, n_Dinner) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    4 0.5000 0.5000 0.0278 0.1667\n2 posterior    74  110 0.4022 0.4011 0.0013 0.0361\n\n\n\n\nthen Lunch\n\nbayesrules::summarize_beta_binomial(alpha_2, beta_2, y_Lunch, n_Lunch) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    74  110 0.4022 0.4011 0.0013 0.0361\n2 posterior    97  155 0.3849 0.3840 0.0009 0.0306"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#data-invariance",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#data-invariance",
    "title": "4: Balance and Sequentiality",
    "section": "Data Invariance",
    "text": "Data Invariance\nLet \\(\\theta\\) be any parameter of interest with prior pdf \\(f(\\theta)\\). Then a sequential analysis in which we first observe a data point \\(y_{1}\\) and then a second data point \\(y_{2}\\) will produce the same posterior model of \\(\\theta\\) as if we first observe \\(y_{2}\\) and then \\(y_{1}\\):\n\\[f(\\theta|y1,y2)=f(\\theta|y2,y1)\\]\nSimilarly, the posterior model is invariant to whether we observe the data all at once or sequentially.\n\n\n\n\n\n\nproof\n\n\n\n\n\n[Please refer to section 4.5 of the textbook]"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#uniform-prior-revisited",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#uniform-prior-revisited",
    "title": "4: Balance and Sequentiality",
    "section": "Uniform Prior Revisited",
    "text": "Uniform Prior Revisited\n\nUniform PriorCode\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_beta(1,1) +\n  labs(title = \"Uniform Prior: Beta(1,1), var(pi) = 0.0833\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-sunday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-sunday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Sunday Customers",
    "text": "Subset: Sunday Customers\n\nSunday &lt;- tips_df |&gt;\n  filter(day == \"Sun\")\n\nn_Sunday &lt;- nrow(Sunday)\ny_Sunday &lt;- Sunday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#one-day-at-a-time",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#one-day-at-a-time",
    "title": "4: Balance and Sequentiality",
    "section": "One Day at a Time",
    "text": "One Day at a Time\n\nCodePlotsStatistics\n\n\n\nalpha &lt;- 1\nbeta  &lt;- 1\np1 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Thursday, n_Thursday) +\n  labs(title = \"Thursday First\")\n\nalpha &lt;- alpha + y_Thursday\nbeta  &lt;- beta + n_Thursday - y_Thursday\np2 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Friday, n_Friday) +\n  labs(title = \"then Friday\")\n\nalpha &lt;- alpha + y_Friday\nbeta  &lt;- beta + n_Friday - y_Friday\np3 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Saturday, n_Saturday) +\n  labs(title = \"then Saturday\")\n\nalpha &lt;- alpha + y_Saturday\nbeta  &lt;- beta + n_Saturday - y_Saturday\np4 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Sunday, n_Sunday) +\n  labs(title = \"then Sunday\")\n\n# patchwork\np1 / p2 / p3 / p4\n\n\n\n\n\n\n\n\n\n\n\nThursday First\n\nalpha &lt;- 1\nbeta  &lt;- 1\nbayesrules::summarize_beta_binomial(alpha, beta, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1    1 0.5000    NaN 0.0833 0.2887\n2 posterior    18   46 0.2812 0.2742 0.0031 0.0558\n\n\n\n\nthen Friday\n\nalpha &lt;- alpha + y_Thursday\nbeta  &lt;- beta + n_Thursday - y_Thursday\nbayesrules::summarize_beta_binomial(alpha, beta, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    18   46 0.2812 0.2742 0.0031 0.0558\n2 posterior    33   50 0.3976 0.3951 0.0029 0.0534\n\n\n\n\nthen Saturday\n\nalpha &lt;- alpha + y_Friday\nbeta  &lt;- beta + n_Friday - y_Friday\nbayesrules::summarize_beta_binomial(alpha, beta, y_Saturday, n_Saturday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    33   50 0.3976 0.3951 0.0029 0.0534\n2 posterior    75   95 0.4412 0.4405 0.0014 0.0380\n\n\n\n\nthen Sunday\n\nalpha &lt;- alpha + y_Saturday\nbeta  &lt;- beta + n_Saturday - y_Saturday\nbayesrules::summarize_beta_binomial(alpha, beta, y_Sunday, n_Sunday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    75   95 0.4412 0.4405 0.0014 0.0380\n2 posterior    94  152 0.3821 0.3811 0.0010 0.0309"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-all-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-all-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: All Customers",
    "text": "Subset: All Customers\n\nn_all &lt;- nrow(tips_df)\ny_all &lt;- tips_df |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#all-at-once",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#all-at-once",
    "title": "4: Balance and Sequentiality",
    "section": "All at Once",
    "text": "All at Once\n\nCodePlotStatistics\n\n\n\nalpha &lt;- 1\nbeta  &lt;- 1\nbayesrules::plot_beta_binomial(alpha, beta, y_all, n_all) +\n  labs(title = \"All at Once\")\n\n\n\n\n\n\n\n\n\n\n\nalpha &lt;- 1\nbeta  &lt;- 1\nbayesrules::summarize_beta_binomial(alpha, beta, y_all, n_all) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1    1 0.5000    NaN 0.0833 0.2887\n2 posterior    94  152 0.3821 0.3811 0.0010 0.0309\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2  forcats_1.0.0    stringr_1.5.0    dplyr_1.1.3     \n [5] purrr_1.0.2      readr_2.1.4      tidyr_1.3.0      tibble_3.2.1    \n [9] ggplot2_3.4.3    tidyverse_2.0.0  patchwork_1.1.2  bayesrules_0.0.2\n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3       inline_0.3.19       rlang_1.1.1        \n  [4] magrittr_2.0.3      snakecase_0.11.0    matrixStats_1.0.0  \n  [7] e1071_1.7-13        compiler_4.3.0      loo_2.6.0          \n [10] callr_3.7.3         vctrs_0.6.3         reshape2_1.4.4     \n [13] pkgconfig_2.0.3     crayon_1.5.2        fastmap_1.1.1      \n [16] ellipsis_0.3.2      labeling_0.4.3      utf8_1.2.3         \n [19] threejs_0.3.3       promises_1.2.1      rmarkdown_2.24     \n [22] tzdb_0.4.0          markdown_1.8        ps_1.7.5           \n [25] nloptr_2.0.3        bit_4.0.5           xfun_0.40          \n [28] jsonlite_1.8.7      later_1.3.1         parallel_4.3.0     \n [31] prettyunits_1.1.1   R6_2.5.1            dygraphs_1.1.1.6   \n [34] stringi_1.7.12      StanHeaders_2.26.26 boot_1.3-28.1      \n [37] Rcpp_1.0.11         rstan_2.21.8        knitr_1.43         \n [40] zoo_1.8-12          base64enc_0.1-3     bayesplot_1.10.0   \n [43] httpuv_1.6.11       Matrix_1.5-4        splines_4.3.0      \n [46] igraph_1.4.3        timechange_0.2.0    tidyselect_1.2.0   \n [49] rstudioapi_0.15.0   yaml_2.3.7          codetools_0.2-19   \n [52] miniUI_0.1.1.1      processx_3.8.1      pkgbuild_1.4.0     \n [55] lattice_0.21-8      plyr_1.8.8          shiny_1.7.5        \n [58] withr_2.5.2         groupdata2_2.0.2    evaluate_0.21      \n [61] survival_3.5-5      proxy_0.4-27        RcppParallel_5.1.7 \n [64] xts_0.13.1          pillar_1.9.0        DT_0.28            \n [67] stats4_4.3.0        shinyjs_2.1.0       generics_0.1.3     \n [70] vroom_1.6.3         hms_1.1.3           rstantools_2.3.1   \n [73] munsell_0.5.0       scales_1.2.1        minqa_1.2.5        \n [76] gtools_3.9.4        xtable_1.8-4        class_7.3-21       \n [79] glue_1.6.2          janitor_2.2.0       tools_4.3.0        \n [82] shinystan_2.6.0     lme4_1.1-33         colourpicker_1.2.0 \n [85] grid_4.3.0          crosstalk_1.2.0     colorspace_2.1-0   \n [88] nlme_3.1-162        cli_3.6.1           fansi_1.0.4        \n [91] gtable_0.3.4        digest_0.6.33       farver_2.1.1       \n [94] htmlwidgets_1.6.2   htmltools_0.5.6     lifecycle_1.0.4    \n [97] mime_0.12           rstanarm_2.21.4     bit64_4.0.5        \n[100] shinythemes_1.2.0   MASS_7.3-58.4"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html",
    "href": "posts/05_conjugate_families/05_conjugate_families.html",
    "title": "5: Conjugate Families",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"ggtext\")\nlibrary(\"gt\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\ntips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#simple-prior",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#simple-prior",
    "title": "5: Conjugate Families",
    "section": "Simple Prior",
    "text": "Simple Prior\nSuppose that we wanted to estimate a probability \\(\\pi \\in [0,1]\\), but perhaps the beta distribution seems complicated. Instead, we can try an elementary math function like \\(f(\\pi) = 3\\pi^{2}\\), where this is a probability density function since\n\\[\\displaystyle\\int_{0}^{1} \\! 3\\pi^{2} \\, d\\pi = 1 \\text{ and } f(\\pi) \\geq 0 \\text{ for } \\pi \\in [0,1]\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#interpretability",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#interpretability",
    "title": "5: Conjugate Families",
    "section": "Interpretability",
    "text": "Interpretability\n\nPlotCodeInterpretation\n\n\n\n\n\n\n\n\n\n\npi &lt;- seq(0, 1, 0.01)\nf_pi &lt;- 3*pi^2\n\ndf_for_line &lt;- data.frame(pi, f_pi)\ndf_for_shade &lt;- df_for_line |&gt;\n  rbind(c(1,0)) #enforce lower-right corner\n\ndf_for_line |&gt;\n  ggplot(aes(x = pi, y = f_pi)) +\n  geom_polygon(data = df_for_shade, fill = \"#E77500\") +\n  geom_line(color = \"#121212\", linewidth = 3) +\n  labs(title = \"&lt;span style='color:#E77500'&gt;Parabolic Prior&lt;/span&gt;: f(pi) = 3pi^2\",\n       subtitle = \"left-skew\",\n       caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown()) #use ggtext package\n\n\n\nIf we start with this prior, we are perhaps assuming a situation over \\([0,1]\\) where we are expecting the event to likely occur:\n\\[\\text{E}(\\pi) = \\displaystyle\\int_{0}^{1} \\! \\pi \\cdot f(\\pi) \\, d\\pi = \\displaystyle\\frac{3}{4}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#likelihood",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#likelihood",
    "title": "5: Conjugate Families",
    "section": "Likelihood",
    "text": "Likelihood\nSuppose that we observe \\(Y = 17\\) successes in \\(n = 32\\) independent trials, then modeling the likelihood with a binomial model yields\n\\[L(\\pi|y = 17) = \\binom{32}{17}\\pi^{17}(1-\\pi)^{15} \\text{ for } \\pi \\in [0,1]\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#posterior-distribution",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#posterior-distribution",
    "title": "5: Conjugate Families",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\nRecall that the posterior distribution is proportional to the product of the prior distribution and the likelihood\n\\[\\begin{array}{rcl}\n  f(\\pi|y=17) & \\propto & f(\\pi) \\cdot L(\\pi|y=17) \\\\\n  ~ & \\propto & \\pi^{2} \\cdot \\pi^{17}(1-\\pi)^{15} \\\\\n\\end{array}\\]\ndoes not have the same form as our prior \\(f(\\pi) = 3\\pi^{2}\\)"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#normalizing-constant",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#normalizing-constant",
    "title": "5: Conjugate Families",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\\[f(\\pi|y=17) = \\displaystyle\\frac{\\pi^{19}(1-\\pi)^{15}}{ \\int_{0}^{1} \\! \\pi^{19}(1-\\pi)^{15} \\, d\\pi } \\text{ for } \\pi \\in [0,1]\\]\n\nintegrals can be tough to compute, even with numerical methods\nvery low interpretability\ndifficult to compute sample statistics for the posterior distribution (such as mean and variance)"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#conjugate-priors",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#conjugate-priors",
    "title": "5: Conjugate Families",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\nConjugate families have both computational ease and interpretable posterior distributions.\n\n\n\n\n\n\nConjugate Priors\n\n\n\n\n\nLet the prior model for parameter \\(\\theta\\) have pdf \\(f(\\theta)\\) and the model of data Y conditioned on \\(\\theta\\) have likelihood function \\(L(\\theta|y)\\). If the resulting posterior model with pdf \\(f(\\theta|y) \\propto f(\\theta)L(\\theta|y)\\) is of the same model family as the prior, then we say this is a conjugate prior."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#poisson-process",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#poisson-process",
    "title": "5: Conjugate Families",
    "section": "Poisson Process",
    "text": "Poisson Process\n\nMotivationGoalInfinitessimalPartial Proof\n\n\n\nAssume a constant \\(\\lambda\\) of arrivals\nLet \\(N_{t}\\) be the number of arrivals in time interval \\([0,t]\\)\nHomogeneity: \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nIndependence: numbers of arrivals in disjoint time intervals are independent random variables\n\n\n\nerive distribution of number of arrivals\n\nWe expect \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nPartition time interval \\([0,t]\\) into \\(n\\) subintervals\nAssuming \\(n\\) is large enough so that each subinterval has zero or one arrival (i.e. Bernoulli trial)\nProbability of arrival in a random subinterval: \\(p = \\displaystyle\\frac{\\lambda t}{n}\\)\n\nSo far, we are assuming \\(N_{t} \\sim \\text{Bin}(n,p)\\)\n\\[P(N_{t} = k) = \\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k} \\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n-k}\\]\n\n\nHowever,\n\n\\(n\\) was arbitrary\ntime is a continuous variable\n\nSo let’s take the limit as \\(n\\) goes to infinity.\n\\[\\displaystyle\\lim_{n \\to \\infty} P(N_{t} = k) = \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}}\\]\n\n\nHandling the limit by its factors: \\[\\displaystyle\\lim_{n \\to \\infty} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}} = 1, \\quad \\displaystyle\\lim_{n \\to \\infty} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} = e^{-\\lambda t}\\]\n\\[\\begin{array}{rcl}\n  \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\binom{n}{k} \\left(\\displaystyle\\frac{1}{n}\\right)^{k} \\\\\n  ~ & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{k!(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#poisson-distribution",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#poisson-distribution",
    "title": "5: Conjugate Families",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nLet discrete random variable \\(Y\\) be the number of independent events that occur in a fixed amount of time or space, where \\(\\lambda&gt;0\\) is the rate at which these events occur. Then the dependence of \\(Y\\) on parameter \\(\\lambda\\) can be modeled by the Poisson.\n\\[Y|\\lambda \\sim \\text{Pois}(\\lambda)\\]\nwith probability mass function\n\\[f(y|\\lambda) = \\displaystyle\\frac{\\lambda^{y}e^{-\\lambda}}{y!} \\text{ for } y \\in \\{0, 1, 2, ...\\}\\]\n\n\\(f(y|\\lambda) \\geq 0\\)\n\\(\\displaystyle\\sum_{y=0}^{\\infty} \\! f(y|\\lambda) = 1\\)\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\nThe Poisson distribution has the curious property where the randomness has equal mean and variance:\n\\[\\text{E}(Y|\\lambda) = \\text{Var}(Y|\\lambda) = \\lambda\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#guidance",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#guidance",
    "title": "5: Conjugate Families",
    "section": "Guidance",
    "text": "Guidance\n\nPlotCodeGuidance\n\n\n\n\n\n\n\n\n\n\ny_i &lt;- 0:10\nf_y &lt;- dpois(y_i, 1)\ndf_for_plots &lt;- data.frame(y_i,f_y)\n\np1 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 1))) + \n  geom_col() + \n  labs(title = \"lambda = 1\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np2 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 2))) + \n  geom_col() + \n  labs(title = \"lambda = 2\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np3 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 3))) + \n  geom_col() + \n  labs(title = \"lambda = 3\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np4 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 4))) + \n  geom_col() + \n  labs(title = \"lambda = 4\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np5 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 5))) + \n  geom_col() + \n  labs(title = \"lambda = 5\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\n# patchwork\np1 + p2 + p3 + p4 + p5\n\n\n\nThe Poisson distribution is a discrete distribution that tends to be used to model rare events."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#joint-pmf",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#joint-pmf",
    "title": "5: Conjugate Families",
    "section": "Joint PMF",
    "text": "Joint PMF\nLet \\((Y_1,Y_2,…,Y_n)\\) be an independent sample of random variables and \\(\\vec{y} = (y_1,y_2,…,y_n)\\) be the corresponding vector of observed values.\n\n\n\n\n\n\nJoint Probability Mass Function\n\n\n\n\n\nFurther, let \\(f(y_i|\\lambda)\\) denote the pmf of an individual observed data point \\(Y_i=y_i\\). Then by the assumption of independence, the following joint pmf specifies the randomness in and plausibility of the collective sample:\n\\[f(\\vec{y}|\\lambda) = \\displaystyle\\prod_{i=1}^{n} f(y_{i}|\\lambda) = f(y_{1}|\\lambda) \\cdot (y_{2}|\\lambda) \\cdots f(y_{n}|\\lambda)\\]\n\n\n\nThe Poisson probability mass function is then\n\\[\\begin{array}{rcl}\n  f(\\vec{y}|\\lambda) & = & \\displaystyle\\prod_{i=1}^{n} f(y_{i}|\\lambda) \\\\\n  ~ & = & \\displaystyle\\prod_{i=1}^{n} \\displaystyle\\frac{\\lambda^{y_{i}}e^{\\lambda}}{y_{i}!} \\\\\n  ~ & = & \\displaystyle\\frac{\\lambda^{y_{1}}e^{\\lambda}}{y_{1}!} \\cdot \\displaystyle\\frac{\\lambda^{y_{2}}e^{\\lambda}}{y_{2}!} \\cdots \\displaystyle\\frac{\\lambda^{y_{n}}e^{\\lambda}}{y_{n}!} \\\\\n  ~ & = & \\displaystyle\\frac{ [\\lambda^{y_{1}}\\lambda^{y_{2}}\\cdots\\lambda^{y_{n}}][e^{-\\lambda}e^{-\\lambda} \\cdots e^{-\\lambda}] }{ y_{1}!y_{2}! \\cdots y_{n}! } \\\\\n  ~ & = & \\displaystyle\\frac{\\lambda^{\\sum y_{i}}e^{-n\\lambda}}{\\prod y_{i}!} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#poisson-likelihood",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#poisson-likelihood",
    "title": "5: Conjugate Families",
    "section": "Poisson Likelihood",
    "text": "Poisson Likelihood\nThe Poisson likelihood function is then\n\\[L(\\lambda|\\vec{y}) = \\displaystyle\\frac{\\lambda^{\\sum y_{i}}e^{-n\\lambda}}{\\prod y_{i}!}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#parameter-selection",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#parameter-selection",
    "title": "5: Conjugate Families",
    "section": "Parameter Selection",
    "text": "Parameter Selection\nHow do we fit a Poisson model with our data? One idea is to seek the maximum likelihood estimate (MLE).\nClaim: The MLE for the \\(\\text{Pois}(\\lambda)\\) distribution is \\[\\lambda^{*} = \\bar{y} = \\displaystyle\\frac{\\sum y_{i}}{n}\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the Poisson distribution’s PMF \\(f(y) = \\displaystyle\\frac{\\lambda^{y}e^{-\\lambda}}{y!}\\), the likelihood function\n\\[L(\\lambda) = \\displaystyle\\frac{\\lambda^{y_{1}}e^{-\\lambda}}{y_{1}!} \\cdot \\displaystyle\\frac{\\lambda^{y_{2}}e^{-\\lambda}}{y_{2}!} \\cdots \\displaystyle\\frac{\\lambda^{y_{n}}e^{-\\lambda}}{y_{n}!} \\]\nTaking the natural logarithm of both sides, we create the log likelihood function \\(\\ell(\\lambda)\\)\n$$\n\\[\\begin{array}{rcl}\n  \\ln L(\\lambda) & = & \\ln \\left(\\displaystyle\\frac{\\lambda^{y_{1}}e^{-\\lambda}}{y_{1}!} \\cdot \\displaystyle\\frac{\\lambda^{y_{2}}e^{-\\lambda}}{y_{2}!} \\cdots \\displaystyle\\frac{\\lambda^{y_{n}}e^{-\\lambda}}{y_{n}!}\\right) \\\\\n  \\ell(\\lambda) & = & \\ln \\displaystyle\\prod_{i=1}^{n} \\displaystyle\\frac{\\lambda^{y_{i}}e^{-\\lambda}}{y_{i}!} \\\\\n  \\ell(\\lambda) & = & \\displaystyle\\sum_{i=1}^{n} \\ln \\displaystyle\\frac{\\lambda^{y_{i}}e^{-\\lambda}}{y_{i}!} \\\\\n  \n  \\ell(\\lambda) & = & \\displaystyle\\sum_{i=1}^{n} \\left( y_{i}\\ln \\lambda + \\ln e^{-\\lambda} - \\ln y_{i}! \\right) \\\\\n  \n  \\ell(\\lambda) & = & (\\ln \\lambda)\\left(\\displaystyle\\sum_{i=1}^{n} y_{i}\\right) -  \\displaystyle\\sum_{i=1}^{n}\\lambda -  \\displaystyle\\sum_{i=1}^{n} \\ln y_{i}! \\\\\n  \n  \\ell(\\lambda) & = &  (\\ln \\lambda)\\left(\\displaystyle\\sum_{i=1}^{n} y_{i}\\right) - n\\lambda - \\displaystyle\\sum_{i=1}^{n} \\ln (y_{i}!) \\\\\n\\end{array}\\]\n$$\nThe motivation for the logarithm usage is to ease the process of taking the derivative. Here, taking the derivative with respect to \\(\\lambda\\),\n\\[0 = \\ell'(\\lambda)  \\quad\\Rightarrow\\quad 0 = -n + \\displaystyle\\frac{ \\sum_{i=1}^{n} y_{i} }{ \\lambda } \\quad\\Rightarrow\\quad \\lambda = \\displaystyle\\frac{ \\sum_{i=1}^{n} y_{i} }{ n } = \\bar{y}\\]\nThat is, the optimal value for parameter \\(\\lambda\\) is the sample mean \\(\\bar{y}\\)."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety",
    "title": "5: Conjugate Families",
    "section": "Example: Campus Safety",
    "text": "Example: Campus Safety\n\nDataLikelihoodCode\n\n\nThe following data on arrests for drug law violations come from the Princeton University Annual Security and Fire Safety Report (in and around the main campus)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n\n\n\n\narrests\n18\n14\n23\n22\n12\n22\n7\n0\n1\n\n\n\nOur maximum likelihood estimate is\n\\[\\lambda^{*} = \\displaystyle\\frac{\\sum y_{i}}{n} = \\displaystyle\\frac{119}{9} \\approx 13.2222 \\text{ arrests per year}\\]\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_poisson_likelihood(\n  y = c(18, 14, 23, 22, 12, 22, 7, 0, 1),\n  lambda_upper_bound = 20\n) +\n  labs(title = \"Likelihood Curve\",\n       subtitle = \"Arrests per year for drug law violations\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#terminology",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#terminology",
    "title": "5: Conjugate Families",
    "section": "Terminology",
    "text": "Terminology\nLet \\(\\lambda &gt; 0\\) be a continuous random variable. For modeling, we might try a Gamma model \\[\\lambda \\sim \\text{Gamma}(s, r)\\]\n\n\\(s\\): shape parameter\n\\(r\\): rate parameter\n\n\n\n\n\n\n\nExplore!\n\n\n\n\n\nMatt Bognar at the University of Iowa created this great webapp to explore the gamma distribution.\n\n\n\n\n\n\n\n\n\nExponential Model\n\n\n\n\n\nThe Gamma model is a generalization of the exponential model. When the shape parameter \\(s = 1\\), then\n\\[\\lambda \\sim \\text{Gamma}(1,r) = \\text{Exp}(r)\\]\nwhere \\(r\\) is once again the rate parameter."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function",
    "title": "5: Conjugate Families",
    "section": "Probablity Density Function",
    "text": "Probablity Density Function\nThe Gamma model has a continuous pdf\n\\[f(\\lambda) = \\displaystyle\\frac{r^{s}}{\\Gamma(s)} \\lambda^{s-1}e^{-r\\lambda} \\text{ for } \\lambda &gt; 0\\]\nwhere the gamma function\n\n\\(\\Gamma(z) = \\displaystyle\\int_{0}^{\\infty} \\! x^{z-1}e^{-x} \\, dx\\)\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\nFormulas for the Gamma model include\n\\[\\begin{array}{rcl}\n  \\text{E}(\\lambda) & = & \\displaystyle\\frac{s}{r} \\\\\n  \\text{Mode}(\\lambda) & = & \\displaystyle\\frac{s-1}{r} \\\\\n  \\text{Var}(\\lambda) & = & \\displaystyle\\frac{s}{r^{2}} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior",
    "title": "5: Conjugate Families",
    "section": "Tuning the Prior",
    "text": "Tuning the Prior\n\nExample: Campus SafetyStatisticsPlotCode\n\n\nSuppose that a parent of an university applicant feels that the university has arrests for drug law violations with counts between 10 and 30 per year. Matching some statistics formulas\n\\[[\\mu - 2\\sigma, \\mu + 2\\sigma] = [10, 30] \\quad\\rightarrow\\quad \\mu = 20, \\quad \\sigma = 5\\]\n\n\n\\[\\text{E}(\\lambda) = \\displaystyle\\frac{s}{r} = 20 \\text{ and } \\text{Var}(\\lambda) = \\displaystyle\\frac{s}{r^{2}} = 5^{2}\\]\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_gamma(16, 0.8, mean = TRUE) +\n  labs(title = \"Gamma(16, 0.8) Prior\",\n       subtitle = \"mean = 20, sd = 5\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#gamma-poisson-bayesian-model",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#gamma-poisson-bayesian-model",
    "title": "5: Conjugate Families",
    "section": "Gamma-Poisson Bayesian Model",
    "text": "Gamma-Poisson Bayesian Model\nLet \\(\\lambda &gt; 0\\) be an unknown rate parameter and let \\(\\{Y_{1}, Y_{2}, ..., Y_{n}\\}\\) be an i.i.d. sample from a \\(\\text{Pois}(\\lambda)\\) distribution. With a setup of a Gamma prior and Poisson likelihood\n\\[\\begin{array}{rcl}\n  \\lambda & \\sim & \\text{Gamma}(s,r) \\\\\n  Y_{i}|\\lambda & \\sim & \\text{Pois}(\\lambda) \\\\\n\\end{array}\\]\nand observing data \\(\\vec{y} = \\{y_{1}, y_{2}, ..., y_{n}\\}\\), the posterior distribution also has a Gamma structure with updated parameters\n\\[\\lambda|\\vec{y} \\sim \\text{Gamma}\\left( s + \\displaystyle\\sum_{i=1}^{n} y_{i}, r + n \\right)\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\\begin{array}{rcl}\n  f(\\lambda|\\vec{y}) & \\propto & f(\\lambda) \\cdot L(\\lambda|\\vec{y}) \\\\\n  ~ & = & \\displaystyle\\frac{r^{s}}{\\Gamma(s)}\\lambda^{s-1}e^{-r\\lambda} \\cdot \\displaystyle\\frac{\\lambda^{\\sum y_{i}}e^{-n\\lambda}}{\\prod y_{i}!} \\\\\n  ~ & \\propto & \\lambda^{s-1}e^{-r\\lambda} \\cdot \\lambda^{\\sum y_{i}}e^{-n\\lambda} \\\\\n  ~ & = & \\lambda^{s+\\sum y_{i} - 1}e^{-(r+n)\\lambda} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety-2",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety-2",
    "title": "5: Conjugate Families",
    "section": "Example: Campus Safety",
    "text": "Example: Campus Safety\n\nRecapPlotsCodeStatistics\n\n\n\nwe tuned a \\(\\text{Gamma}(16, 0.8)\\) prior\nwe observed 119 arrests for drug law violations over a \\(n = 9\\) year time span\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_gamma_poisson(shape = 16, rate = 0.8,\n                               sum_y = 119, n = 9) +\n  labs(title = \"Gamma-Poisson Model\",\n       subtitle = \"Drug Law Violations Example\",\n       caption = \"SML 320\",\n       x = \"arrests for drug law violations\") +\n  theme_minimal()\n\n\n\n\nbayesrules::summarize_gamma_poisson(shape = 16, rate = 0.8,\n                                    sum_y = 119, n = 9) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model shape rate    mean    mode     var     sd\n1     prior    16  0.8 20.0000 18.7500 25.0000 5.0000\n2 posterior   135  9.8 13.7755 13.6735  1.4057 1.1856"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#terminology-1",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#terminology-1",
    "title": "5: Conjugate Families",
    "section": "Terminology",
    "text": "Terminology\nLet \\(Y &gt; 0\\) be a continuous random variable over all real numbers \\(())-\\infty, \\infty)\\). For modeling, we might try a normal distribution \\[Y \\sim \\text{N}(\\mu, \\sigma^{2})\\]\n\n\\(\\mu\\): mean\n\\(\\sigma\\): standard deviation"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function-1",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function-1",
    "title": "5: Conjugate Families",
    "section": "Probablity Density Function",
    "text": "Probablity Density Function\nThe normal distribution has a continuous probability density function\n\\[f(y) = \\displaystyle\\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} \\text{exp}\\left[ -\\displaystyle\\frac{(y-\\mu)^{2}}{2\\sigma^{2}}\\right] \\text{ for } y \\in (-\\infty, \\infty)\\]\n\n\n\n\n\n\nStatistics\n\n\n\n\n\nDescriptions of normal distributions are dictated by their statistics\n\\[\\begin{array}{rcl}\n  \\text{E}(Y) & = & \\mu \\\\\n  \\text{Mode}(Y) & = & \\mu \\\\\n  \\text{Var}(Y) & = & \\sigma^{2} \\\\\n  \\text{SD}(Y) & = & \\sigma \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior-1",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior-1",
    "title": "5: Conjugate Families",
    "section": "Tuning the Prior",
    "text": "Tuning the Prior\n\nExample: TipsStatisticsPlotCode\n\n\n\nhead(tips_df)\n\n# A tibble: 6 × 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n3       21.0  3.5  Male   No     Sun   Dinner     3\n4       23.7  3.31 Male   No     Sun   Dinner     2\n5       24.6  3.61 Female No     Sun   Dinner     4\n6       25.3  4.71 Male   No     Sun   Dinner     4\n\n\n\n\nLet us guess that Americans tend to tip between 5 and 25 percent of the total bill.\n\\[[\\mu - 2\\sigma, \\mu + 2\\sigma] = [5, 25] \\quad\\rightarrow\\quad \\mu = 15, \\quad \\sigma = 5\\]\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_normal(mean = 15, sd = 5) +\n  labs(title = \"N(15, 25) Prior\",\n       subtitle = \"mean = 15, sd = 5\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#likelihood-2",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#likelihood-2",
    "title": "5: Conjugate Families",
    "section": "Likelihood",
    "text": "Likelihood\nIn this conjugate prior relationship, the likelihood is also modeled as a normal distribution.\n\\[L(\\mu, \\sigma|\\vec{y}) \\propto \\displaystyle\\prod_{i=1}^{n} \\text{exp}\\left[-\\displaystyle\\frac{(y_{i} - \\mu)^{2}}{2\\sigma^{2}}\\right] = \\text{exp}\\left[-\\displaystyle\\frac{\\sum_{i=1}^{n} (y_{i}-\\mu)^{2}}{2\\sigma^{2}}\\right]\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#mles",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#mles",
    "title": "5: Conjugate Families",
    "section": "MLEs",
    "text": "MLEs\nThe likelihood can also be expressed in terms of the sample mean \\(\\bar{y}\\) and the sample size \\(n\\)\n\\[L(\\mu, \\sigma|\\vec{y}) \\propto \\text{exp}\\left[-\\displaystyle\\frac{ (\\bar{y}-\\mu)^{2}}{\\frac{2\\sigma^{2}}{n}}\\right]\\]\nIt follows that the maximum likelihood estimates for the parameters are\n\\[\\begin{array}{rcl}\n  \\mu^{*} & = & \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} y_{i} \\\\\n  \\sigma^{*} & = & \\sqrt{\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (y_{i} - \\mu^{2})^{2}} \\\\\n\\end{array}\\]\nwhich are the sample mean and from the not-corrected population variance (source)."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#dplyr",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#dplyr",
    "title": "5: Conjugate Families",
    "section": "dplyr",
    "text": "dplyr\n\nn &lt;- nrow(tips_df)\ntips_df |&gt;\n  mutate(tips_pct = tip/total_bill * 100) |&gt;\n  summarize(mu = mean(tips_pct, na.rm = TRUE),\n            sigma = sqrt(var(tips_pct, na.rm = TRUE) *(n-1)/(n)))\n\n# A tibble: 1 × 2\n     mu sigma\n  &lt;dbl&gt; &lt;dbl&gt;\n1  16.1  6.09"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#plot-4",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#plot-4",
    "title": "5: Conjugate Families",
    "section": "Plot",
    "text": "Plot"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#code-6",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#code-6",
    "title": "5: Conjugate Families",
    "section": "Code",
    "text": "Code\n\nbayesrules::plot_normal(mean = 16.08026, sd = 6.094693  ) +\n  labs(title = \"Normal \",\n       subtitle = \"MLEs: ybar = 16.08026, sigma = 6.094693\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#normal-normal-conjugacy",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#normal-normal-conjugacy",
    "title": "5: Conjugate Families",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\nLet \\(\\mu \\in (-\\infty, \\infty)\\) be an unknown mean parameter and let \\(\\sigma^{2} &gt; 0\\) be an unknown variance parameter and let \\(\\{Y_{1}, Y_{2}, ..., Y_{n}\\}\\) be an i.i.d. sample from a \\(\\text{N}(\\mu, \\sigma^{2})\\) distribution. With a setup of a normal prior and normal likelihood\n\\[\\begin{array}{rcl}\n  \\mu,\\sigma^{2} & \\sim & \\text{N}(\\theta,\\tau^{2}) \\\\\n  Y_{i}|\\mu, \\sigma^{2} & \\sim & \\text{N}(\\mu,\\sigma^{2}) \\\\\n\\end{array}\\]\nand observing data \\(\\vec{y} = \\{y_{1}, y_{2}, ..., y_{n}\\}\\), the posterior distribution also has a normal structure with updated parameters\n\\[\\mu,\\sigma^{2}|\\vec{y} \\sim \\text{N}\\left( \\displaystyle\\frac{\\sigma^{2}}{n\\tau^{2}+\\sigma^{2}} \\cdot \\theta + \\displaystyle\\frac{n\\tau^{2}}{n\\tau^{2}+\\sigma^{2}} \\cdot \\bar{y}, \\quad \\displaystyle\\frac{\\tau^{2}\\sigma^{2}}{n\\tau^{2}+\\sigma^{2}} \\right)\\]\n\nWhat happens if we have relatively small data sets?\nWhat happens if we have relatively large data sets?"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#example",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#example",
    "title": "5: Conjugate Families",
    "section": "Example",
    "text": "Example\n\nCodePlotsStatistics\n\n\n\nbayesrules::plot_normal_normal(\n  \n  # from prior\n  mean = 15, sd = 5,\n  \n  # from observations\n  y_bar = 16.08026, sigma = 6.094693, n = 244\n) +\n  labs(title = \"Normal-Normal Model\",\n       subtitle = \"Restaurant Tips Example\",\n       caption = \"SML 320\",\n       x = \"percent of total food bill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_normal_normal(\n  \n  # from prior\n  mean = 15, sd = 5,\n  \n  # from observations\n  y_bar = 16.08026, sigma = 6.094693, n = 244\n) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model    mean    mode     var    sd\n1     prior 15.0000 15.0000 25.0000 5.000\n2 posterior 16.0737 16.0737  0.1513 0.389"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#model-selection",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#model-selection",
    "title": "5: Conjugate Families",
    "section": "Model Selection",
    "text": "Model Selection\nWe looked at 3 conjugate families.\n\nBeta-BinomialGamma-PoissonNormal-Normal\n\n\n\nestimate \\(\\pi \\in [0,1]\\)\npro: good for interpretability\ncon: computationally expensive for large \\(n\\)\n\n\n\n\nestimate \\(\\lambda &gt; 0\\)\npro: models rare events and skewed data well\ncon: discussion of rates instead of counts\n\n\n\n\nestimate mean \\(\\mu\\) and variance \\(\\sigma\\)\npro: ubiquitous in scientific communities\ncons:\n\ninfinite support may lead to suboptimal results in larger networks\nwas the data symmetric?"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html",
    "href": "posts/06_approx_posterior/06_approx_posterior.html",
    "title": "6: Approximating the Posterior",
    "section": "",
    "text": "library(\"bayesplot\")\nlibrary(\"ggtext\")\nlibrary(\"rstan\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# tips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#setting-noaa",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#setting-noaa",
    "title": "6: Approximating the Posterior",
    "section": "Setting: NOAA",
    "text": "Setting: NOAA\nI will be studying weather data for a semester theme. The main data will probably come from NOAA (National Oceanic and Atmospheric Administration), where the data consists of several readings (variables) from several research stations over many years.\nLet \\(\\theta\\) represent the high temperature recorded. Across \\(k\\) research stations,\n\\[\\vec{\\theta} = (\\theta_{1}, \\theta_{2}, ..., \\theta_{k})\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#bayesian-approach",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#bayesian-approach",
    "title": "6: Approximating the Posterior",
    "section": "Bayesian Approach",
    "text": "Bayesian Approach\nIf we have a vector of parameters to estimate,\n\\[f(\\vec{\\theta} | \\vec{y} ) \\propto \\text{prior}*\\text{likelihood} = f(\\vec{\\theta}) \\cdot L(\\vec{\\theta} | \\vec{y})\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#normalizing-constant",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#normalizing-constant",
    "title": "6: Approximating the Posterior",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\\[f(y) = \\displaystyle\\int_{\\theta_{1}}\\int_{\\theta_{2}} \\cdots \\int_{\\theta_{k}} \\! f(\\vec{\\theta}) \\cdot L(\\vec{\\theta} | \\vec{y} ) \\, d\\theta_{k} \\cdots d\\theta_{2} \\, d\\theta_{1}\\]\n\nclosed form solution probably does not exist\nvery expensive computationally"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#conjugate-priors",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#conjugate-priors",
    "title": "6: Approximating the Posterior",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\nFor today, we will revisit the conjugate priors where we know a lot about the posterior distributions.\n\nBeta-Binomial\n\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Beta}(\\alpha, \\beta) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Bin}(n, \\pi) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Beta}(\\alpha + y, \\beta + n - y) \\\\\n\\end{array}\\]\n\nGamma-Poisson\n\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Gamma}(s, r) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Pois}(\\lambda) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Gamma}\\left(s + \\displaystyle\\sum_{i=1}^{n} y, r + n\\right) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#broad-idea",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#broad-idea",
    "title": "6: Approximating the Posterior",
    "section": "Broad Idea",
    "text": "Broad Idea\nAs we gather pieces, the overall picture might become clear.\n\n\n\nArt of Ellis Rowan\n\n\n\nimage source: National Museum of Australia"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#math-definitions",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#math-definitions",
    "title": "6: Approximating the Posterior",
    "section": "Math Definitions",
    "text": "Math Definitions\nGrid approximation produces a sample of \\(N\\) independent \\(\\theta\\) values, \\(\\{\\theta^(1),\\theta^(2),…,\\theta^(N)\\}\\), from a discretized approximation of posterior pdf \\(f(\\theta|y)\\). This algorithm evolves in four steps:\n\nDefine a discrete grid of possible \\(\\theta\\) values.\nEvaluate the prior pdf \\(f(\\theta)\\) and likelihood function \\(L(\\theta|y)\\) at each \\(\\theta\\) grid value.\nObtain a discrete approximation of the posterior pdf \\(f(\\theta|y)\\) by:\n\ncalculating the product \\(f(\\theta)L(\\theta|y)\\) at each \\(\\theta\\) grid value; and then\nnormalizing the products so that they sum to 1 across all \\(\\theta\\).\n\nRandomly sample \\(N\\) \\(\\theta\\) grid values with respect to their corresponding normalized posterior probabilities."
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#example-beta-binomial",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#example-beta-binomial",
    "title": "6: Approximating the Posterior",
    "section": "Example: Beta-Binomial",
    "text": "Example: Beta-Binomial\n\nScenario: Smokers in Restaurants\nLet us start with a vague beta prior, use a binomial model to get the likelihood of \\(y = 4\\) smokers among \\(n = 9\\) customers, and then get a beta posterior.\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Beta}(3, 3) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Bin}(9, \\pi) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Beta}(7, 8) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#sparse-grid",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#sparse-grid",
    "title": "6: Approximating the Posterior",
    "section": "Sparse Grid",
    "text": "Sparse Grid\nHere we will try this grid approximation idea over \\(N = 5\\) values\n\\[\\pi \\in \\{0, 0.25, 0.50, 0.75, 1.0\\}\\]\n\nBayesGrid DataGrid ValuesGraph 1 CodePosterior SamplingAlignmentGraph 2 Code\n\n\n\n# Step 1: Define a grid of 6 pi values\ngrid_data &lt;- data.frame(pi_grid = seq(from = 0, to = 1, \n                                      length = 5))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dbeta(pi_grid, 3, 3),\n         likelihood = dbinom(4, 9, pi_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\n\n\nround(grid_data, 4)\n\n  pi_grid  prior likelihood unnormalized posterior\n1    0.00 0.0000     0.0000       0.0000    0.0000\n2    0.25 1.0547     0.1168       0.1232    0.1969\n3    0.50 1.8750     0.2461       0.4614    0.7375\n4    0.75 1.0547     0.0389       0.0411    0.0656\n5    1.00 0.0000     0.0000       0.0000    0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot the grid approximated posterior\nggplot(grid_data, aes(x = pi_grid, y = posterior)) + \n  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior),\n               color = \"gray50\",\n               linewidth = 2) +\n  geom_point(size = 7) + \n  labs(title = \"Sparse Grid\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal()\n\n\n\n\n# Step 4: sample from the discretized posterior\nposterior_sample &lt;- sample_n(grid_data, \n                             size = 10000, \n                             weight = posterior, \n                             replace = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nggplot(posterior_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.1,\n                 fill = \"gray50\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  lims(x = c(0, 1)) +\n  labs(title = \"Sparse Grid: &lt;span style='color:#7F7F7F'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#dense-grid",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#dense-grid",
    "title": "6: Approximating the Posterior",
    "section": "Dense Grid",
    "text": "Dense Grid\nHere we will try this grid approximation idea over \\(N = 101\\) values\n\\[\\pi \\in [0,1]\\]\n\nBayesGrid DataGrid ValuesGraph 1 CodePosterior SamplingAlignmentGraph 2 Code\n\n\n\n# Step 1: Define a grid of 6 pi values\ngrid_data &lt;- data.frame(pi_grid = seq(from = 0, to = 1, \n                                      length = 101))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dbeta(pi_grid, 3, 3),\n         likelihood = dbinom(4, 9, pi_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\n\n\nround(grid_data, 4)\n\n    pi_grid  prior likelihood unnormalized posterior\n1      0.00 0.0000     0.0000       0.0000    0.0000\n2      0.01 0.0029     0.0000       0.0000    0.0000\n3      0.02 0.0115     0.0000       0.0000    0.0000\n4      0.03 0.0254     0.0001       0.0000    0.0000\n5      0.04 0.0442     0.0003       0.0000    0.0000\n6      0.05 0.0677     0.0006       0.0000    0.0000\n7      0.06 0.0954     0.0012       0.0001    0.0000\n8      0.07 0.1271     0.0021       0.0003    0.0000\n9      0.08 0.1625     0.0034       0.0006    0.0000\n10     0.09 0.2012     0.0052       0.0010    0.0001\n11     0.10 0.2430     0.0074       0.0018    0.0001\n12     0.11 0.2875     0.0103       0.0030    0.0002\n13     0.12 0.3345     0.0138       0.0046    0.0003\n14     0.13 0.3837     0.0179       0.0069    0.0004\n15     0.14 0.4349     0.0228       0.0099    0.0006\n16     0.15 0.4877     0.0283       0.0138    0.0009\n17     0.16 0.5419     0.0345       0.0187    0.0012\n18     0.17 0.5973     0.0415       0.0248    0.0016\n19     0.18 0.6536     0.0490       0.0320    0.0020\n20     0.19 0.7106     0.0573       0.0407    0.0026\n21     0.20 0.7680     0.0661       0.0507    0.0032\n22     0.21 0.8257     0.0754       0.0623    0.0040\n23     0.22 0.8834     0.0852       0.0753    0.0048\n24     0.23 0.9409     0.0954       0.0898    0.0057\n25     0.24 0.9981     0.1060       0.1058    0.0067\n26     0.25 1.0547     0.1168       0.1232    0.0078\n27     0.26 1.1105     0.1278       0.1419    0.0090\n28     0.27 1.1655     0.1388       0.1618    0.0103\n29     0.28 1.2193     0.1499       0.1827    0.0116\n30     0.29 1.2718     0.1608       0.2045    0.0130\n31     0.30 1.3230     0.1715       0.2269    0.0144\n32     0.31 1.3726     0.1820       0.2498    0.0159\n33     0.32 1.4205     0.1921       0.2729    0.0173\n34     0.33 1.4666     0.2017       0.2959    0.0188\n35     0.34 1.5107     0.2109       0.3185    0.0202\n36     0.35 1.5527     0.2194       0.3406    0.0216\n37     0.36 1.5925     0.2272       0.3619    0.0230\n38     0.37 1.6301     0.2344       0.3820    0.0243\n39     0.38 1.6652     0.2407       0.4008    0.0255\n40     0.39 1.6979     0.2462       0.4180    0.0266\n41     0.40 1.7280     0.2508       0.4334    0.0275\n42     0.41 1.7555     0.2545       0.4468    0.0284\n43     0.42 1.7802     0.2573       0.4581    0.0291\n44     0.43 1.8022     0.2592       0.4671    0.0297\n45     0.44 1.8214     0.2601       0.4737    0.0301\n46     0.45 1.8377     0.2600       0.4779    0.0304\n47     0.46 1.8511     0.2590       0.4795    0.0305\n48     0.47 1.8615     0.2571       0.4786    0.0304\n49     0.48 1.8690     0.2543       0.4753    0.0302\n50     0.49 1.8735     0.2506       0.4695    0.0298\n51     0.50 1.8750     0.2461       0.4614    0.0293\n52     0.51 1.8735     0.2408       0.4511    0.0287\n53     0.52 1.8690     0.2347       0.4387    0.0279\n54     0.53 1.8615     0.2280       0.4245    0.0270\n55     0.54 1.8511     0.2207       0.4085    0.0260\n56     0.55 1.8377     0.2128       0.3910    0.0248\n57     0.56 1.8214     0.2044       0.3722    0.0237\n58     0.57 1.8022     0.1955       0.3524    0.0224\n59     0.58 1.7802     0.1863       0.3317    0.0211\n60     0.59 1.7555     0.1769       0.3105    0.0197\n61     0.60 1.7280     0.1672       0.2889    0.0184\n62     0.61 1.6979     0.1574       0.2673    0.0170\n63     0.62 1.6652     0.1475       0.2457    0.0156\n64     0.63 1.6301     0.1376       0.2244    0.0143\n65     0.64 1.5925     0.1278       0.2036    0.0129\n66     0.65 1.5527     0.1181       0.1834    0.0117\n67     0.66 1.5107     0.1086       0.1641    0.0104\n68     0.67 1.4666     0.0994       0.1457    0.0093\n69     0.68 1.4205     0.0904       0.1284    0.0082\n70     0.69 1.3726     0.0818       0.1122    0.0071\n71     0.70 1.3230     0.0735       0.0973    0.0062\n72     0.71 1.2718     0.0657       0.0835    0.0053\n73     0.72 1.2193     0.0583       0.0711    0.0045\n74     0.73 1.1655     0.0513       0.0598    0.0038\n75     0.74 1.1105     0.0449       0.0499    0.0032\n76     0.75 1.0547     0.0389       0.0411    0.0026\n77     0.76 0.9981     0.0335       0.0334    0.0021\n78     0.77 0.9409     0.0285       0.0268    0.0017\n79     0.78 0.8834     0.0240       0.0212    0.0013\n80     0.79 0.8257     0.0200       0.0165    0.0011\n81     0.80 0.7680     0.0165       0.0127    0.0008\n82     0.81 0.7106     0.0134       0.0095    0.0006\n83     0.82 0.6536     0.0108       0.0070    0.0004\n84     0.83 0.5973     0.0085       0.0051    0.0003\n85     0.84 0.5419     0.0066       0.0036    0.0002\n86     0.85 0.4877     0.0050       0.0024    0.0002\n87     0.86 0.4349     0.0037       0.0016    0.0001\n88     0.87 0.3837     0.0027       0.0010    0.0001\n89     0.88 0.3345     0.0019       0.0006    0.0000\n90     0.89 0.2875     0.0013       0.0004    0.0000\n91     0.90 0.2430     0.0008       0.0002    0.0000\n92     0.91 0.2012     0.0005       0.0001    0.0000\n93     0.92 0.1625     0.0003       0.0000    0.0000\n94     0.93 0.1271     0.0002       0.0000    0.0000\n95     0.94 0.0954     0.0001       0.0000    0.0000\n96     0.95 0.0677     0.0000       0.0000    0.0000\n97     0.96 0.0442     0.0000       0.0000    0.0000\n98     0.97 0.0254     0.0000       0.0000    0.0000\n99     0.98 0.0115     0.0000       0.0000    0.0000\n100    0.99 0.0029     0.0000       0.0000    0.0000\n101    1.00 0.0000     0.0000       0.0000    0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot the grid approximated posterior\nggplot(grid_data, aes(x = pi_grid, y = posterior)) + \n  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior),\n               color = \"gray50\",\n               linewidth = 1) +\n  geom_point(size = 2) + \n  labs(title = \"Dense Grid\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal()\n\n\n\n\n# Step 4: sample from the discretized posterior\nposterior_sample &lt;- sample_n(grid_data, \n                             size = 10000, \n                             weight = posterior, \n                             replace = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nggplot(posterior_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 color = \"black\", \n                 binwidth = 0.05,\n                 fill = \"gray50\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  lims(x = c(0, 1)) +\n  labs(title = \"Dense Grid: &lt;span style='color:#7F7F7F'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#example-gamma-poisson",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#example-gamma-poisson",
    "title": "6: Approximating the Posterior",
    "section": "Example: Gamma-Poisson",
    "text": "Example: Gamma-Poisson\n\nScenario: Drug Law Violations\nLet us start with a vague Gamma prior, use a binomial model to get the likelihood of \\(\\sum y = 119\\) drug law violations over \\(n = 9\\) years, and then get a Gamma posterior.\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Gamma}(16, 0.8) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Pois}(119/9) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Gamma}(135, 9.8) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#sparse-grid-1",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#sparse-grid-1",
    "title": "6: Approximating the Posterior",
    "section": "Sparse Grid",
    "text": "Sparse Grid\nHere we will try this grid approximation idea over \\(N = 11\\) values\n\\[\\lambda \\in \\{0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30\\}\\]\n\nBayesGrid DataGrid ValuesGraph 1 CodePosterior SamplingAlignmentGraph 2 Code\n\n\n\nobs_counts &lt;- c(18, 14, 23, 22, 12, 22, 7, 0, 1)\n\n# Step 1: Define a grid of 11 pi values\ngrid_data &lt;- data.frame(lambda_grid = seq(from = 0, to = 30, \n                                      length = 11))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dgamma(lambda_grid, 16, 0.8),\n         likelihood = dpois(18, lambda_grid)*\n           dpois(14, lambda_grid)*\n           dpois(23, lambda_grid)*\n           dpois(22, lambda_grid)*\n           dpois(12, lambda_grid)*\n           dpois(22, lambda_grid)*\n           dpois(7, lambda_grid)*\n           dpois(0, lambda_grid)*\n           dpois(1, lambda_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\n\n\nround(grid_data, 4)\n\n   lambda_grid  prior likelihood unnormalized posterior\n1            0 0.0000          0            0    0.0000\n2            3 0.0000          0            0    0.0000\n3            6 0.0001          0            0    0.0000\n4            9 0.0033          0            0    0.0000\n5           12 0.0225          0            0    0.3756\n6           15 0.0579          0            0    0.6200\n7           18 0.0809          0            0    0.0043\n8           21 0.0741          0            0    0.0000\n9           24 0.0498          0            0    0.0000\n10          27 0.0265          0            0    0.0000\n11          30 0.0117          0            0    0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot the grid approximated posterior\nggplot(grid_data, aes(x = lambda_grid, y = posterior)) + \n  geom_segment(aes(x = lambda_grid, xend = lambda_grid, y = 0, yend = posterior),\n               color = \"gray50\",\n               linewidth = 2) +\n  geom_point(size = 7) + \n  labs(title = \"Sparse Grid\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal()\n\n\n\n\n# Step 4: sample from the discretized posterior\nposterior_sample &lt;- sample_n(grid_data, \n                             size = 10000, \n                             weight = posterior, \n                             replace = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nggplot(posterior_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.1,\n                 fill = \"gray50\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  lims(x = c(0, 1)) +\n  labs(title = \"Sparse Grid: &lt;span style='color:#7F7F7F'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#dense-grid-1",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#dense-grid-1",
    "title": "6: Approximating the Posterior",
    "section": "Dense Grid",
    "text": "Dense Grid\nHere we will try this grid approximation idea over \\(N = 501\\) values\n\\[\\lambda \\in [0, 30]\\]\n\nBayesGrid DataGrid ValuesGraph 1 CodePosterior SamplingAlignmentGraph 2 Code\n\n\n\nobs_counts &lt;- c(18, 14, 23, 22, 12, 22, 7, 0, 1)\n\n# Step 1: Define a grid of 11 pi values\ngrid_data &lt;- data.frame(lambda_grid = seq(from = 0, to = 30, \n                                      length = 501))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dgamma(lambda_grid, 16, 0.8),\n         likelihood = dpois(18, lambda_grid)*\n           dpois(14, lambda_grid)*\n           dpois(23, lambda_grid)*\n           dpois(22, lambda_grid)*\n           dpois(12, lambda_grid)*\n           dpois(22, lambda_grid)*\n           dpois(7, lambda_grid)*\n           dpois(0, lambda_grid)*\n           dpois(1, lambda_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\n\n\nround(grid_data, 4)\n\n    lambda_grid  prior likelihood unnormalized posterior\n1          0.00 0.0000          0            0    0.0000\n2          0.06 0.0000          0            0    0.0000\n3          0.12 0.0000          0            0    0.0000\n4          0.18 0.0000          0            0    0.0000\n5          0.24 0.0000          0            0    0.0000\n6          0.30 0.0000          0            0    0.0000\n7          0.36 0.0000          0            0    0.0000\n8          0.42 0.0000          0            0    0.0000\n9          0.48 0.0000          0            0    0.0000\n10         0.54 0.0000          0            0    0.0000\n11         0.60 0.0000          0            0    0.0000\n12         0.66 0.0000          0            0    0.0000\n13         0.72 0.0000          0            0    0.0000\n14         0.78 0.0000          0            0    0.0000\n15         0.84 0.0000          0            0    0.0000\n16         0.90 0.0000          0            0    0.0000\n17         0.96 0.0000          0            0    0.0000\n18         1.02 0.0000          0            0    0.0000\n19         1.08 0.0000          0            0    0.0000\n20         1.14 0.0000          0            0    0.0000\n21         1.20 0.0000          0            0    0.0000\n22         1.26 0.0000          0            0    0.0000\n23         1.32 0.0000          0            0    0.0000\n24         1.38 0.0000          0            0    0.0000\n25         1.44 0.0000          0            0    0.0000\n26         1.50 0.0000          0            0    0.0000\n27         1.56 0.0000          0            0    0.0000\n28         1.62 0.0000          0            0    0.0000\n29         1.68 0.0000          0            0    0.0000\n30         1.74 0.0000          0            0    0.0000\n31         1.80 0.0000          0            0    0.0000\n32         1.86 0.0000          0            0    0.0000\n33         1.92 0.0000          0            0    0.0000\n34         1.98 0.0000          0            0    0.0000\n35         2.04 0.0000          0            0    0.0000\n36         2.10 0.0000          0            0    0.0000\n37         2.16 0.0000          0            0    0.0000\n38         2.22 0.0000          0            0    0.0000\n39         2.28 0.0000          0            0    0.0000\n40         2.34 0.0000          0            0    0.0000\n41         2.40 0.0000          0            0    0.0000\n42         2.46 0.0000          0            0    0.0000\n43         2.52 0.0000          0            0    0.0000\n44         2.58 0.0000          0            0    0.0000\n45         2.64 0.0000          0            0    0.0000\n46         2.70 0.0000          0            0    0.0000\n47         2.76 0.0000          0            0    0.0000\n48         2.82 0.0000          0            0    0.0000\n49         2.88 0.0000          0            0    0.0000\n50         2.94 0.0000          0            0    0.0000\n51         3.00 0.0000          0            0    0.0000\n52         3.06 0.0000          0            0    0.0000\n53         3.12 0.0000          0            0    0.0000\n54         3.18 0.0000          0            0    0.0000\n55         3.24 0.0000          0            0    0.0000\n56         3.30 0.0000          0            0    0.0000\n57         3.36 0.0000          0            0    0.0000\n58         3.42 0.0000          0            0    0.0000\n59         3.48 0.0000          0            0    0.0000\n60         3.54 0.0000          0            0    0.0000\n61         3.60 0.0000          0            0    0.0000\n62         3.66 0.0000          0            0    0.0000\n63         3.72 0.0000          0            0    0.0000\n64         3.78 0.0000          0            0    0.0000\n65         3.84 0.0000          0            0    0.0000\n66         3.90 0.0000          0            0    0.0000\n67         3.96 0.0000          0            0    0.0000\n68         4.02 0.0000          0            0    0.0000\n69         4.08 0.0000          0            0    0.0000\n70         4.14 0.0000          0            0    0.0000\n71         4.20 0.0000          0            0    0.0000\n72         4.26 0.0000          0            0    0.0000\n73         4.32 0.0000          0            0    0.0000\n74         4.38 0.0000          0            0    0.0000\n75         4.44 0.0000          0            0    0.0000\n76         4.50 0.0000          0            0    0.0000\n77         4.56 0.0000          0            0    0.0000\n78         4.62 0.0000          0            0    0.0000\n79         4.68 0.0000          0            0    0.0000\n80         4.74 0.0000          0            0    0.0000\n81         4.80 0.0000          0            0    0.0000\n82         4.86 0.0000          0            0    0.0000\n83         4.92 0.0000          0            0    0.0000\n84         4.98 0.0000          0            0    0.0000\n85         5.04 0.0000          0            0    0.0000\n86         5.10 0.0000          0            0    0.0000\n87         5.16 0.0000          0            0    0.0000\n88         5.22 0.0000          0            0    0.0000\n89         5.28 0.0000          0            0    0.0000\n90         5.34 0.0000          0            0    0.0000\n91         5.40 0.0000          0            0    0.0000\n92         5.46 0.0000          0            0    0.0000\n93         5.52 0.0000          0            0    0.0000\n94         5.58 0.0000          0            0    0.0000\n95         5.64 0.0000          0            0    0.0000\n96         5.70 0.0000          0            0    0.0000\n97         5.76 0.0001          0            0    0.0000\n98         5.82 0.0001          0            0    0.0000\n99         5.88 0.0001          0            0    0.0000\n100        5.94 0.0001          0            0    0.0000\n101        6.00 0.0001          0            0    0.0000\n102        6.06 0.0001          0            0    0.0000\n103        6.12 0.0001          0            0    0.0000\n104        6.18 0.0001          0            0    0.0000\n105        6.24 0.0001          0            0    0.0000\n106        6.30 0.0001          0            0    0.0000\n107        6.36 0.0001          0            0    0.0000\n108        6.42 0.0002          0            0    0.0000\n109        6.48 0.0002          0            0    0.0000\n110        6.54 0.0002          0            0    0.0000\n111        6.60 0.0002          0            0    0.0000\n112        6.66 0.0002          0            0    0.0000\n113        6.72 0.0003          0            0    0.0000\n114        6.78 0.0003          0            0    0.0000\n115        6.84 0.0003          0            0    0.0000\n116        6.90 0.0003          0            0    0.0000\n117        6.96 0.0004          0            0    0.0000\n118        7.02 0.0004          0            0    0.0000\n119        7.08 0.0004          0            0    0.0000\n120        7.14 0.0005          0            0    0.0000\n121        7.20 0.0005          0            0    0.0000\n122        7.26 0.0005          0            0    0.0000\n123        7.32 0.0006          0            0    0.0000\n124        7.38 0.0006          0            0    0.0000\n125        7.44 0.0007          0            0    0.0000\n126        7.50 0.0007          0            0    0.0000\n127        7.56 0.0008          0            0    0.0000\n128        7.62 0.0008          0            0    0.0000\n129        7.68 0.0009          0            0    0.0000\n130        7.74 0.0009          0            0    0.0000\n131        7.80 0.0010          0            0    0.0000\n132        7.86 0.0011          0            0    0.0000\n133        7.92 0.0012          0            0    0.0000\n134        7.98 0.0012          0            0    0.0000\n135        8.04 0.0013          0            0    0.0000\n136        8.10 0.0014          0            0    0.0000\n137        8.16 0.0015          0            0    0.0000\n138        8.22 0.0016          0            0    0.0000\n139        8.28 0.0017          0            0    0.0000\n140        8.34 0.0018          0            0    0.0000\n141        8.40 0.0019          0            0    0.0000\n142        8.46 0.0020          0            0    0.0000\n143        8.52 0.0021          0            0    0.0000\n144        8.58 0.0023          0            0    0.0000\n145        8.64 0.0024          0            0    0.0000\n146        8.70 0.0025          0            0    0.0000\n147        8.76 0.0027          0            0    0.0000\n148        8.82 0.0028          0            0    0.0000\n149        8.88 0.0030          0            0    0.0000\n150        8.94 0.0031          0            0    0.0000\n151        9.00 0.0033          0            0    0.0000\n152        9.06 0.0035          0            0    0.0000\n153        9.12 0.0037          0            0    0.0000\n154        9.18 0.0039          0            0    0.0000\n155        9.24 0.0041          0            0    0.0000\n156        9.30 0.0043          0            0    0.0000\n157        9.36 0.0045          0            0    0.0000\n158        9.42 0.0047          0            0    0.0000\n159        9.48 0.0049          0            0    0.0000\n160        9.54 0.0051          0            0    0.0000\n161        9.60 0.0054          0            0    0.0000\n162        9.66 0.0056          0            0    0.0000\n163        9.72 0.0059          0            0    0.0000\n164        9.78 0.0062          0            0    0.0000\n165        9.84 0.0064          0            0    0.0000\n166        9.90 0.0067          0            0    0.0000\n167        9.96 0.0070          0            0    0.0000\n168       10.02 0.0073          0            0    0.0001\n169       10.08 0.0076          0            0    0.0001\n170       10.14 0.0080          0            0    0.0001\n171       10.20 0.0083          0            0    0.0001\n172       10.26 0.0086          0            0    0.0001\n173       10.32 0.0090          0            0    0.0002\n174       10.38 0.0093          0            0    0.0002\n175       10.44 0.0097          0            0    0.0002\n176       10.50 0.0101          0            0    0.0003\n177       10.56 0.0104          0            0    0.0003\n178       10.62 0.0108          0            0    0.0004\n179       10.68 0.0112          0            0    0.0005\n180       10.74 0.0117          0            0    0.0005\n181       10.80 0.0121          0            0    0.0006\n182       10.86 0.0125          0            0    0.0007\n183       10.92 0.0130          0            0    0.0009\n184       10.98 0.0134          0            0    0.0010\n185       11.04 0.0139          0            0    0.0012\n186       11.10 0.0143          0            0    0.0013\n187       11.16 0.0148          0            0    0.0015\n188       11.22 0.0153          0            0    0.0017\n189       11.28 0.0158          0            0    0.0020\n190       11.34 0.0163          0            0    0.0022\n191       11.40 0.0168          0            0    0.0025\n192       11.46 0.0173          0            0    0.0028\n193       11.52 0.0179          0            0    0.0032\n194       11.58 0.0184          0            0    0.0035\n195       11.64 0.0190          0            0    0.0039\n196       11.70 0.0195          0            0    0.0043\n197       11.76 0.0201          0            0    0.0048\n198       11.82 0.0207          0            0    0.0052\n199       11.88 0.0213          0            0    0.0057\n200       11.94 0.0219          0            0    0.0062\n201       12.00 0.0225          0            0    0.0068\n202       12.06 0.0231          0            0    0.0074\n203       12.12 0.0237          0            0    0.0079\n204       12.18 0.0243          0            0    0.0085\n205       12.24 0.0249          0            0    0.0092\n206       12.30 0.0256          0            0    0.0098\n207       12.36 0.0262          0            0    0.0105\n208       12.42 0.0269          0            0    0.0111\n209       12.48 0.0276          0            0    0.0118\n210       12.54 0.0282          0            0    0.0124\n211       12.60 0.0289          0            0    0.0131\n212       12.66 0.0296          0            0    0.0137\n213       12.72 0.0303          0            0    0.0144\n214       12.78 0.0310          0            0    0.0150\n215       12.84 0.0316          0            0    0.0156\n216       12.90 0.0323          0            0    0.0162\n217       12.96 0.0331          0            0    0.0168\n218       13.02 0.0338          0            0    0.0173\n219       13.08 0.0345          0            0    0.0178\n220       13.14 0.0352          0            0    0.0182\n221       13.20 0.0359          0            0    0.0187\n222       13.26 0.0367          0            0    0.0190\n223       13.32 0.0374          0            0    0.0193\n224       13.38 0.0381          0            0    0.0196\n225       13.44 0.0389          0            0    0.0199\n226       13.50 0.0396          0            0    0.0200\n227       13.56 0.0403          0            0    0.0202\n228       13.62 0.0411          0            0    0.0202\n229       13.68 0.0418          0            0    0.0203\n230       13.74 0.0426          0            0    0.0202\n231       13.80 0.0433          0            0    0.0201\n232       13.86 0.0440          0            0    0.0200\n233       13.92 0.0448          0            0    0.0198\n234       13.98 0.0455          0            0    0.0196\n235       14.04 0.0463          0            0    0.0193\n236       14.10 0.0470          0            0    0.0190\n237       14.16 0.0478          0            0    0.0186\n238       14.22 0.0485          0            0    0.0182\n239       14.28 0.0493          0            0    0.0178\n240       14.34 0.0500          0            0    0.0174\n241       14.40 0.0507          0            0    0.0169\n242       14.46 0.0515          0            0    0.0164\n243       14.52 0.0522          0            0    0.0158\n244       14.58 0.0529          0            0    0.0153\n245       14.64 0.0537          0            0    0.0147\n246       14.70 0.0544          0            0    0.0141\n247       14.76 0.0551          0            0    0.0135\n248       14.82 0.0558          0            0    0.0130\n249       14.88 0.0565          0            0    0.0124\n250       14.94 0.0572          0            0    0.0118\n251       15.00 0.0579          0            0    0.0112\n252       15.06 0.0586          0            0    0.0106\n253       15.12 0.0593          0            0    0.0100\n254       15.18 0.0600          0            0    0.0095\n255       15.24 0.0606          0            0    0.0089\n256       15.30 0.0613          0            0    0.0084\n257       15.36 0.0620          0            0    0.0079\n258       15.42 0.0626          0            0    0.0074\n259       15.48 0.0633          0            0    0.0069\n260       15.54 0.0639          0            0    0.0064\n261       15.60 0.0645          0            0    0.0060\n262       15.66 0.0652          0            0    0.0056\n263       15.72 0.0658          0            0    0.0052\n264       15.78 0.0664          0            0    0.0048\n265       15.84 0.0670          0            0    0.0044\n266       15.90 0.0676          0            0    0.0041\n267       15.96 0.0681          0            0    0.0037\n268       16.02 0.0687          0            0    0.0034\n269       16.08 0.0693          0            0    0.0032\n270       16.14 0.0698          0            0    0.0029\n271       16.20 0.0703          0            0    0.0026\n272       16.26 0.0709          0            0    0.0024\n273       16.32 0.0714          0            0    0.0022\n274       16.38 0.0719          0            0    0.0020\n275       16.44 0.0724          0            0    0.0018\n276       16.50 0.0729          0            0    0.0016\n277       16.56 0.0733          0            0    0.0015\n278       16.62 0.0738          0            0    0.0013\n279       16.68 0.0742          0            0    0.0012\n280       16.74 0.0747          0            0    0.0011\n281       16.80 0.0751          0            0    0.0010\n282       16.86 0.0755          0            0    0.0009\n283       16.92 0.0759          0            0    0.0008\n284       16.98 0.0763          0            0    0.0007\n285       17.04 0.0767          0            0    0.0006\n286       17.10 0.0770          0            0    0.0005\n287       17.16 0.0774          0            0    0.0005\n288       17.22 0.0777          0            0    0.0004\n289       17.28 0.0781          0            0    0.0004\n290       17.34 0.0784          0            0    0.0003\n291       17.40 0.0787          0            0    0.0003\n292       17.46 0.0790          0            0    0.0003\n293       17.52 0.0792          0            0    0.0002\n294       17.58 0.0795          0            0    0.0002\n295       17.64 0.0797          0            0    0.0002\n296       17.70 0.0800          0            0    0.0002\n297       17.76 0.0802          0            0    0.0001\n298       17.82 0.0804          0            0    0.0001\n299       17.88 0.0806          0            0    0.0001\n300       17.94 0.0808          0            0    0.0001\n301       18.00 0.0809          0            0    0.0001\n302       18.06 0.0811          0            0    0.0001\n303       18.12 0.0812          0            0    0.0001\n304       18.18 0.0814          0            0    0.0001\n305       18.24 0.0815          0            0    0.0000\n306       18.30 0.0816          0            0    0.0000\n307       18.36 0.0817          0            0    0.0000\n308       18.42 0.0818          0            0    0.0000\n309       18.48 0.0818          0            0    0.0000\n310       18.54 0.0819          0            0    0.0000\n311       18.60 0.0819          0            0    0.0000\n312       18.66 0.0819          0            0    0.0000\n313       18.72 0.0819          0            0    0.0000\n314       18.78 0.0819          0            0    0.0000\n315       18.84 0.0819          0            0    0.0000\n316       18.90 0.0819          0            0    0.0000\n317       18.96 0.0819          0            0    0.0000\n318       19.02 0.0818          0            0    0.0000\n319       19.08 0.0818          0            0    0.0000\n320       19.14 0.0817          0            0    0.0000\n321       19.20 0.0816          0            0    0.0000\n322       19.26 0.0815          0            0    0.0000\n323       19.32 0.0814          0            0    0.0000\n324       19.38 0.0813          0            0    0.0000\n325       19.44 0.0811          0            0    0.0000\n326       19.50 0.0810          0            0    0.0000\n327       19.56 0.0808          0            0    0.0000\n328       19.62 0.0807          0            0    0.0000\n329       19.68 0.0805          0            0    0.0000\n330       19.74 0.0803          0            0    0.0000\n331       19.80 0.0801          0            0    0.0000\n332       19.86 0.0799          0            0    0.0000\n333       19.92 0.0797          0            0    0.0000\n334       19.98 0.0795          0            0    0.0000\n335       20.04 0.0792          0            0    0.0000\n336       20.10 0.0790          0            0    0.0000\n337       20.16 0.0787          0            0    0.0000\n338       20.22 0.0784          0            0    0.0000\n339       20.28 0.0782          0            0    0.0000\n340       20.34 0.0779          0            0    0.0000\n341       20.40 0.0776          0            0    0.0000\n342       20.46 0.0773          0            0    0.0000\n343       20.52 0.0770          0            0    0.0000\n344       20.58 0.0766          0            0    0.0000\n345       20.64 0.0763          0            0    0.0000\n346       20.70 0.0760          0            0    0.0000\n347       20.76 0.0756          0            0    0.0000\n348       20.82 0.0753          0            0    0.0000\n349       20.88 0.0749          0            0    0.0000\n350       20.94 0.0745          0            0    0.0000\n351       21.00 0.0741          0            0    0.0000\n352       21.06 0.0738          0            0    0.0000\n353       21.12 0.0734          0            0    0.0000\n354       21.18 0.0730          0            0    0.0000\n355       21.24 0.0726          0            0    0.0000\n356       21.30 0.0722          0            0    0.0000\n357       21.36 0.0717          0            0    0.0000\n358       21.42 0.0713          0            0    0.0000\n359       21.48 0.0709          0            0    0.0000\n360       21.54 0.0704          0            0    0.0000\n361       21.60 0.0700          0            0    0.0000\n362       21.66 0.0696          0            0    0.0000\n363       21.72 0.0691          0            0    0.0000\n364       21.78 0.0687          0            0    0.0000\n365       21.84 0.0682          0            0    0.0000\n366       21.90 0.0677          0            0    0.0000\n367       21.96 0.0673          0            0    0.0000\n368       22.02 0.0668          0            0    0.0000\n369       22.08 0.0663          0            0    0.0000\n370       22.14 0.0658          0            0    0.0000\n371       22.20 0.0653          0            0    0.0000\n372       22.26 0.0648          0            0    0.0000\n373       22.32 0.0644          0            0    0.0000\n374       22.38 0.0639          0            0    0.0000\n375       22.44 0.0634          0            0    0.0000\n376       22.50 0.0629          0            0    0.0000\n377       22.56 0.0624          0            0    0.0000\n378       22.62 0.0618          0            0    0.0000\n379       22.68 0.0613          0            0    0.0000\n380       22.74 0.0608          0            0    0.0000\n381       22.80 0.0603          0            0    0.0000\n382       22.86 0.0598          0            0    0.0000\n383       22.92 0.0593          0            0    0.0000\n384       22.98 0.0588          0            0    0.0000\n385       23.04 0.0582          0            0    0.0000\n386       23.10 0.0577          0            0    0.0000\n387       23.16 0.0572          0            0    0.0000\n388       23.22 0.0567          0            0    0.0000\n389       23.28 0.0562          0            0    0.0000\n390       23.34 0.0556          0            0    0.0000\n391       23.40 0.0551          0            0    0.0000\n392       23.46 0.0546          0            0    0.0000\n393       23.52 0.0541          0            0    0.0000\n394       23.58 0.0535          0            0    0.0000\n395       23.64 0.0530          0            0    0.0000\n396       23.70 0.0525          0            0    0.0000\n397       23.76 0.0519          0            0    0.0000\n398       23.82 0.0514          0            0    0.0000\n399       23.88 0.0509          0            0    0.0000\n400       23.94 0.0504          0            0    0.0000\n401       24.00 0.0498          0            0    0.0000\n402       24.06 0.0493          0            0    0.0000\n403       24.12 0.0488          0            0    0.0000\n404       24.18 0.0483          0            0    0.0000\n405       24.24 0.0478          0            0    0.0000\n406       24.30 0.0472          0            0    0.0000\n407       24.36 0.0467          0            0    0.0000\n408       24.42 0.0462          0            0    0.0000\n409       24.48 0.0457          0            0    0.0000\n410       24.54 0.0452          0            0    0.0000\n411       24.60 0.0447          0            0    0.0000\n412       24.66 0.0442          0            0    0.0000\n413       24.72 0.0437          0            0    0.0000\n414       24.78 0.0432          0            0    0.0000\n415       24.84 0.0426          0            0    0.0000\n416       24.90 0.0421          0            0    0.0000\n417       24.96 0.0417          0            0    0.0000\n418       25.02 0.0412          0            0    0.0000\n419       25.08 0.0407          0            0    0.0000\n420       25.14 0.0402          0            0    0.0000\n421       25.20 0.0397          0            0    0.0000\n422       25.26 0.0392          0            0    0.0000\n423       25.32 0.0387          0            0    0.0000\n424       25.38 0.0382          0            0    0.0000\n425       25.44 0.0378          0            0    0.0000\n426       25.50 0.0373          0            0    0.0000\n427       25.56 0.0368          0            0    0.0000\n428       25.62 0.0363          0            0    0.0000\n429       25.68 0.0359          0            0    0.0000\n430       25.74 0.0354          0            0    0.0000\n431       25.80 0.0349          0            0    0.0000\n432       25.86 0.0345          0            0    0.0000\n433       25.92 0.0340          0            0    0.0000\n434       25.98 0.0336          0            0    0.0000\n435       26.04 0.0331          0            0    0.0000\n436       26.10 0.0327          0            0    0.0000\n437       26.16 0.0323          0            0    0.0000\n438       26.22 0.0318          0            0    0.0000\n439       26.28 0.0314          0            0    0.0000\n440       26.34 0.0310          0            0    0.0000\n441       26.40 0.0305          0            0    0.0000\n442       26.46 0.0301          0            0    0.0000\n443       26.52 0.0297          0            0    0.0000\n444       26.58 0.0293          0            0    0.0000\n445       26.64 0.0289          0            0    0.0000\n446       26.70 0.0284          0            0    0.0000\n447       26.76 0.0280          0            0    0.0000\n448       26.82 0.0276          0            0    0.0000\n449       26.88 0.0272          0            0    0.0000\n450       26.94 0.0269          0            0    0.0000\n451       27.00 0.0265          0            0    0.0000\n452       27.06 0.0261          0            0    0.0000\n453       27.12 0.0257          0            0    0.0000\n454       27.18 0.0253          0            0    0.0000\n455       27.24 0.0249          0            0    0.0000\n456       27.30 0.0246          0            0    0.0000\n457       27.36 0.0242          0            0    0.0000\n458       27.42 0.0238          0            0    0.0000\n459       27.48 0.0235          0            0    0.0000\n460       27.54 0.0231          0            0    0.0000\n461       27.60 0.0228          0            0    0.0000\n462       27.66 0.0224          0            0    0.0000\n463       27.72 0.0221          0            0    0.0000\n464       27.78 0.0217          0            0    0.0000\n465       27.84 0.0214          0            0    0.0000\n466       27.90 0.0211          0            0    0.0000\n467       27.96 0.0207          0            0    0.0000\n468       28.02 0.0204          0            0    0.0000\n469       28.08 0.0201          0            0    0.0000\n470       28.14 0.0198          0            0    0.0000\n471       28.20 0.0195          0            0    0.0000\n472       28.26 0.0191          0            0    0.0000\n473       28.32 0.0188          0            0    0.0000\n474       28.38 0.0185          0            0    0.0000\n475       28.44 0.0182          0            0    0.0000\n476       28.50 0.0179          0            0    0.0000\n477       28.56 0.0176          0            0    0.0000\n478       28.62 0.0174          0            0    0.0000\n479       28.68 0.0171          0            0    0.0000\n480       28.74 0.0168          0            0    0.0000\n481       28.80 0.0165          0            0    0.0000\n482       28.86 0.0162          0            0    0.0000\n483       28.92 0.0160          0            0    0.0000\n484       28.98 0.0157          0            0    0.0000\n485       29.04 0.0154          0            0    0.0000\n486       29.10 0.0152          0            0    0.0000\n487       29.16 0.0149          0            0    0.0000\n488       29.22 0.0147          0            0    0.0000\n489       29.28 0.0144          0            0    0.0000\n490       29.34 0.0142          0            0    0.0000\n491       29.40 0.0139          0            0    0.0000\n492       29.46 0.0137          0            0    0.0000\n493       29.52 0.0134          0            0    0.0000\n494       29.58 0.0132          0            0    0.0000\n495       29.64 0.0130          0            0    0.0000\n496       29.70 0.0127          0            0    0.0000\n497       29.76 0.0125          0            0    0.0000\n498       29.82 0.0123          0            0    0.0000\n499       29.88 0.0121          0            0    0.0000\n500       29.94 0.0119          0            0    0.0000\n501       30.00 0.0117          0            0    0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot the grid approximated posterior\nggplot(grid_data, aes(x = lambda_grid, y = posterior)) + \n  geom_segment(aes(x = lambda_grid, xend = lambda_grid, y = 0, yend = posterior),\n               color = \"gray50\",\n               linewidth = 1) +\n  geom_point(size = 2) + \n  labs(title = \"Dense Grid\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal()\n\n\n\n\n# Step 4: sample from the discretized posterior\nposterior_sample &lt;- sample_n(grid_data, \n                             size = 10000, \n                             weight = posterior, \n                             replace = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nggplot(posterior_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.5,\n                 color = \"black\",\n                 fill = \"gray50\") + \n  stat_function(fun = dgamma, args = list(135, 9.8),\n                color = \"#E77500\", linewidth = 3) + \n  lims(x = c(0, 1)) +\n  labs(title = \"Dense Grid: &lt;span style='color:#7F7F7F'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#limitations",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#limitations",
    "title": "6: Approximating the Posterior",
    "section": "Limitations",
    "text": "Limitations\n\nHandling larger data sets\nHandling multiple parameters\n\n\\[\\vec{\\theta} = (\\theta_{1}, \\theta_{2}, ..., \\theta_{k})\\]\n\nCurse of dimensionality: As the number of variables increases, “the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality” — Wikipedia\n\nBeta-Binomial example: \\(N = 101\\) grid points\nGamma-Poisson example: \\(N = 501\\) grid points"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#andrey-markov",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#andrey-markov",
    "title": "6: Approximating the Posterior",
    "section": "Andrey Markov",
    "text": "Andrey Markov\n\n\n\n1868 - 1908\nRussian Mathematician\nknown for stochastic processes\nthesis supervisor of Georgy Voronoi\n\n\n\n\n\nAndrey Markov"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#stanislaw-ulam",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#stanislaw-ulam",
    "title": "6: Approximating the Posterior",
    "section": "Stanislaw Ulam",
    "text": "Stanislaw Ulam\n\n\n\n1909 - 1984\nPolish Mathematician\nknown for Monte Carlo methods\nindefinite appointment at IAS\n\n\n\n\n\nJohn von Neumann, Richard Feynman, and Stanislaw Ulam, at Bandelier National Monument near Los Alamos, 1949\n\n\n\nimage source: Institute for Advanced Study"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#mcmc-chains",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#mcmc-chains",
    "title": "6: Approximating the Posterior",
    "section": "MCMC Chains",
    "text": "MCMC Chains\nLet \\(\\{ \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)} \\}\\) be an MCMC chain (Markov Chain Monte Carlo).\n\nMarkov Property:\n\n\\[f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(i)}, y \\right) = f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(i)}, y \\right)\\]\n\nMCMC simulation produces a chain of \\(N\\) dependent values\nThese values are not drawn from the posterior pdf \\(f(\\theta|y)\\)"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#stan",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#stan",
    "title": "6: Approximating the Posterior",
    "section": "Stan",
    "text": "Stan"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#example-beta-binomial-1",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#example-beta-binomial-1",
    "title": "6: Approximating the Posterior",
    "section": "Example: Beta-Binomial",
    "text": "Example: Beta-Binomial\n\nScenario: Smokers in Restaurants\nLet us start with a vague beta prior, use a binomial model to get the likelihood of \\(y = 4\\) smokers among \\(n = 9\\) customers, and then get a beta posterior.\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Beta}(3, 3) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Bin}(9, \\pi) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Beta}(7, 8) \\\\\n\\end{array}\\]\n\nDefine ModelSimulate PosteriorHistogramDensity\n\n\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 9&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(9, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n\n\n\n\nstart_time &lt;- Sys.time()\n\n# STEP 2: SIMULATE the posterior\nbb_sim &lt;- stan(model_code = bb_model, data = list(Y = 4), \n               chains = 4, iter = 5000*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 1:                0.029 seconds (Sampling)\nChain 1:                0.057 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 2:                0.03 seconds (Sampling)\nChain 2:                0.058 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 3:                0.029 seconds (Sampling)\nChain 3:                0.056 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 9e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 4:                0.029 seconds (Sampling)\nChain 4:                0.056 seconds (Total)\nChain 4: \n\nend_time &lt;- Sys.time()\nprint(round(end_time- start_time))\n\nTime difference of 38 secs\n\n\n\n\n\nbayesplot::mcmc_hist(bb_sim, pars = \"pi\")\n\n\n\n\n\n\n\nbayesplot::mcmc_dens(bb_sim, pars = \"pi\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  labs(title = \"MCMC: &lt;span style='color:#619CFF'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#example-gamma-poisson-1",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#example-gamma-poisson-1",
    "title": "6: Approximating the Posterior",
    "section": "Example: Gamma-Poisson",
    "text": "Example: Gamma-Poisson\n\nScenario: Drug Law Violations\nLet us start with a vague Gamma prior, use a binomial model to get the likelihood of \\(\\sum y = 119\\) drug law violations over \\(n = 9\\) years, and then get a Gamma posterior.\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Gamma}(16, 0.8) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Pois}(119/9) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Gamma}(135, 9.8) \\\\\n\\end{array}\\]\n\nDefine ModelSimulate PosteriorHistogramDensity\n\n\n\n# STEP 1: DEFINE the model\ngp_model &lt;- \"\n  data {\n    int&lt;lower = 0&gt; Y[9];\n  }\n  parameters {\n    real&lt;lower = 0&gt; lambda;\n  }\n  model {\n    Y ~ poisson(lambda);\n    lambda ~ gamma(16, 0.8);\n  }\n\"\n\n\n\n\nstart_time &lt;- Sys.time()\n\n# STEP 2: SIMULATE the posterior\nobs_counts &lt;- c(18, 14, 23, 22, 12, 22, 7, 0, 1)\ngp_sim &lt;- stan(model_code = gp_model, \n               data = list(Y = obs_counts), \n               chains = 4, iter = 5000*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 1:                0.032 seconds (Sampling)\nChain 1:                0.063 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 2:                0.028 seconds (Sampling)\nChain 2:                0.066 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.032 seconds (Warm-up)\nChain 3:                0.034 seconds (Sampling)\nChain 3:                0.066 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 4:                0.028 seconds (Sampling)\nChain 4:                0.062 seconds (Total)\nChain 4: \n\nend_time &lt;- Sys.time()\nprint(round(end_time- start_time))\n\nTime difference of 38 secs\n\n\n\n\n\nbayesplot::mcmc_hist(gp_sim, pars = \"lambda\")\n\n\n\n\n\n\n\nbayesplot::mcmc_dens(gp_sim, pars = \"lambda\") + \n  stat_function(fun = dgamma, args = list(135, 9.8),\n                color = \"#E77500\", linewidth = 3) + \n  labs(title = \"MCMC: &lt;span style='color:#619CFF'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html",
    "href": "posts/07_mcmc_1/07_mcmc_1.html",
    "title": "7: MCMC",
    "section": "",
    "text": "library(\"bayesplot\")\nlibrary(\"ggtext\")\nlibrary(\"rstan\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# tips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#metropolis-hastings",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#metropolis-hastings",
    "title": "7: MCMC",
    "section": "Metropolis-Hastings",
    "text": "Metropolis-Hastings\nUsing the analogy from the Bayes Rules! textbook, “As tour manager, you can automate the tour route using the Metropolis-Hastings algorithm. This algorithm iterates through a two-step process. Assuming the Markov chain is at location \\(\\mu(i)=\\mu\\) at iteration or “tour stop” \\(i\\), the next tour stop \\(\\mu(i+1)\\) is selected as follows:\n\nStep 1: Propose a random location, \\(\\mu^{′}\\), for the next tour stop.\nStep 2: Decide whether to\n\nto to the proposed location \\[\\mu(i+1)=\\mu^{′}\\]\nor to stay at the current location for another iteration \\[\\mu(i+1)=\\mu\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#monte-carlo",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#monte-carlo",
    "title": "7: MCMC",
    "section": "Monte Carlo",
    "text": "Monte Carlo\nIf we know the posterior distribution, this special case of the Metropolis-Hastings algorithm has a special name\n\n\n\n\n\n\nMonte Carlo algorithm\n\n\n\n\n\nTo construct an independent Monte Carlo sample directly from posterior pdf \\(f(\\mu|y)\\), \\[\\{\\mu^{(1)},\\mu^{(2)},...,\\mu^{(N)}\\}\\]\nselect each tour stop \\(\\mu^{(i)}=\\mu\\)\nas follows:\n\nStep 1: Propose a location. Draw a location μ from the posterior model with pdf \\(f(\\mu|y)\\)\nStep 2: Go there."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#generalizing",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#generalizing",
    "title": "7: MCMC",
    "section": "Generalizing",
    "text": "Generalizing\n\nwe only need MCMC to approximate a Bayesian posterior when that posterior is too complicated to specify\nif a posterior is too complicated to specify, it’s typically too complicated to directly sample or draw from as we did in our Monte Carlo tour above\nMetropolis-Hastings relies on the fact that, even if we don’t know the posterior model, we do know that the posterior pdf is proportional to the product of the known prior pdf and likelihood function\n\n\\[f(\\mu|y) \\propto f(\\mu) \\cdot L(\\mu|y)\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#nicholas-metropolis",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#nicholas-metropolis",
    "title": "7: MCMC",
    "section": "Nicholas Metropolis",
    "text": "Nicholas Metropolis\n\n\n\nPhD: University of Chicago\nRecruited by Oppenheimer\nTeam included von Neumann and Ulam\n\n\n\n\n\nNicholas Metropolis\n\n\nImage Source: Nuclear Museum"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#wilfred-hastings",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#wilfred-hastings",
    "title": "7: MCMC",
    "section": "Wilfred Hastings",
    "text": "Wilfred Hastings\n\n\nPhD: University of Toronto\nBell Labs (New Jersey)\ngeneralized Metropolis algorithm and MCMC\n\n\n\n\n\n\nWilfred Hastings\n\n\nImage Source: McCall Gardens"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#rosenbluths",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#rosenbluths",
    "title": "7: MCMC",
    "section": "Rosenbluths",
    "text": "Rosenbluths\n\n\nArianna Rosenbluth\n\nPhD: Harvard University\nqualified for Olympics (fencing)\nwrote first MCMC algorithm\n\n\n\nMarshall Rosenbluth\n\nPhD: University of Chicago\nWWII veteran (navy)\n\n\n\n\n\n\n\nMarshall and Arianna Rosenbluth\n\n\nImage Source: Los Alamos National Laboratories"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#uniform-proposal-model",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#uniform-proposal-model",
    "title": "7: MCMC",
    "section": "Uniform proposal model",
    "text": "Uniform proposal model\nLet \\(\\mu^{(i)} = \\mu\\) denote the current tour location with \\(w\\) being a half-width distance:\n\nrandom draw: \\(\\mu^{'}|\\mu \\sim \\text{Unif}(\\mu - w, \\mu + w)\\)\npdf: \\(q(\\mu^{'}|\\mu) = \\displaystyle\\frac{1}{2w} \\text{ for } \\mu^{'} \\in [(\\mu - w, \\mu + w]\\)"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#rejected-ideas",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#rejected-ideas",
    "title": "7: MCMC",
    "section": "Rejected Ideas",
    "text": "Rejected Ideas\n\nNever accept the proposed location.\nAlways accept the proposed location.\nOnly accept the proposed location if its (unnormalized) posterior plausibility is greater than that of the current location."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#main-idea",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#main-idea",
    "title": "7: MCMC",
    "section": "Main Idea",
    "text": "Main Idea\n\nStep 1: Propose a location, \\(\\mu^{'}\\), for the next tour stop by taking a draw from a proposal model.\nStep 2: Decide whether to go to the proposed location (\\(μ^{(i+1)}=\\mu^{'}\\)) or to stay at the current location for another iteration (\\(μ^{(i+1)}=\\mu\\)) as follows.\n\nIf the (unnormalized) posterior plausibility of the proposed location \\(\\mu^{'}\\) is greater than that of the current location \\(\\mu\\), \\[f(\\mu^{'})L(\\mu^{'}|y)&gt;f(\\mu)L(\\mu|y)\\] definitely go there.\nOtherwise, maybe go there."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#definition-1",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#definition-1",
    "title": "7: MCMC",
    "section": "Definition",
    "text": "Definition\n\n\n\n\n\n\nMetropolis-Hastings algorithm\n\n\n\n\n\nConditioned on data \\(y\\), let parameter \\(\\mu\\) have posterior pdf \\[f(\\mu|y)\\propto f(\\mu) \\cdot L(\\mu|y)\\] A Metropolis-Hastings Markov chain for \\(f(\\mu|y)\\), \\(\\{\\mu^{(1)},\\mu^{(2)},...,\\mu^{(N)}\\}\\), evolves as follows. Let \\(\\mu^{(i)}=\\mu\\) be the chain’s location at iteration \\(i\\in\\{1,2,...,N−1\\}\\) and identify the next location \\(\\mu^{(i+1)}\\) through a two-step process:\n\nStep 1: Propose a new location. Conditioned on the current location \\(\\mu\\), draw a location \\(\\mu^{′}\\) from a proposal model with pdf \\(q(\\mu^{′}|\\mu)\\).\nStep 2: Decide whether or not to go there.\n\nCalculate the acceptance probability (i.e., the probability of accepting the proposal \\(\\mu^{′}\\)): \\[\\alpha = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y)}{f(\\mu) \\cdot L(\\mu|y)} \\cdot \\displaystyle\\frac{q(\\mu^{′}|\\mu)}{q(\\mu|\\mu^{′})} \\right\\}\\]\nFiguratively, flip a weighted coin. If it’s Heads, with probability \\(\\alpha\\), go to the proposed location \\(\\mu^{′}\\). If it’s Tails, with probability \\(1−\\alpha\\), stay at \\(\\mu\\): \\[\\mu^{(i+1)} = \\begin{cases}\n\\mu^{'} & \\text{with probability } \\alpha \\\\\n\\mu & \\text{with probability } 1-\\alpha \\\\\n  \\end{cases}\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#symmetric-proposal",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#symmetric-proposal",
    "title": "7: MCMC",
    "section": "Symmetric Proposal",
    "text": "Symmetric Proposal\nWhat happens if we have a symmetric proposal model? For instance,\n\\[\\mu^{'}|\\mu \\sim \\text{Unif}(\\mu - w, \\mu + w)\\]\nleads to the probability density function\n\\[q(\\mu^{′}|\\mu) = q(\\mu|\\mu^{′}) = \\begin{cases}\n  \\frac{1}{2w}, & |\\mu - \\mu^{'}| &lt; w \\\\\n  0, & \\text{otherwise} \\\\\n\\end{cases}\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#metropolis-algorithm",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#metropolis-algorithm",
    "title": "7: MCMC",
    "section": "Metropolis Algorithm",
    "text": "Metropolis Algorithm\n\n\n\n\n\n\nMetropolis Algorithm\n\n\n\n\n\nThe Metropolis algorithm is a special case of the Metropolis-Hastings in which the proposal model is symmetric. That is, the chance of proposing a move to \\(\\mu^{′}\\) from \\(\\mu\\) is equal to that of proposing a move to \\(\\mu\\) from \\(\\mu^{′}\\): \\[q(\\mu^{′}|\\mu) = q(\\mu|\\mu^{′})\\]\nThe acceptance probability simplifies to \\[\\alpha = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y)}{f(\\mu) \\cdot L(\\mu|y)} \\right\\}\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#bayesian-ratio",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#bayesian-ratio",
    "title": "7: MCMC",
    "section": "Bayesian Ratio",
    "text": "Bayesian Ratio\nBy dividing both the numerator and denominator by \\(f(y)\\)\n\\[\\alpha = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y) / f(y)}{f(\\mu)  \\cdot L(\\mu|y) / f(y)} \\right\\} = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{'}|y)}{f(\\mu|y)} \\right\\}\\]\nThis rewrite emphasizes that, though we can’t calculate the posterior pdfs of \\(\\mu^{′}\\) and \\(\\mu\\), \\(f(\\mu^{'}|y)\\) and \\(f(\\mu|y)\\), their ratio is equivalent to that of the unnormalized posterior pdfs (which we can calculate)"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#tour-decisions",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#tour-decisions",
    "title": "7: MCMC",
    "section": "Tour Decisions",
    "text": "Tour Decisions\nThus, the probability of accepting a move from a current location \\(\\mu\\) to a proposed location \\(\\mu^{′}\\) comes down to a comparison of their posterior plausibility: \\(f(\\mu^{'}|y)\\) versus \\(f(\\mu|y)\\). There are two possible scenarios here:\n\nScenario 1: \\(f(\\mu^{'}|y) \\geq f(\\mu|y)\\). When the posterior plausibility of \\(\\mu^{′}\\) is at least as great as that of \\(\\mu\\), \\(\\alpha=1\\). Thus, we’ll definitely move there.\nScenario 2: \\(f(\\mu^{'}|y) &lt; f(\\mu|y)\\). If the posterior plausibility of \\(\\mu^{′}\\) is less than that of \\(\\mu\\), then\n\n\\[α=\\displaystyle\\frac{f(\\mu^{′}|y)}{f(\\mu|y)}&lt;1\\]\nThus, we might move there.\n\n\n\n\n\n\nNear\n\n\n\n\n\nFurther, \\(\\alpha\\) approaches 1 as \\(f(\\mu^{'}|y)\\) nears \\(f(\\mu|y)\\). That is, the probability of accepting the proposal increases with the plausibility of \\(\\mu^{′}\\) relative to \\(\\mu\\)."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#detailed-account",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#detailed-account",
    "title": "7: MCMC",
    "section": "Detailed Account",
    "text": "Detailed Account\n\nstart location: \\(\\mu^{(0)} = 3\\)\nhalf-width: \\(w = 0.5\\)\nnormal-normal model:\n\nprior: \\(\\mu \\sim \\text{N}(3, 0.50^2)\\)\nlikelihood: \\(Y|\\mu \\sim \\text{N}(\\mu, 0.75^2)\\)\n\nobserved value: \\(y = 3.20\\)\n\n\nset.seed(20240220)\ncurrent  &lt;- 3\nproposal &lt;- runif(1, min = current - 1, max = current + 1)\nprint(paste0(\"The proposed value is: \", proposal))\n\n[1] \"The proposed value is: 2.8043764475733\"\n\nproposal_plausibility &lt;- dnorm(proposal, 3, 0.5) * dnorm(3.20, proposal, 0.75)\nprint(paste0(\"The proposed plausibility is: \", proposal_plausibility))\n\n[1] \"The proposed plausibility is: 0.342079515048734\"\n\ncurrent_plausibility &lt;- dnorm(current, 3, 0.5) * dnorm(3.20, current, 0.75)\nprint(paste0(\"The current plausibility is: \", current_plausibility))\n\n[1] \"The current plausibility is: 0.409588054724166\"\n\nalpha &lt;- min(1, proposal_plausibility / current_plausibility)\nprint(paste0(\"The acceptance probability is: \", alpha))\n\n[1] \"The acceptance probability is: 0.835179422600849\""
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#helper-function",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#helper-function",
    "title": "7: MCMC",
    "section": "Helper Function",
    "text": "Helper Function\n\none_mh_iteration &lt;- function(current, w, obs_value, tau, sigma){\n  # Step 1: propose next location in chain\n  proposal &lt;- runif(1, min = current - w, current + w)\n  \n  # Step 2: decide whether or not to go there\n  proposal_plausibility &lt;- dnorm(proposal, 3, tau) * dnorm(obs_value, proposal, sigma)\n  current_plausibility &lt;- dnorm(current, 3, tau) * dnorm(obs_value, current, sigma)\n  alpha &lt;- min(1, proposal_plausibility / current_plausibility)\n  next_stop &lt;- sample(c(proposal, current), \n                      size = 1, prob = c(alpha, 1 - alpha))\n  \n  # Return the results as a data frame\n  return(data.frame(proposal, alpha, next_stop))\n}\n\n\nset.seed(1)\none_mh_iteration(current = 3, w = 0.5, \n                 obs_value = 3.20, tau = 0.75, sigma = 0.50)\n\n  proposal     alpha next_stop\n1 2.765509 0.7071998  2.765509\n\n\n\nset.seed(4)\none_mh_iteration(current = 3, w = 0.5, \n                 obs_value = 3.20, tau = 0.75, sigma = 0.50)\n\n  proposal alpha next_stop\n1   3.0858     1    3.0858\n\n\n\nset.seed(5)\none_mh_iteration(current = 3, w = 0.5, \n                 obs_value = 3.20, tau = 0.75, sigma = 0.50)\n\n  proposal     alpha next_stop\n1 2.700214 0.6068602         3"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#for-loop",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#for-loop",
    "title": "7: MCMC",
    "section": "For Loop",
    "text": "For Loop\n\nmh_tour &lt;- function(N, current, w, obs_value, tau, sigma){\n  # N: chain length\n  # initialize vector\n  mu &lt;- rep(0, N)\n  \n  # simulate N Markov chain stops\n  for(i in 1:N){\n    # simulate one iteration\n    this_iteration &lt;- one_mh_iteration(current, w, obs_value, tau, sigma)\n    \n    # record next location\n    mu[i] &lt;- this_iteration$next_stop\n    \n    # update current location\n    current &lt;- this_iteration$next_stop\n  }\n  \n  # return the chain locations\n  return(data.frame(iteration = c(1:N), mu))\n}"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#go-on-tour",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#go-on-tour",
    "title": "7: MCMC",
    "section": "Go on Tour!",
    "text": "Go on Tour!\n\nour_mh_tour &lt;- mh_tour(N = 5000, current = 3, w = 1, \n                 obs_value = 3.20, tau = 0.50, sigma = 0.75)"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#traces",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#traces",
    "title": "7: MCMC",
    "section": "Traces",
    "text": "Traces\n\nOur Metropolis-Hastings Tour\nFrom our knowledge of the Normal-Normal model,\n\nbayesrules::summarize_normal_normal(\n  # from prior\n  mean = 3, sd = 0.50,\n  \n  # from observations\n  y_bar = 3.20, sigma = 0.75, n = 1\n) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model   mean   mode    var    sd\n1     prior 3.0000 3.0000 0.2500 0.500\n2 posterior 3.0615 3.0615 0.1731 0.416\n\n\nwe should have a \\(\\text{N}(3.0615, 0.4160^2)\\) posterior distribution\n\np1 &lt;- ggplot(our_mh_tour, aes(x = iteration, y = mu)) + \n  geom_line() +\n  labs(title = \"Our Metropolis-Hastings Tour\",\n       subtitle = \"Posterior: N(3.0615, 0.4160^2)\",\n       caption = \"SML 320\") +\n  theme_minimal()\n\np2 &lt;- ggplot(our_mh_tour, aes(x = mu)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.1,\n                 color = \"black\", fill = \"gray50\") + \n  stat_function(fun = dnorm, args = list(3.0615, 0.4160), \n                color = \"#E77500\",\n                linewidth = 2) +\n  theme_minimal()\n\n# patchwork\np1 + p2"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#bad-examples",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#bad-examples",
    "title": "7: MCMC",
    "section": "Bad Examples",
    "text": "Bad Examples\n\n\n\nbad examples\n\n\nImage Source: [Bayes Rules!](https://www.bayesrulesbook.com/chapter-6#diagnostics"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#density-overlay",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#density-overlay",
    "title": "7: MCMC",
    "section": "Density Overlay",
    "text": "Density Overlay\n\nPlotsCode\n\n\n\n\n\n\n\n\n\n\np1 &lt;- mcmc_dens_overlay(bad_simulation, pars = \"pi\") + \n  labs(title = \"Density Overlay\",\n       subtitle = \"bad simulation\",\n       caption = \"SML 320\",\n       y = \"density\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np2 &lt;- mcmc_dens_overlay(good_simulation, pars = \"pi\") + \n  labs(title = \"Density Overlay\",\n       subtitle = \"good simulation\",\n       caption = \"SML 320\",\n       y = \"density\") +\n  theme_minimal()\n\n# patchwork\np1 + p2"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#legacy",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#legacy",
    "title": "7: MCMC",
    "section": "Legacy",
    "text": "Legacy\nThe Metropolis-Hastings algorithm appeared in a top-ten list called ““the greatest influence on the development and practice of science and engineering in the 20th century”."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#music-recommendation",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#music-recommendation",
    "title": "7: MCMC",
    "section": "Music Recommendation",
    "text": "Music Recommendation\nShould I Stay or Should I Go by The Clash\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2    forcats_1.0.0      stringr_1.5.0      dplyr_1.1.3       \n [5] purrr_1.0.2        readr_2.1.4        tidyr_1.3.0        tibble_3.2.1      \n [9] ggplot2_3.4.3      tidyverse_2.0.0    patchwork_1.1.2    rstan_2.32.5      \n[13] StanHeaders_2.32.5 ggtext_0.1.2       bayesplot_1.10.0  \n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3        inline_0.3.19        rlang_1.1.1         \n  [4] magrittr_2.0.3       snakecase_0.11.0     matrixStats_1.0.0   \n  [7] e1071_1.7-13         compiler_4.3.0       loo_2.6.0           \n [10] callr_3.7.3          vctrs_0.6.3          reshape2_1.4.4      \n [13] pkgconfig_2.0.3      crayon_1.5.2         fastmap_1.1.1       \n [16] backports_1.4.1      ellipsis_0.3.2       labeling_0.4.3      \n [19] utf8_1.2.3           threejs_0.3.3        promises_1.2.1      \n [22] rmarkdown_2.24       markdown_1.8         tzdb_0.4.0          \n [25] nloptr_2.0.3         ps_1.7.5             xfun_0.40           \n [28] jsonlite_1.8.7       later_1.3.1          parallel_4.3.0      \n [31] prettyunits_1.1.1    R6_2.5.1             dygraphs_1.1.1.6    \n [34] stringi_1.7.12       boot_1.3-28.1        Rcpp_1.0.11         \n [37] knitr_1.43           zoo_1.8-12           base64enc_0.1-3     \n [40] splines_4.3.0        Matrix_1.5-4         igraph_1.4.3        \n [43] httpuv_1.6.11        timechange_0.2.0     tidyselect_1.2.0    \n [46] rstudioapi_0.15.0    abind_1.4-5          yaml_2.3.7          \n [49] miniUI_0.1.1.1       codetools_0.2-19     curl_5.0.2          \n [52] processx_3.8.1       pkgbuild_1.4.0       lattice_0.21-8      \n [55] plyr_1.8.8           shiny_1.7.5          withr_2.5.2         \n [58] groupdata2_2.0.2     posterior_1.4.1      evaluate_0.21       \n [61] survival_3.5-5       proxy_0.4-27         RcppParallel_5.1.7  \n [64] xts_0.13.1           xml2_1.3.5           pillar_1.9.0        \n [67] tensorA_0.36.2       DT_0.28              checkmate_2.2.0     \n [70] stats4_4.3.0         shinyjs_2.1.0        distributional_0.3.2\n [73] generics_0.1.3       hms_1.1.3            rstantools_2.3.1    \n [76] munsell_0.5.0        commonmark_1.9.0     scales_1.2.1        \n [79] minqa_1.2.5          gtools_3.9.4         xtable_1.8-4        \n [82] class_7.3-21         glue_1.6.2           janitor_2.2.0       \n [85] tools_4.3.0          shinystan_2.6.0      lme4_1.1-33         \n [88] colourpicker_1.2.0   bayesrules_0.0.2     grid_4.3.0          \n [91] crosstalk_1.2.0      QuickJSR_1.1.3       colorspace_2.1-0    \n [94] nlme_3.1-162         cli_3.6.1            fansi_1.0.4         \n [97] V8_4.3.0             gtable_0.3.4         digest_0.6.33       \n[100] htmlwidgets_1.6.2    farver_2.1.1         htmltools_0.5.6     \n[103] lifecycle_1.0.4      mime_0.12            rstanarm_2.21.4     \n[106] MASS_7.3-58.4        shinythemes_1.2.0    gridtext_0.1.5"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html",
    "href": "posts/08_mcmc_2/08_mcmc_2.html",
    "title": "8: MCMC",
    "section": "",
    "text": "library(\"bayesplot\")\nlibrary(\"ggtext\")\nlibrary(\"rstan\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#autocorrelation",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#autocorrelation",
    "title": "8: MCMC",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nMarkov PropertyPlotsCode\n\n\nIf we are assuming the Markov property\n\\[f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(i)}, y \\right) = f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(i)}, y \\right)\\]\nthen autocorrelation measurements should only be “large” with a lag of one.\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesplot::mcmc_acf(good_simulation, pars = \"pi\") +\n  labs(title = \"Good Simulation\",\n       subtitle = \"10000 iterations\",\n       caption = \"SML 320\")\n\np2 &lt;- bayesplot::mcmc_acf(bad_simulation, pars = \"pi\") +\n  labs(title = \"Bad Simulation\",\n       subtitle = \"100 iterations\",\n       caption = \"SML 320\")\n\np3 &lt;- bayesplot::mcmc_acf(thin_simulation, pars = \"pi\") +\n  labs(title = \"Thinned Simulation\",\n       subtitle = \"Retained every 10 values\",\n       caption = \"SML 320\")\n\n# patchwork\np1 + p2 + p3"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#split-r-metric",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#split-r-metric",
    "title": "8: MCMC",
    "section": "Split R Metric",
    "text": "Split R Metric\n\nAnalysis of VarianceDefinitionGuidanceExamples\n\n\n\n\n\nBayes Rules! Figure 6.19\n\n\n\n\n\n\n\n\n\n\nR-Hat\n\n\n\n\n\nConsider a Markov chain simulation of parameter \\(\\theta\\) which utilizes four parallel chains. Let \\(\\text{Var}_{\\text{combined}}\\) denote the variability in \\(\\theta\\) across all four chains combined and \\(\\text{Var}_{\\text{within}}\\) denote the typical variability within any individual chain. The R-hat metric calculates the ratio between these two sources of variability:\n\\[\\text{R-hat} \\approx \\sqrt{\\displaystyle\\frac{\\text{Var}_{\\text{combined}}}{\\text{Var}_{\\text{within}}}}\\]\n\n\n\n\n\n\nIdeally, \\(\\text{R-hat}\\approx 1\\), reflecting stability across the parallel chains.\nIn contrast, \\(\\text{R-hat} &gt; 1\\) indicates instability, with the variability in the combined chains exceeding that within the chains.\nThough no golden rule exists, an R-hat ratio greater than 1.05 raises some red flags about the stability of the simulation.\n\n\n\n\nbayesplot::rhat(good_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 1.0003\n\n\n\nbayesplot::rhat(bad_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 1.052\n\n\n\nbayesplot::rhat(thin_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 0.9983"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#effective-sample-size",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#effective-sample-size",
    "title": "8: MCMC",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\nMotivationDescriptionGuidanceExamples\n\n\nLoosely speaking, how many independent sample values would it take to produce an equivalently accurate posterior approximation?\n\n\n\n\n\n\n\n\nEffective Sample Size Ratio\n\n\n\n\n\nLet \\(N\\) denote the actual sample size or length of a dependent Markov chain. The effective sample size of this chain, \\(N_{\\text{eff}\\), quantifies the number of independent samples it would take to produce an equivalently accurate posterior approximation. The greater the \\(N_{\\text{eff}\\) the better, yet it’s typically true that the accuracy of a Markov chain approximation is only as good as that of a smaller independent sample. That is, it’s typically true that \\(N_{\\text{eff} &lt; N\\), thus the effective sample size ratio is less than 1: \\[\\displaystyle\\frac{N_{\\text{eff}}{N} &lt; 1 \\quad\\text{(usually)}\\]\n\n\n\n\n\nThere’s no magic rule for interpreting this ratio, and it should be utilized alongside other diagnostics such as the trace plot. That said, we might be suspicious of a Markov chain for which the effective sample size ratio is less than 0.1, i.e., the effective sample size \\(N_{\\text{eff}\\) is less than 10% of the actual sample size N.\n\n\n\nbayesplot::neff_ratio(good_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 0.3873\n\n\n\nbayesplot::neff_ratio(bad_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 0.3434\n\n\n\nbayesplot::neff_ratio(thin_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 1.0543"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#metropolis-algorithm",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#metropolis-algorithm",
    "title": "8: MCMC",
    "section": "Metropolis Algorithm",
    "text": "Metropolis Algorithm\nIf we have a symmetric proposal model, the probability of accepting a move from a current location \\(\\mu\\) to a proposed location \\(\\mu^{′}\\) comes down to a comparison of their posterior plausibility: \\(f(\\mu^{'}|y)\\) versus \\(f(\\mu|y)\\). There are two possible scenarios here:\n\nScenario 1: \\(f(\\mu^{'}|y) \\geq f(\\mu|y)\\). When the posterior plausibility of \\(\\mu^{′}\\) is at least as great as that of \\(\\mu\\), \\(\\alpha=1\\). Thus, we’ll definitely move there.\nScenario 2: \\(f(\\mu^{'}|y) &lt; f(\\mu|y)\\). If the posterior plausibility of \\(\\mu^{′}\\) is less than that of \\(\\mu\\), then\n\n\\[α=\\displaystyle\\frac{f(\\mu^{′}|y)}{f(\\mu|y)}&lt;1\\]\nThus, we might move there."
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#metropolis-hastings-algorithm",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#metropolis-hastings-algorithm",
    "title": "8: MCMC",
    "section": "Metropolis-Hastings Algorithm",
    "text": "Metropolis-Hastings Algorithm\nRemoving the assumption of a symmetric proposal model, we have to convey \\(q(\\mu^{'}|\\mu)\\), the probability density function of the proposal model.\nConditioned on data \\(y\\), let parameter \\(\\mu\\) have posterior pdf \\[f(\\mu|y)\\propto f(\\mu) \\cdot L(\\mu|y)\\] A Metropolis-Hastings Markov chain for \\(f(\\mu|y)\\), \\(\\{\\mu^{(1)},\\mu^{(2)},...,\\mu^{(N)}\\}\\), evolves as follows. Let \\(\\mu^{(i)}=\\mu\\) be the chain’s location at iteration \\(i\\in\\{1,2,...,N−1\\}\\) and identify the next location \\(\\mu^{(i+1)}\\) through a two-step process:\n\nStep 1: Propose a new location. Conditioned on the current location \\(\\mu\\), draw a location \\(\\mu^{′}\\) from a proposal model with pdf \\(q(\\mu^{′}|\\mu)\\).\nStep 2: Decide whether or not to go there.\n\nCalculate the acceptance probability (i.e., the probability of accepting the proposal \\(\\mu^{′}\\)): \\[\\alpha = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y)}{f(\\mu) \\cdot L(\\mu|y)} \\cdot \\displaystyle\\frac{q(\\mu^{′}|\\mu)}{q(\\mu|\\mu^{′})} \\right\\}\\]\nFiguratively, flip a weighted coin. If it’s Heads, with probability \\(\\alpha\\), go to the proposed location \\(\\mu^{′}\\). If it’s Tails, with probability \\(1−\\alpha\\), stay at \\(\\mu\\): \\[\\mu^{(i+1)} = \\begin{cases}\n\\mu^{'} & \\text{with probability } \\alpha \\\\\n\\mu & \\text{with probability } 1-\\alpha \\\\\n  \\end{cases}\\]"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#motivation-1",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#motivation-1",
    "title": "8: MCMC",
    "section": "Motivation",
    "text": "Motivation\nWhy generalize to a non-symmetric proposal model?\n\nflexibility to estimate a variety of parameters, such as standard deviations (or other nonnegative values)\nleads to more clever searches"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#gibbs-sampling",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#gibbs-sampling",
    "title": "8: MCMC",
    "section": "Gibbs Sampling",
    "text": "Gibbs Sampling\n\nadaptive algorithm (especially with conjugate pairs)\ndescribed in 1984 by Stuart Geman and Donald Geman\nnamed after Josiah Willard Gibbs (for Gibbs’ work in statistical physics)\ngood for conditional distributions and marginal distributions"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#interlude-mountains-of-laos",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#interlude-mountains-of-laos",
    "title": "8: MCMC",
    "section": "Interlude: Mountains of Laos",
    "text": "Interlude: Mountains of Laos\n\n\n\nVang Pao"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#hamiltonian-monte-carlo",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#hamiltonian-monte-carlo",
    "title": "8: MCMC",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\nToward simulating a posterior distribution, Hamiltonian Monte Carlo (HMC) uses the topology by seeking out the gradient of maximum ascent\n\\[\\displaystyle\\text{max}_{\\vec{h}} \\lim_{|h| \\to 0} \\displaystyle\\frac{f(\\vec{x} + \\vec{h}) - f(\\vec{x})}{|h|}\\]\n“Path of least resistance”"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#app",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#app",
    "title": "8: MCMC",
    "section": "App",
    "text": "App\nTry out some MCMC simulations!\n\nlink: MCMC Demo by Chi Feng\nalgorithms:\n\nRandomWalkMH\nGibbsSampling\nHamiltonianMC\nEfficientNUTS\n\nTarget distributions:\n\nstandard\nbanana\ndonut\nmultimodal"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#tuning",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#tuning",
    "title": "8: MCMC",
    "section": "Tuning",
    "text": "Tuning\n\ngood_simulation &lt;- stan(model_code = bb_model, data = list(Y = 4), \n                        chains = 4, iter = 5000*2, seed = 84735)\n\nWhere did the simulation parameters, such as half-width and step-size, go?"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html",
    "href": "posts/09_posterior_inference/09_posterior_inference.html",
    "title": "9: Posterior Inference",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"infer\") #for frequentist approaches\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#neon",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#neon",
    "title": "9: Posterior Inference",
    "section": "NEON",
    "text": "NEON\n“The National Science Foundation’s National Ecological Observatory Network (NEON) is a continental-scale observation facility operated by Battelle and designed to collect long-term open access ecological data to better understand how U.S. ecosystems are changing.”"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#neon-field-sites",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#neon-field-sites",
    "title": "9: Posterior Inference",
    "section": "NEON Field Sites",
    "text": "NEON Field Sites\n“NEON’s aquatic and terrestrial sites are strategically located across the U.S. within 20 ecoclimatic Domains that represent regions of distinct landforms, vegetation, climate and ecosystem dynamics.”\n\n\n\nNEON Field Sites"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#neon-forecasts-challenge",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#neon-forecasts-challenge",
    "title": "9: Posterior Inference",
    "section": "NEON Forecasts Challenge",
    "text": "NEON Forecasts Challenge\n\n\n“The National Science Foundation funded Ecological Forecasting Initiative Research Coordination Network (EFI-RCN) is hosting a NEON Ecological Forecast Challenge with the goal to create a community of practice that builds capacity for ecological forecasting by leveraging NEON data products. The Challenge revolves around the five theme areas listed below that span aquatic and terrestrial systems, and population, community, and ecosystem processes across a broad range of ecoregions that uses data collected by NEON.”\n\n\n\n\nNEON Forecasts Themes"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#neon4casts-package",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#neon4casts-package",
    "title": "9: Posterior Inference",
    "section": "neon4casts package",
    "text": "neon4casts package\nThe neon4cast package “provides a collection of convenient helper utilities for anyone entering the EFI NEON Forecasting Challenge.”\n\nreal-world data\nupdated nearly daily\nvery easy API access\n\n\nraw_df &lt;- readr::read_csv(\"https://data.ecoforecast.org/neon4cast-targets/aquatics/aquatics-targets.csv.gz\")"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#alternative-data-source-aqicn",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#alternative-data-source-aqicn",
    "title": "9: Posterior Inference",
    "section": "Alternative Data Source: AQICN",
    "text": "Alternative Data Source: AQICN\n\nWorld Air Quality Index Project\nAPI: Purple Air"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#data-product",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#data-product",
    "title": "9: Posterior Inference",
    "section": "Data Product",
    "text": "Data Product\n\nhead(raw_df)\n\n# A tibble: 6 × 4\n  datetime   site_id variable observation\n  &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2016-03-05 ARIK    chla              NA\n2 2016-03-05 ARIK    oxygen            NA\n3 2016-03-06 ARIK    chla              NA\n4 2016-03-06 ARIK    oxygen            NA\n5 2016-03-07 ARIK    chla              NA\n6 2016-03-07 ARIK    oxygen            NA"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#data-wrangling",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#data-wrangling",
    "title": "9: Posterior Inference",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\naquatic_df &lt;- raw_df %&gt;% \n    pivot_wider(names_from = \"variable\", values_from = \"observation\")\n\nhead(aquatic_df)\n\n# A tibble: 6 × 5\n  datetime   site_id  chla oxygen temperature\n  &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n1 2016-03-05 ARIK       NA     NA          NA\n2 2016-03-06 ARIK       NA     NA          NA\n3 2016-03-07 ARIK       NA     NA          NA\n4 2016-03-08 ARIK       NA     NA          NA\n5 2016-03-09 ARIK       NA     NA          NA\n6 2016-03-10 ARIK       NA     NA          NA\n\n\n\ncolnames(aquatic_df)\n\n[1] \"datetime\"    \"site_id\"     \"chla\"        \"oxygen\"      \"temperature\""
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#variables",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#variables",
    "title": "9: Posterior Inference",
    "section": "Variables",
    "text": "Variables\n\nTimeframeField SitesChlorophyll-aFocus: Lewis RunOxygen versus Temperature\n\n\nWe have about 7 years of daily data.\n\naquatic_df |&gt; select(datetime) |&gt; slice(1, n())\n\n# A tibble: 2 × 1\n  datetime  \n  &lt;date&gt;    \n1 2016-03-05\n2 2024-02-24\n\n\n\n\n\nhttps://www.neonscience.org/field-sites/about-field-sites\n\n\naquatic_df |&gt; select(site_id) |&gt; distinct() |&gt; as.vector()\n\n$site_id\n [1] \"ARIK\" \"BARC\" \"BIGC\" \"BLDE\" \"BLUE\" \"BLWA\" \"CARI\" \"COMO\" \"CRAM\" \"CUPE\"\n[11] \"FLNT\" \"GUIL\" \"HOPB\" \"KING\" \"LECO\" \"LEWI\" \"LIRO\" \"MART\" \"MAYF\" \"MCDI\"\n[21] \"MCRA\" \"OKSR\" \"POSE\" \"PRIN\" \"PRLA\" \"PRPO\" \"REDB\" \"SUGG\" \"SYCA\" \"TECR\"\n[31] \"TOMB\" \"TOOK\" \"WALK\" \"WLOU\"\n\n\n\naquatic_df |&gt; group_by(site_id) |&gt; count() |&gt; select(site_id, n)\n\n# A tibble: 34 × 2\n# Groups:   site_id [34]\n   site_id     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 ARIK     2912\n 2 BARC     2307\n 3 BIGC     2113\n 4 BLDE     1980\n 5 BLUE     1978\n 6 BLWA     1573\n 7 CARI     1236\n 8 COMO     2463\n 9 CRAM     1150\n10 CUPE     2107\n# ℹ 24 more rows\n\n\n\n\n\nnon-wadeable river sites (“lakes”)\n“Phytoplankton biomass are the base of the aquatic food-web and an important indicator of water quality for managers.”\n\n\naquatic_df |&gt;\n  ggplot(aes(x = chla)) +\n  geom_density(color = \"darkgreen\", linewidth = 3) +\n  labs(title = \"Chlorophyll-a Concentration\",\n       subtitle = \"Data Source: NEON Forecasts\",\n       caption = \"SML 320\") +\n  theme_minimal() +\n  xlim(0, 50)\n\n\n\n\n\n\nThe Lewis Run NEON station is located 60 miles west of Washingon DC. It is in the “Mid-Atlantic” region of the NEON stations, and it is perhaps the closest in climate to Princeton, New Jersey.\n\naquatic_df |&gt; filter(site_id == \"LEWI\") |&gt; tail()\n\n# A tibble: 6 × 5\n  datetime   site_id  chla oxygen temperature\n  &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n1 2024-02-19 LEWI       NA   6.47        6.80\n2 2024-02-20 LEWI       NA   8.10        6.62\n3 2024-02-21 LEWI       NA   3.31        7.24\n4 2024-02-22 LEWI       NA  NA           7.76\n5 2024-02-23 LEWI       NA  NA           8.59\n6 2024-02-24 LEWI       NA  NA           8.82\n\n\n\n\n\ndissolved oxygen concentrations less than 2 mg/L are considered hypoxic.\n\n\nlewi_df &lt;- aquatic_df |&gt; \n  filter(site_id == \"LEWI\") |&gt;\n  filter(!is.na(oxygen)) |&gt;\n  mutate(hypoxic = oxygen &lt; 2)\n\naquatic_df |&gt;\n  ggplot() +\n  geom_point(aes(x = temperature, y = oxygen),\n             color = \"gray50\") +\n  geom_point(aes(x = temperature, y = oxygen, color = hypoxic),\n             data = lewi_df, size = 3) +\n  labs(title = \"Hypoxia in Lewis Run?\",\n       subtitle = \"Data Source: NEON Forecasts\",\n       caption = \"SML 320\") +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#frequentist-approach-confidence-interval",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#frequentist-approach-confidence-interval",
    "title": "9: Posterior Inference",
    "section": "Frequentist Approach: Confidence Interval",
    "text": "Frequentist Approach: Confidence Interval\n\nDefinitionCodeVisualizationInference\n\n\nA confidence interval records percentile endpoints of a sampling distribution that was made by resampling the data with replacement.\n\n\nUsing code from the infer package, as seen in the ModernDive textbook\n\nbootstrap_distribution &lt;- aquatic_df %&gt;% \n  specify(response = oxygen) %&gt;% \n  generate(reps = 1000) %&gt;% \n  calculate(stat = \"mean\")\n\nWarning: Removed 19159 rows containing missing values.\n\n\nSetting `type = \"bootstrap\"` in `generate()`.\n\npercentile_ci &lt;- bootstrap_distribution %&gt;% \n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\n\n\n\nvisualize(bootstrap_distribution) + \n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\npercentile_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     9.18     9.22\n\n\nWe are 95% confident that the true average concentration level in American bodies of water is between 9.18 and 9.22 mg/L."
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#prior",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#prior",
    "title": "9: Posterior Inference",
    "section": "Prior",
    "text": "Prior\nConsidering the oxygen concentration levels, we will model both the prior and observed data as continuous variables, which lends itself to a normal-normal conjugate pair.\nFor a rather informative prior (perhaps too restrictive), let us get the oxygen concentrations from the year 2023 data.\n\naquatic_df |&gt;\n  filter(between(datetime,\n                 as_date(\"2023-01-01\"), \n                 as_date(\"2023-12-31\"))) |&gt;\n  summarise(mean = mean(oxygen, na.rm = TRUE),\n            sd = sd(oxygen, na.rm = TRUE),\n            n_prior = n())\n\n# A tibble: 1 × 3\n   mean    sd n_prior\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n1  9.20  2.16   11155"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#observed-data",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#observed-data",
    "title": "9: Posterior Inference",
    "section": "Observed Data",
    "text": "Observed Data\nThen, let us focus on the year 2024 data (before today).\n\naquatic_df |&gt;\n  filter(datetime &gt;as_date(\"2024-01-01\")) |&gt;\n  summarise(y_bar = mean(oxygen, na.rm = TRUE),\n            sigma = sd(oxygen, na.rm = TRUE),\n            n_obs = n())\n\n# A tibble: 1 × 3\n  y_bar sigma n_obs\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  10.8  1.77  1462"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#posterior-distribution",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#posterior-distribution",
    "title": "9: Posterior Inference",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\nbayesrules::summarize_normal_normal(\n  #from prior\n  mean = 9.202716, sd = 2.162342,\n  \n  # from observed data\n  sigma = 1.768468, y_bar = 10.83557, n = 1462\n) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model    mean    mode    var     sd\n1     prior  9.2027  9.2027 4.6757 2.1623\n2 posterior 10.8348 10.8348 0.0021 0.0462"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#credible-interval",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#credible-interval",
    "title": "9: Posterior Inference",
    "section": "Credible Interval",
    "text": "Credible Interval\nTo get a 95 percent credible interval, retrieve the locations of the 2.5 and 97.5 percentiles from the posterior distribution.\n\nqnorm(c(0.025, 0.975), mean = 10.8348, sd = 0.0462)\n\n[1] 10.74425 10.92535"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#inference-and-commentary",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#inference-and-commentary",
    "title": "9: Posterior Inference",
    "section": "Inference and Commentary",
    "text": "Inference and Commentary\nThe posterior oxygen concentration levels are between 10.74 and 10.93 mg/L.\n\nSo far, it appears that credible intervals are smaller than confidence intervals (partly because of noting sample sizes)\nWe did not have to aim for 95% credibility\nWe did not have to retrieve the “middle” 95%\nDo we have “recency bias”?"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#frequentist-approach",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#frequentist-approach",
    "title": "9: Posterior Inference",
    "section": "Frequentist Approach",
    "text": "Frequentist Approach\n\nNull HypothesisDefinitionCodeVisualizationInference\n\n\nRecall that hypoxia is classified as having dissolved oxygen concentration levels under 2 mg/L.\n\\[\\text{H}_{o}: \\mu = 2\\] \\[\\text{H}_{a}: \\mu &lt; 2\\]\n\n\nIn frequentist null hypothesis significance testing (NHST), the p-value is the probability of encountering results that are at least as extreme as the observed results under the assumption that the null hypothesis is true.\n\np-value &lt; 0.05: reject the null hypothesis\np-value &gt;= 0.05: fail to reject the null hypothesis\n\n\n\n\nobs_mean &lt;- aquatic_df |&gt;\n  specify(response = oxygen) |&gt;\n  calculate(stat = \"mean\")\n\nWarning: Removed 19159 rows containing missing values.\n\nnull_distribution &lt;- aquatic_df |&gt;\n  specify(response = oxygen) |&gt;\n  hypothesize(null = \"point\", mu = 2) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")\n\nWarning: Removed 19159 rows containing missing values.\n\n\n\n\n\nnull_distribution |&gt;\n  visualise() +\n  shade_p_value(obs_stat = obs_mean, direction = \"less\")\n\n\n\n\n\n\n\nnull_distribution |&gt;\n  get_p_value(obs_stat = obs_mean, direction = \"less\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       1\n\n\nSince the p-value &gt; 0.05, we fail to reject the claim that the dissolved oxygen concentration level in American bodies of water is 2 mg/L."
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#prior-and-posterior-probabilities",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#prior-and-posterior-probabilities",
    "title": "9: Posterior Inference",
    "section": "Prior and Posterior Probabilities",
    "text": "Prior and Posterior Probabilities\n\n\n      model    mean    mode    var     sd\n1     prior  9.2027  9.2027 4.6757 2.1623\n2 posterior 10.8348 10.8348 0.0021 0.0462\n\n\n\nprior_prob &lt;- pnorm(2, mean = 9.2027, sd = 2.1623)\n\nposterior_prob &lt;- pnorm(2, mean = 10.8348, sd = 0.0462)"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#prior-and-posterior-odds",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#prior-and-posterior-odds",
    "title": "9: Posterior Inference",
    "section": "Prior and Posterior Odds",
    "text": "Prior and Posterior Odds\n\nprior_odds &lt;- prior_prob / (1 - prior_prob)\n\nposterior_odds &lt;- posterior_prob / (1 - posterior_prob)"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#bayes-factor",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#bayes-factor",
    "title": "9: Posterior Inference",
    "section": "Bayes Factor",
    "text": "Bayes Factor\n\\[\\text{Bayes Factor} = \\displaystyle\\frac{\\text{posterior odds}}{\\text{prior odds}}\\]\n\n\\(BF = 1\\): the plausibility of \\(H_{a}\\) did not change with the observed data\n\\(BF &gt; 1\\): the plausibility of \\(H_{a}\\) increased with the observed data\n\\(BF &lt; 1\\): the plausibility of \\(H_{a}\\) decreased with the observed data"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#inference-2",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#inference-2",
    "title": "9: Posterior Inference",
    "section": "Inference",
    "text": "Inference\n\n# Bayes factor\nBF &lt;- posterior_odds / prior_odds\nBF\n\n[1] 0\n\n\nThe plausibility of observing hypoxic conditions decreased with the inclusion of the year 2024 data."
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#null-hypothesis-1",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#null-hypothesis-1",
    "title": "9: Posterior Inference",
    "section": "Null Hypothesis",
    "text": "Null Hypothesis\nFor this example, let us claim that the dissolved oxygen concentration level is “close” to 10.7 mg/L\n\\[\\text{H}_{o}: \\mu \\in (10.6, 10.8)\\] \\[\\text{H}_{a}: \\mu \\notin (10.6, 10.8)\\]"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#prior-and-posterior-probabilities-1",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#prior-and-posterior-probabilities-1",
    "title": "9: Posterior Inference",
    "section": "Prior and Posterior Probabilities",
    "text": "Prior and Posterior Probabilities\n\nprior_prob &lt;- diff(pnorm(c(10.6, 10.8),\n                         mean = 9.2027, sd = 2.1623))\n\nposterior_prob &lt;- diff(pnorm(c(10.6, 10.8),\n                             mean = 10.8348, sd = 0.0462))"
  },
  {
    "objectID": "posts/09_posterior_inference/09_posterior_inference.html#bayes-factor-1",
    "href": "posts/09_posterior_inference/09_posterior_inference.html#bayes-factor-1",
    "title": "9: Posterior Inference",
    "section": "Bayes Factor",
    "text": "Bayes Factor\n\nprior_odds &lt;- prior_prob / (1 - prior_prob)\nposterior_odds &lt;- posterior_prob / (1 - posterior_prob)\nBF &lt;- posterior_odds / prior_odds\nBF\n\n[1] 9.747294\n\n\nThe plausibility of observing dissolved oxygen concentration level “near” 10.7 mg/L increased with the inclusion of the year 2024 data."
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html",
    "title": "10: Posterior Prediction",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"bayesplot\")\nlibrary(\"DiagrammeR\")\nlibrary(\"infer\")\nlibrary(\"janitor\")\nlibrary(\"patchwork\")\nlibrary(\"rstan\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#scenario-1",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#scenario-1",
    "title": "10: Posterior Prediction",
    "section": "Scenario 1",
    "text": "Scenario 1\nHow many days per year did we observe hypoxic conditions?\n\ny2022_data |&gt; count(hypoxic)\n\n# A tibble: 2 × 2\n  hypoxic     n\n  &lt;lgl&gt;   &lt;int&gt;\n1 FALSE     231\n2 TRUE        3\n\n\nIn the year 2022, there were 3 days with hypoxic conditions."
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#gamma-prior",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#gamma-prior",
    "title": "10: Posterior Prediction",
    "section": "Gamma Prior",
    "text": "Gamma Prior\nWe can tune a vague gamma prior with a mean of 3 days and a variance of 1 day:\n\\[\\text{E}(\\lambda) = \\displaystyle\\frac{s}{r} = 3 \\text{ and } \\text{Var}(\\lambda) = \\displaystyle\\frac{s}{r^{2}} = 1\\]\nand obtain\n\nshape parameter: \\(s = 9\\)\nrate parameter: \\(r = 3\\)"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#observed-data",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#observed-data",
    "title": "10: Posterior Prediction",
    "section": "Observed Data",
    "text": "Observed Data\n\ny2023_data |&gt; count(hypoxic)\n\n# A tibble: 2 × 2\n  hypoxic     n\n  &lt;lgl&gt;   &lt;int&gt;\n1 FALSE     260\n2 TRUE       27\n\n\nIn the year 2022, there were 27 days with hypoxic conditions."
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#stan",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#stan",
    "title": "10: Posterior Prediction",
    "section": "Stan",
    "text": "Stan\n\nModelSimulationMetricsCode\n\n\n\ngp_model &lt;- \"\n  data {\n    int&lt;lower = 0&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0&gt; lambda;\n  }\n  model {\n    Y ~ poisson(lambda);\n    lambda ~ gamma(9, 3);\n  }\n\"\n\n\n\n\ngp_sim &lt;- stan(model_code = gp_model, \n               data = list(Y = 27), \n               chains = 4, iter = 5000*2, \n               refresh = 0, seed = 320)\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesplot::mcmc_trace(gp_sim, pars = \"lambda\", size = 0.1) +\n  labs(title = \"MCMC Trace\")\np2 &lt;- bayesplot::mcmc_dens_overlay(gp_sim, pars = \"lambda\") +\n  labs(title = \"Density Plots\")\np3 &lt;- bayesplot::mcmc_acf(gp_sim, pars = \"lambda\") +\n  labs(title = \"Autocorrelation\")\n\nn_eff &lt;- bayesplot::neff_ratio(gp_sim, pars = \"lambda\")\n\np4 &lt;- data.frame(x = 0, y = n_eff) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_bar(fill = \"#E77500\", stat = \"identity\") +\n  geom_hline(yintercept = 0.10, color = \"red\", \n             linetype = 2, linewidth = 2) +\n  labs(title = \"Effective Sample Size Ratio\") +\n  theme_minimal()\n\n# patchwork\n(p1 + p2) / (p3 + p4)"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#retrieving-posterior",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#retrieving-posterior",
    "title": "10: Posterior Prediction",
    "section": "Retrieving Posterior",
    "text": "Retrieving Posterior\nWe can perform later calculations with the simulated posterior distribution if we first recast the information into an R data frame.\n\ngp_df &lt;- as.data.frame(gp_sim, pars = \"lambda\")\n\nhead(gp_df)\n\n     lambda\n1 11.563530\n2 11.532309\n3 10.145772\n4 12.085536\n5  8.916516\n6  8.598020"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#sample-statistics",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#sample-statistics",
    "title": "10: Posterior Prediction",
    "section": "Sample Statistics",
    "text": "Sample Statistics\n\ngp_df |&gt;\n  summarize(post_mean = round(mean(lambda), 4),\n            post_median = round(median(lambda), 4),\n            post_sd = round(sd(lambda), 4),\n            lower_95 = round(quantile(lambda, 0.025), 2),\n            upper_95 = round(quantile(lambda, 0.975), 2))\n\n  post_mean post_median post_sd lower_95 upper_95\n1    9.0047      8.9052  1.4928     6.33    12.23"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#theoretical-statistics",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#theoretical-statistics",
    "title": "10: Posterior Prediction",
    "section": "Theoretical Statistics",
    "text": "Theoretical Statistics\n\nbayesrules::summarize_gamma_poisson(shape = 9, rate = 3,\n                                    sum_y = 27, n = 1) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model shape rate mean   mode  var  sd\n1     prior     9    3    3 2.6667 1.00 1.0\n2 posterior    36    4    9 8.7500 2.25 1.5\n\n\n\n# credible interval\nqgamma(c(0.025, 0.975), shape = 36, rate = 4)\n\n[1]  6.303489 12.169132"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#nhst",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#nhst",
    "title": "10: Posterior Prediction",
    "section": "NHST",
    "text": "NHST\nClaim: There are 7 days of hypoxic conditions per year at the ARIK NEON site.\nWe can express this two-sided hypothesis test as\n\\[H_{o}: \\lambda \\in (6,8)\\] \\[H_{a}: \\lambda \\notin (6,8)\\]"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#probabilities",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#probabilities",
    "title": "10: Posterior Prediction",
    "section": "Probabilities",
    "text": "Probabilities\n\nprior_prob &lt;- diff(pgamma(c(6,8), shape = 9, rate = 3))\nposterior_prob &lt;- gp_df |&gt; \n  mutate(in_interval = lambda &gt; 6 & lambda &lt; 8) |&gt; \n  pull(in_interval) |&gt;\n  mean()"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#bayes-factor",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#bayes-factor",
    "title": "10: Posterior Prediction",
    "section": "Bayes Factor",
    "text": "Bayes Factor\n\nprior_odds &lt;- prior_prob / (1 - prior_prob)\nposterior_odds &lt;- posterior_prob / (1 - posterior_prob)\nBF &lt;- posterior_odds / prior_odds\nBF\n\n[1] 46.61742\n\n\nSince the Bayes factor is greater than one, we have found some evidence that it is more plausible that the number of days per year of hypoxic conditions at the ARIK NEON site is between 6 and 8 days."
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#scenario-2",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#scenario-2",
    "title": "10: Posterior Prediction",
    "section": "Scenario 2",
    "text": "Scenario 2\nHow much dissolved oxygen concentration do we find at the ARIK NEON site?\n\ny2022_data |&gt;\n  summarize(prior_mean = round(mean(oxygen), 4),\n            prior_median = round(median(oxygen), 4),\n            prior_sd = round(sd(oxygen), 4),\n            lower_95 = round(quantile(oxygen, 0.025), 2),\n            upper_95 = round(quantile(oxygen, 0.975), 2))\n\n# A tibble: 1 × 5\n  prior_mean prior_median prior_sd lower_95 upper_95\n       &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1       7.73         7.64     2.24     3.47     11.4"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#observed-data-1",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#observed-data-1",
    "title": "10: Posterior Prediction",
    "section": "Observed Data",
    "text": "Observed Data\n\ny2023_data |&gt;\n  summarize(obs_mean = round(mean(oxygen), 4),\n            obs_median = round(median(oxygen), 4),\n            obs_sd = round(sd(oxygen), 4),\n            lower_95 = round(quantile(oxygen, 0.025), 2),\n            upper_95 = round(quantile(oxygen, 0.975), 2),\n            count = n())\n\n# A tibble: 1 × 6\n  obs_mean obs_median obs_sd lower_95 upper_95 count\n     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1     6.73       6.71   2.54      2.2     10.2   287"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#stan-1",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#stan-1",
    "title": "10: Posterior Prediction",
    "section": "Stan",
    "text": "Stan\n\nModelSimulationMetricsCode\n\n\nIn this example, we are estimating the parameter \\(\\mu\\) and assuming a constant standard deviation from the prior.\n\nnn_model &lt;- \"\n  data {\n    real Y[287];\n  }\n  parameters {\n    real mu;\n  }\n  model {\n    Y ~ normal(mu, 2.2375^2);\n    mu ~ normal(7.7274, 2.2375^2);\n  }\n\"\n\n\n\n\nnn_sim &lt;- stan(model_code = nn_model, \n               data = list(Y = y2023_data$oxygen), \n               chains = 4, iter = 5000*2, \n               refresh = 0, seed = 320)\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesplot::mcmc_trace(nn_sim, pars = \"mu\", size = 0.1) +\n  labs(title = \"MCMC Trace\")\np2 &lt;- bayesplot::mcmc_dens_overlay(nn_sim, pars = \"mu\") +\n  labs(title = \"Density Plots\")\np3 &lt;- bayesplot::mcmc_acf(nn_sim, pars = \"mu\") +\n  labs(title = \"Autocorrelation\")\n\nn_eff &lt;- bayesplot::neff_ratio(nn_sim, pars = \"mu\")\n\np4 &lt;- data.frame(x = 0, y = n_eff) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_bar(fill = \"#E77500\", stat = \"identity\") +\n  geom_hline(yintercept = 0.10, color = \"red\", \n             linetype = 2, linewidth = 2) +\n  labs(title = \"Effective Sample Size Ratio\") +\n  theme_minimal()\n\n# patchwork\n(p1 + p2) / (p3 + p4)"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#retrieving-posterior-1",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#retrieving-posterior-1",
    "title": "10: Posterior Prediction",
    "section": "Retrieving Posterior",
    "text": "Retrieving Posterior\nWe can perform later calculations with the simulated posterior distribution if we first recast the information into an R data frame.\n\nnn_df &lt;- as.data.frame(nn_sim, pars = \"mu\")\n\nhead(nn_df)\n\n        mu\n1 6.746397\n2 6.839667\n3 6.901907\n4 7.078138\n5 7.116624\n6 6.511242"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#sample-statistics-1",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#sample-statistics-1",
    "title": "10: Posterior Prediction",
    "section": "Sample Statistics",
    "text": "Sample Statistics\n\nnn_df |&gt;\n  summarize(post_mean = round(mean(mu), 4),\n            post_median = round(median(mu), 4),\n            post_sd = round(sd(mu), 4),\n            lower_95 = round(quantile(mu, 0.025), 2),\n            upper_95 = round(quantile(mu, 0.975), 2))\n\n  post_mean post_median post_sd lower_95 upper_95\n1    6.7309      6.7322  0.2935     6.15      7.3"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#nhst-1",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#nhst-1",
    "title": "10: Posterior Prediction",
    "section": "NHST",
    "text": "NHST\nClaim: At the ARIK NEON site, the average dissolved oxygen concentration level is less than 6 mg/L.\nWe can express this one-sided hypothesis test as\n\\[H_{o}: \\mu &lt; 6.0\\] \\[H_{a}: \\mu \\geq 6.0\\]"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#probabilities-1",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#probabilities-1",
    "title": "10: Posterior Prediction",
    "section": "Probabilities",
    "text": "Probabilities\n\nprior_prob &lt;- pnorm(6, mean = 7.7274, sd = 2.2375)\nposterior_prob &lt;- nn_df |&gt; \n  mutate(in_interval = mu &lt; 6) |&gt; \n  pull(in_interval) |&gt;\n  mean()"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#bayes-factor-1",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#bayes-factor-1",
    "title": "10: Posterior Prediction",
    "section": "Bayes Factor",
    "text": "Bayes Factor\n\nprior_odds &lt;- prior_prob / (1 - prior_prob)\nposterior_odds &lt;- posterior_prob / (1 - posterior_prob)\nBF &lt;- posterior_odds / prior_odds\nBF\n\n[1] 0.02229188\n\n\nSince the Bayes factor is less than one, we have found some evidence that it is less plausible that the average dissolved oxygen concentration level at the ARIK NEON site is less than 6 mg/L."
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#objectives",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#objectives",
    "title": "10: Posterior Prediction",
    "section": "Objectives",
    "text": "Objectives\nIn the second half of the semester, we may revisit this data set to\n\nuse normal regression to predict dissolved oxygen levels\nuse Poisson regression to predict chlorophyll-a levels\nuse logistic regression and Naive Bayes for classification tasks\nuse hierarchical models"
  },
  {
    "objectID": "posts/10_posterior_prediction/10_posterior_prediction.html#pooling",
    "href": "posts/10_posterior_prediction/10_posterior_prediction.html#pooling",
    "title": "10: Posterior Prediction",
    "section": "Pooling",
    "text": "Pooling\n\nFlowchartMermaid Code\n\n\n\n\n\n\n\n\n\n\n\ntri_state_sites_tree &lt;- DiagrammeR::mermaid(\"\n  graph TD\n  \n  oxygen1[oxygen]\n  oxygen2[oxygen]\n  oxygen3[oxygen]\n  oxygen4[oxygen]\n  oxygen5[oxygen]\n  oxygen6[oxygen]\n  temp1[temperature]\n  temp2[temperature]\n  temp3[temperature]\n  temp4[temperature]\n  temp5[temperature]\n  temp6[temperature]\n  \n  walk[WALK]\n  neco[NECO]\n  lewi[LEWI]\n  serc[SERC]\n  hopb[HOPB]\n  harv[HARV]\n  \n  appalachians[Appalachians]\n  mid_atlantic[Mid-Atlantic]\n  northeast[Northeast]\n  \n  response[Response Variable]\n  \n  oxygen1 --&gt; walk\n  temp1   --&gt; walk\n  oxygen2 --&gt; neco\n  temp2   --&gt; neco\n  oxygen3 --&gt; lewi\n  temp3   --&gt; lewi\n  oxygen4 --&gt; serc\n  temp4   --&gt; serc\n  oxygen5 --&gt; hopb\n  temp5   --&gt; hopb\n  oxygen6 --&gt; harv\n  temp6   --&gt; harv\n  \n  walk --&gt; appalachians\n  neco --&gt; appalachians\n  lewi --&gt; mid_atlantic\n  serc --&gt; mid_atlantic\n  hopb --&gt; northeast\n  harv --&gt; northeast\n  \n  appalachians --&gt; response\n  mid_atlantic --&gt; response\n  northeast    --&gt; response\n\")\n\n# print\ntri_state_sites_tree"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#start",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#start",
    "title": "1: Conditional Probability",
    "section": "",
    "text": "Goal: Motivate the Bayesian inversion of events\nObjective: Review conditional probability and metrics\n\n\n\n\n\nBayes Rules"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html",
    "href": "posts/11_normal_regression/11_normal_regression.html",
    "title": "11: Normal Regression",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"bayesplot\")\nlibrary(\"broom.mixed\")\nlibrary(\"patchwork\")\nlibrary(\"rstan\")\nlibrary(\"rstanarm\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#san-francisco-rental-market",
    "href": "posts/11_normal_regression/11_normal_regression.html#san-francisco-rental-market",
    "title": "11: Normal Regression",
    "section": "San Francisco Rental Market",
    "text": "San Francisco Rental Market\n\n\n\nsource: TidyTuesday\n2022-07-05\nPennington, Kate (2018). Bay Area Craigslist Rental Housing Posts, 2000-2018. Retrieved from https://github.com/katepennington/historic_bay_area_craigslist_housing_posts/blob/master/clean_2000_2018.csv.zip.\n\nCraigslist listings\n\n\n\n\n\n\n\n\nSF Counties"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#sf-rent-dataset",
    "href": "posts/11_normal_regression/11_normal_regression.html#sf-rent-dataset",
    "title": "11: Normal Regression",
    "section": "SF Rent Dataset",
    "text": "SF Rent Dataset\n\n\n\nResponse Variable\n\nY: price (in US dollars)\n\n\n\n\n\n\nPredictor Variables\nSome possible predictable variables include\n\narea (sq feet of rental)\nnumber of bedrooms\nnumber of bathrooms\ncounty\nyear\nlatitude/longitude"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#loading",
    "href": "posts/11_normal_regression/11_normal_regression.html#loading",
    "title": "11: Normal Regression",
    "section": "Loading",
    "text": "Loading\n\nrent_raw &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-05/rent.csv')"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#wrangling",
    "href": "posts/11_normal_regression/11_normal_regression.html#wrangling",
    "title": "11: Normal Regression",
    "section": "Wrangling",
    "text": "Wrangling\n\nfiltered out lowest 5 percent and largest 5 percent of rentals (by price)\nfiltered out lowest 5 percent and largest 5 percent of rentals (by area)\nat least year 2009 (i.e. decade from 2009 to 2018)\n55143 observations\n\n\nrent_df &lt;- rent_raw |&gt;\n  filter(price &gt;= 800 & price &lt;= 5000) |&gt;\n  filter(sqft &gt;= 500 & sqft &lt;= 2500) |&gt;\n  filter(year &gt;= 2009)"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#normal-normal",
    "href": "posts/11_normal_regression/11_normal_regression.html#normal-normal",
    "title": "11: Normal Regression",
    "section": "Normal Normal",
    "text": "Normal Normal\nAt first\n\\[\\begin{array}{rcl}\n  Y|\\mu & \\sim & N(\\mu, \\sigma^{2}) \\\\\n  \\mu & \\sim & N(\\theta, \\tau^{2}) \\\\\n\\end{array}\\]\nassumed known variability \\(\\sigma^{2}\\)"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#sigma",
    "href": "posts/11_normal_regression/11_normal_regression.html#sigma",
    "title": "11: Normal Regression",
    "section": "Sigma",
    "text": "Sigma\nStandard deviation \\(\\sigma\\) is another unknown parameter\n\\[\\begin{array}{rcl}\n  Y|\\mu & \\sim & N(\\mu, \\sigma^{2}) \\\\\n  \\mu & \\sim & N(\\theta, \\tau^{2}) \\\\\n  \\sigma & \\sim & \\text{[some other model]}\n\\end{array}\\]"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#local-mean",
    "href": "posts/11_normal_regression/11_normal_regression.html#local-mean",
    "title": "11: Normal Regression",
    "section": "Local Mean",
    "text": "Local Mean\nInstead of estimating a global mean\n\n\\(\\mu\\): average price across all rental sizes\n\nwe can refine the models with a local mean\n\n\\(\\mu_{i} = \\beta_{0} + \\beta_{1}x_{i}\\)\nspecific to the rental size"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#capturing-variability",
    "href": "posts/11_normal_regression/11_normal_regression.html#capturing-variability",
    "title": "11: Normal Regression",
    "section": "Capturing Variability",
    "text": "Capturing Variability\n\nVariabilityCodeLocal\n\n\n\n\n\n\n\n\n\n\np1 &lt;- rent_df |&gt;\n  mutate(nearby = abs(price - (1439 + 0.899*sqft)) &lt; 1000) |&gt;\n  filter(nearby) |&gt;\n  ggplot(aes(x = sqft, y = price)) +\n  geom_point(color = \"gray75\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"High Variability\",\n       x = \"area (square feet)\",\n       y = \"price (USD)\") +\n  theme_minimal() +\n  ylim(0, 5000)\n\np2 &lt;- rent_df |&gt;\n  mutate(nearby = abs(price - (1439 + 0.899*sqft)) &lt; 250) |&gt;\n  filter(nearby) |&gt;\n  ggplot(aes(x = sqft, y = price)) +\n  geom_point(color = \"gray75\") +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"Low Variability\",\n       caption = \"SML 320\",\n       x = \"area (square feet)\",\n       y = \"price (USD)\") +\n  theme_minimal() +\n  ylim(0, 5000)\n\n# patchwork\np1 + p2\n\n\n\nParameter \\(\\sigma\\) now represents variability from the local mean\n\\[\\mu_{i} = \\beta_{0} + \\beta_{1}x_{i}\\]"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#normal-data-model-assumptions",
    "href": "posts/11_normal_regression/11_normal_regression.html#normal-data-model-assumptions",
    "title": "11: Normal Regression",
    "section": "Normal Data Model Assumptions",
    "text": "Normal Data Model Assumptions\n\\[Y|\\beta_{0}, \\beta_{1}, \\sigma \\sim \\mu_{i} = \\beta_{0} + \\beta_{1}X_{i}\\]\n\nStructure of the data: Accounting for predictor \\(X\\), the observed data \\(Y_i\\) on case \\(i\\) is independent of the observed data on any other case \\(j\\).\nStructure of the relationship: The typical \\(Y\\) outcome can be written as a linear function of predictor \\(X\\), \\(\\mu=\\beta_{0} + \\beta_{1}X\\).\nStructure of the variability: At any value of predictor \\(X\\), the observed values of \\(Y\\) will vary normally around their average \\(\\mu\\) with consistent standard deviation \\(\\sigma\\)."
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#tuning-priors",
    "href": "posts/11_normal_regression/11_normal_regression.html#tuning-priors",
    "title": "11: Normal Regression",
    "section": "Tuning Priors",
    "text": "Tuning Priors\n\nIntuitionPrior DistsPrior ModelSimulation Space\n\n\nDomain expertise would help an analyst create some informative priors for the parameters. Otherwise, I will temporarily use haphazard code (below) to ascertain that\n\nrental prices tend to be between 900 and 1200 dollars per month\nfor every one square foot increase in area, the rental price tends to increase between -5 and 47 cents\nrental prices tend to vary with a standard deviation of about 90 dollars\n\n\nset.seed(320)\nN &lt;- 1000 # sample size\n\nslopes &lt;- rep(NA, N)\nintercepts &lt;- rep(NA, N)\nranges &lt;- rep(NA, N)\ncenters &lt;- rep(NA,N)\n\nfor(i in 1:N){\n  two_houses &lt;- rent_df |&gt;\n    slice_sample(n = 2) |&gt;\n    select(price, sqft)\n  \n  x1 &lt;- two_houses[1,1] |&gt; unlist()\n  x2 &lt;- two_houses[2,1] |&gt; unlist()\n  y1 &lt;- two_houses[1,2] |&gt; unlist()\n  y2 &lt;- two_houses[2,2] |&gt; unlist()\n  \n  if(x1 == x2){\n    slopes[i] &lt;- NA\n    intercepts[i] &lt;- NA\n  }else{\n    slopes[i] &lt;- (y2 - y1) / (x2 - x1)\n    intercepts[i] &lt;- y1 - slopes[i]*x1\n  }\n  \n  ranges[i] &lt;- abs(y2 - y1)\n  centers[i] &lt;- (y1 + y2) / 2\n}\n\nprint(\"Intercept estimates\")\n\n[1] \"Intercept estimates\"\n\nprint(round(quantile(intercepts, c(0.3, 0.5, 0.7), na.rm = TRUE)))\n\n 30%  50%  70% \n   6  547 1142 \n\nprint(\"Slope estimates\")\n\n[1] \"Slope estimates\"\n\nprint(quantile(slopes, c(0.3, 0.5, 0.7), na.rm = TRUE))\n\n       30%        50%        70% \n-0.0548437  0.1613792  0.4677822 \n\nprint(\"Deviation estimates\")\n\n[1] \"Deviation estimates\"\n\nprint(round(quantile(ranges/4, c(0.3, 0.5, 0.7), na.rm = TRUE)))\n\n30% 50% 70% \n 43  88 142 \n\nprint(\"Centered intercept estimates\")\n\n[1] \"Centered intercept estimates\"\n\nprint(round(quantile(centers, c(0.3, 0.5, 0.7), na.rm = TRUE)))\n\n 30%  50%  70% \n 901 1041 1203 \n\n\n\n\n\np1 &lt;- bayesrules::plot_normal(mean = 547, sd = 1142-547) + \n  labs(title = \"intercept\", x = \"beta_0\", y = \"\") +\n  theme_minimal()\np2 &lt;- bayesrules::plot_normal(mean = 0.16, sd = 0.47-0.16) + \n  labs(title = \"slope\", x = \"beta_1\", y = \"\") +\n  theme_minimal()\np3 &lt;- bayesrules::plot_gamma(shape = 1, rate = 1/88) + \n  labs(title = \"deviation\", x = \"sigma\", y = \"\") +\n  theme_minimal()\n\n# patchwork\np1 + p2 + p3\n\n\n\n\n\n\n\\[\\begin{array}{rcl}\n  Y_{i}|\\beta_{0}, \\beta_{1}, \\sigma & \\sim & N(\\mu_{i}, \\sigma^{2}) \\text{ with } \\mu_{i} = \\beta_{0} + \\beta_{1}X_{1} \\\\\n  \\beta_{0} & \\sim & N(547, 597^{2}) \\\\\n  \\beta_{1} & \\sim & N(0.16, 0.31^{2}) \\\\\n  \\sigma & \\sim & \\text{Exp}(1/88) \\\\\n\\end{array}\\]\n\n\n\nfor(i in 1:25){\n  this_plot &lt;- rent_df |&gt;\n    ggplot(aes(x = sqft, y = price)) +\n    geom_point(color = \"white\") +\n    geom_abline(intercept = rnorm(1, 547, 597),\n                slope = rnorm(1, 0.16, 0.31),\n                color = \"red\") +\n    labs(title = \"Prior Models\",\n         subtitle = \"Simulation Space\",\n         caption = \"SML 320\",\n         x = \"area (square feet)\",\n         y = \"price (USD)\") +\n    theme_minimal()\n  \n  ggsave(filename = paste0(\"for_animations/lin_sim_plot_\", i, \".png\"),\n         plot = this_plot)\n}\n\n# then combined using the website https://ezgif.com/"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#rstan",
    "href": "posts/11_normal_regression/11_normal_regression.html#rstan",
    "title": "11: Normal Regression",
    "section": "rstan",
    "text": "rstan\n\nDefine ModelSimulationMetricsPosterior Stats\n\n\n\nrent_stan_model &lt;- \"\n  data {\n    int&lt;lower = 0&gt; n;\n    vector[n] Y;\n    vector[n] X;\n  }\n  parameters {\n    real beta0;\n    real beta1;\n    real&lt;lower = 0&gt; sigma;\n  }\n  model {\n    Y ~ normal(beta0 + beta1 * X, sigma);\n    beta0 ~ normal(547, 597);\n    beta1 ~ normal(0.16, 0.31);\n    sigma ~ exponential(0.01136364);\n  }\n\"\n\n\n\n\ntime_start &lt;- Sys.time()\n\nrent_df_for_stan &lt;- rent_df |&gt;\n  slice_sample(prop = 0.05)\n\nrent_stan_sim &lt;- \n  stan(model_code = rent_stan_model, \n       data = list(n = nrow(rent_df_for_stan), \n                   Y = rent_df_for_stan$price, \n                   X = rent_df_for_stan$sqft), \n       chains = 4, iter = 5000*2, refresh = 0, seed = 320)\n\ntime_end &lt;- Sys.time()\ntime_end - time_start\n\nTime difference of 58.85536 secs\n\n\n\n\n\nbayesplot::mcmc_trace(rent_stan_sim, \n                      pars = c(\"beta0\", \"beta1\", \"sigma\"), \n                      size = 0.1) +\n  labs(title = \"MCMC Traces\")\n\n\n\n\n\nbayesplot::mcmc_dens_overlay(rent_stan_sim,\n                             pars = c(\"beta0\", \"beta1\", \"sigma\")) +\n  labs(title = \"Density Plots\")\n\n\n\n\n\nbayesplot::mcmc_acf(rent_stan_sim,\n                    pars = c(\"beta0\", \"beta1\", \"sigma\")) +\n  labs(title = \"Autocorrelations\")\n\n\n\n\n\nbayesplot::neff_ratio(rent_stan_sim,\n                      pars = c(\"beta0\", \"beta1\", \"sigma\"))\n\n    beta0     beta1     sigma \n0.3692006 0.3690437 0.4980528 \n\n\n\nbayesplot::rhat(rent_stan_sim,\n                pars = c(\"beta0\", \"beta1\", \"sigma\"))\n\n    beta0     beta1     sigma \n0.9999020 0.9998856 1.0001496 \n\n\n\n\n\nbroom.mixed::tidy(rent_stan_sim,\n                  effects = c(\"fixed\", \"aux\"),\n                  conf.int = TRUE, conf.level = 0.80) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n# A tibble: 3 × 5\n  term  estimate std.error conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 beta0 1482.      42.5    1428.     1537.   \n2 beta1    0.872    0.0371    0.824     0.919\n3 sigma  805.      10.8     791.      819."
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#rstan-1",
    "href": "posts/11_normal_regression/11_normal_regression.html#rstan-1",
    "title": "11: Normal Regression",
    "section": "rstan",
    "text": "rstan\n\n\n# A tibble: 3 × 3\n  term  estimate std.error\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 beta0 1482.      42.5   \n2 beta1    0.872    0.0371\n3 sigma  805.      10.8"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#rstanarm-1",
    "href": "posts/11_normal_regression/11_normal_regression.html#rstanarm-1",
    "title": "11: Normal Regression",
    "section": "rstanarm",
    "text": "rstanarm\n\n\n# A tibble: 4 × 3\n  term        estimate std.error\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 1527.      42.9   \n2 sqft           0.803    0.0367\n3 sigma        809.      10.8   \n4 mean_PPD    2390.      21.6"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#rstanarm-2",
    "href": "posts/11_normal_regression/11_normal_regression.html#rstanarm-2",
    "title": "11: Normal Regression",
    "section": "rstanarm",
    "text": "rstanarm\n\n\n# A tibble: 4 × 3\n  term        estimate std.error\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 1527.      42.9   \n2 sqft           0.803    0.0367\n3 sigma        809.      10.8   \n4 mean_PPD    2390.      21.6"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#autoscale-1",
    "href": "posts/11_normal_regression/11_normal_regression.html#autoscale-1",
    "title": "11: Normal Regression",
    "section": "autoscale",
    "text": "autoscale\n\n\n# A tibble: 4 × 3\n  term        estimate std.error\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 1505.       44.0  \n2 sqft           0.862     0.038\n3 sigma        828.       11.4  \n4 mean_PPD    2444.       22.2"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#many-models",
    "href": "posts/11_normal_regression/11_normal_regression.html#many-models",
    "title": "11: Normal Regression",
    "section": "Many Models",
    "text": "Many Models\n\nrent_model_df &lt;- as.data.frame(rent_autoscale_sim)\n\n# nrow(rent_model_df)\n# 20000 models from 20000 chains\n\nhead(rent_model_df)\n\n  (Intercept)      sqft    sigma\n1    1605.731 0.8024525 823.1324\n2    1595.615 0.8117000 822.9240\n3    1429.658 0.9107199 835.7474\n4    1536.464 0.8128524 835.6783\n5    1463.692 0.8782975 835.7370\n6    1476.271 0.8683633 833.1336"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#many-lines",
    "href": "posts/11_normal_regression/11_normal_regression.html#many-lines",
    "title": "11: Normal Regression",
    "section": "Many Lines",
    "text": "Many Lines\n\nPlotCode\n\n\n\n\n\n\nfor(i in 1:25){\n  \n  beta_0 &lt;- rent_model_df$`(Intercept)`[i]\n  beta_1 &lt;- rent_model_df$sqft[i]\n  \n  this_plot &lt;- rent_df |&gt;\n    ggplot(aes(x = sqft, y = price)) +\n    geom_point(color = \"gray75\") +\n    geom_vline(xintercept = 1500, color = \"blue\", linewidth = 3) +\n    geom_abline(intercept = beta_0, slope = beta_1,\n                color = \"red\", linewidth = 3) +\n    geom_point(x = 1500, y = beta_0 + beta_1*1500,\n               color = \"purple\", size = 5) +\n    labs(title = \"Bayesian Regression\",\n         subtitle = \"What is the expected monthly rent for a property of 1500 square feet in size?\",\n         caption = \"Source: Tidy Tuesday\",\n         x = \"area (square feet)\",\n         y = \"price (USD)\") +\n    theme_minimal()\n  \n  ggsave(filename = paste0(\"for_animations/bayes_reg_plot_\", i, \".png\"),\n         plot = this_plot)\n}\n\n# then combined using the website https://ezgif.com/"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#evidence",
    "href": "posts/11_normal_regression/11_normal_regression.html#evidence",
    "title": "11: Normal Regression",
    "section": "Evidence",
    "text": "Evidence\n\nObjectiveVisualCredible IntervalPosterior Probability\n\n\nDo we have posterior evidence that there is a positive relationship between area and price?\n\\[\\beta_{1} &gt; 0?\\]\n\n\nIt appears that all of our slopes are positive.\n\n\n\nThe credible interval does not contain zero.\n\nbroom.mixed::tidy(rent_autoscale_sim,\n                  effects = c(\"fixed\", \"aux\"),\n                  conf.int = TRUE, conf.level = 0.80) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n# A tibble: 4 × 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 1505.       44.0   1447.     1561.   \n2 sqft           0.862     0.038    0.813     0.911\n3 sigma        828.       11.4    813.      842.   \n4 mean_PPD    2444.       22.2   2416.     2472.   \n\n\n\n\n\\[P(\\beta_{1} &gt; 0 | \\vec{y}) \\approx 1\\]\n\nrent_model_df |&gt;\n  mutate(slope_exceeds_0 = sqft &gt; 0) |&gt;\n  janitor::tabyl(slope_exceeds_0) |&gt;\n  janitor::adorn_totals()\n\n slope_exceeds_0     n percent\n            TRUE 20000       1\n           Total 20000       1"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#posterior-predictive-model",
    "href": "posts/11_normal_regression/11_normal_regression.html#posterior-predictive-model",
    "title": "11: Normal Regression",
    "section": "Posterior Predictive Model",
    "text": "Posterior Predictive Model\n\\[Y_{\\text{new}} | \\beta_{0}, \\beta_{1}, \\sigma \\sim N(\\mu, \\sigma^{2}) \\text{ with } \\mu = \\beta_{0} + \\beta_{1}*1500\\]\n\nset.seed(320)\nrent_predict_df &lt;- rent_model_df |&gt;\n  mutate(mu = `(Intercept)` + sqft*1500,\n         y_new = rnorm(20000, mean = mu, sd = sigma))"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#many-models-1",
    "href": "posts/11_normal_regression/11_normal_regression.html#many-models-1",
    "title": "11: Normal Regression",
    "section": "Many Models",
    "text": "Many Models\n\nhead(rent_predict_df)\n\n  (Intercept)      sqft    sigma       mu    y_new\n1    1605.731 0.8024525 823.1324 2809.410 1096.676\n2    1595.615 0.8117000 822.9240 2813.165 1835.540\n3    1429.658 0.9107199 835.7474 2795.738 2218.671\n4    1536.464 0.8128524 835.6783 2755.743 3398.422\n5    1463.692 0.8782975 835.7370 2781.139 2335.670\n6    1476.271 0.8683633 833.1336 2778.816 2966.288"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#intervals",
    "href": "posts/11_normal_regression/11_normal_regression.html#intervals",
    "title": "11: Normal Regression",
    "section": "Intervals",
    "text": "Intervals\n\nrent_predict_df |&gt;\n  summarize(lower_mu = quantile(mu, 0.025),\n            upper_mu = quantile(mu, 0.975),\n            lower_new = quantile(y_new, 0.025),\n            upper_new = quantile(y_new, 0.975))\n\n  lower_mu upper_mu lower_new upper_new\n1 2753.982 2841.034  1153.325  4437.741\n\n\n\ntypical rent: (2754, 2841)\nposterior prediction interval: (1153, 4437)"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#averaging",
    "href": "posts/11_normal_regression/11_normal_regression.html#averaging",
    "title": "11: Normal Regression",
    "section": "Averaging",
    "text": "Averaging\n\nrent_predict_df |&gt;\n  ggplot(aes(x = mu)) +\n  geom_density(fill = \"purple\") +\n  labs(title = \"Typical Rent\",\n       subtitle = \"distribution for mu\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/11_normal_regression/11_normal_regression.html#distributions",
    "href": "posts/11_normal_regression/11_normal_regression.html#distributions",
    "title": "11: Normal Regression",
    "section": "Distributions",
    "text": "Distributions\n\nPlotsCode"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html",
    "title": "12: Evaluating Regression Models",
    "section": "",
    "text": "Goal: Discuss the fairness and accuracy of Bayesian regression models"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#how-was-the-data-collected",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#how-was-the-data-collected",
    "title": "12: Evaluating Regression Models",
    "section": "How was the data collected?",
    "text": "How was the data collected?\n\nThousands of listings scraped from Craiglist—a freely available retail message board.\nsome lack of confidentiality:\n\nuser names removed\naddresses could be scraped"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#by-whom-and-for-what-purpose-was-the-data-collected",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#by-whom-and-for-what-purpose-was-the-data-collected",
    "title": "12: Evaluating Regression Models",
    "section": "By whom and for what purpose was the data collected?",
    "text": "By whom and for what purpose was the data collected?\n\ncollected by Dr Kate Pennington, research economist at the US Census Bureau in the Center for Economic Studies\non Gentrification and Displacement in San Francisco"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#how-might-the-results-of-the-analysis-or-the-data-collection-itself-impact-individuals-and-society",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#how-might-the-results-of-the-analysis-or-the-data-collection-itself-impact-individuals-and-society",
    "title": "12: Evaluating Regression Models",
    "section": "How might the results of the analysis, or the data collection itself, impact individuals and society?",
    "text": "How might the results of the analysis, or the data collection itself, impact individuals and society?\n\nresearch investigation, publication, and communication may affect rental market; but probably differently than other media"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#what-biases-might-be-baked-into-this-analysis",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#what-biases-might-be-baked-into-this-analysis",
    "title": "12: Evaluating Regression Models",
    "section": "What biases might be baked into this analysis?",
    "text": "What biases might be baked into this analysis?\n\nWhile Craiglist webscraping produced a sizeable data set, lack of information from other rental market communications may detract from diagnosing trends in market behavior."
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#model-assumptions",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#model-assumptions",
    "title": "12: Evaluating Regression Models",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nrentals are independent observations (not true in reality)\nlinear model between predictor \\(X\\) and price \\(Y\\)\n\nlast time: \\(\\beta_{1} &gt; 0\\)\n\nAt any \\(X\\) value, \\(Y\\) varies normally around \\(\\mu\\) with consistent variability \\(\\sigma\\).\n\nif these assumptions are valid, then the simulated posterior distribution should model the data well"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#exploratory-data-analysis",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#exploratory-data-analysis",
    "title": "12: Evaluating Regression Models",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nVariablesVisualizationCodeMLS\n\n\n\nresponse: price (monthly rent in USD)\npredictor: year (discrete, 2009 to 2018)\ncommentary: the dataset allows a refined investigation of a time series, but we are going for an intentionally flawed analysis for this lecture session\n\n\n\n\n\n\n\n\n\n\n\nrent_df$year_fac &lt;- factor(rent_df$year)\n\nrent_df |&gt;\n  ggplot(aes(x = year_fac, y = price, color = year_fac)) +\n  ggbeeswarm::geom_quasirandom() +\n  labs(title = \"SF Rental Market\",\n       subtitle = \"2009 to 2018\",\n       caption = \"Source: TidyTuesday\",\n       x = \"year\",\n       y = \"monthly rent (USD)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#mcmc",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#mcmc",
    "title": "12: Evaluating Regression Models",
    "section": "MCMC",
    "text": "MCMC\n\nIf the priors are not specified, rstanarm will create weakly informative priors through its autoscale process\n\n\nset.seed(320)\nrent_df_for_stan &lt;- rent_df |&gt;\n  slice_sample(prop = 0.10)\n\nmodel_area &lt;- stan_glm(price ~ sqft, data = rent_df_for_stan,\n                       chains = 4, iter = 5000*2, refresh = 0, seed = 320)\nmodel_year &lt;- stan_glm(price ~ year, data = rent_df_for_stan,\n                       chains = 4, iter = 5000*2, refresh = 0, seed = 320)"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#priors",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#priors",
    "title": "12: Evaluating Regression Models",
    "section": "Priors",
    "text": "Priors\n\narea modelyear model\n\n\n\nrstanarm::prior_summary(model_area)\n\nPriors for model 'model_area' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 2405, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 2405, scale = 2227)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 5.4)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.0011)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\n\n\nrstanarm::prior_summary(model_year)\n\nPriors for model 'model_year' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 2405, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 2405, scale = 2227)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 1083)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.0011)\n------\nSee help('prior_summary.stanreg') for more details"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#area-model-1",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#area-model-1",
    "title": "12: Evaluating Regression Models",
    "section": "Area Model",
    "text": "Area Model\n\nOne ModelSimulationOne SimulationDensity\n\n\n\nmodel_area_df &lt;- data.frame(model_area)\nfirst_set &lt;- head(model_area_df, 1)\nfirst_set\n\n  X.Intercept.      sqft    sigma\n1     1408.002 0.9145315 805.5614\n\n\n\n\n\nbeta_0 &lt;- first_set$`X.Intercept.`\nbeta_1 &lt;- first_set$sqft\nsigma  &lt;- first_set$sigma\nset.seed(320)\none_simulation &lt;- rent_df_for_stan |&gt;\n  mutate(mu = beta_0 + beta_1 * sqft,\n         simulated_price = rnorm(nrow(rent_df_for_stan), \n                                 mean = mu, sd = sigma)) |&gt;\n  select(sqft, price, simulated_price)\n\n\n\n\nhead(one_simulation, 5)\n\n# A tibble: 5 × 3\n   sqft price simulated_price\n  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1  1346  1390            963.\n2  1570  2950           1887.\n3   700  2595           1492.\n4  1046  2008           2984.\n5  1000  2650           1893.\n\n\n\n\n\n\n\n\n\n\nsubtitle_string &lt;- \"&lt;span style='color:#BF40BF'&gt;Real rents&lt;/span&gt; versus &lt;span style='color:#7f7f7f'&gt;simulated rents&lt;/span&gt;\"\n\none_simulation |&gt;\n  ggplot() +\n  geom_density(aes(x = price), color = \"#BF40BF\", linewidth = 3) +\n  geom_density(aes(x = simulated_price), color = \"#7f7f7f\", linewidth = 2) +\n  labs(title = \"Model: price versus area\",\n       subtitle = subtitle_string,\n       caption = \"SML 320\") +\n  theme_minimal() +\n  theme(axis.title.y = element_blank(),\n        axis.text.y  = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.subtitle = element_markdown()) #use ggtext package"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#year-model-1",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#year-model-1",
    "title": "12: Evaluating Regression Models",
    "section": "Year Model",
    "text": "Year Model\n\nOne ModelSimulationOne SimulationDensity\n\n\n\nmodel_year_df &lt;- data.frame(model_year)\nfirst_set &lt;- head(model_year_df, 1)\nfirst_set\n\n  X.Intercept.     year    sigma\n1      -316870 158.5494 826.6045\n\n\n\n\n\nbeta_0 &lt;- first_set$`X.Intercept.`\nbeta_1 &lt;- first_set$year\nsigma  &lt;- first_set$sigma\nset.seed(320)\none_simulation &lt;- rent_df_for_stan |&gt;\n  mutate(mu = beta_0 + beta_1 * year,\n         simulated_price = rnorm(nrow(rent_df_for_stan), \n                                 mean = mu, sd = sigma)) |&gt;\n  select(year, price, simulated_price)\n\n\n\n\nhead(one_simulation, 5)\n\n# A tibble: 5 × 3\n   year price simulated_price\n  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;\n1  2014  3815            729.\n2  2012  1177           1149.\n3  2012  1888           1561.\n4  2012  2500           2767.\n5  2016  4150           2325.\n\n\n\n\n\n\n\n\n\n\nsubtitle_string &lt;- \"&lt;span style='color:#BF40BF'&gt;Real rents&lt;/span&gt; versus &lt;span style='color:#7f7f7f'&gt;simulated rents&lt;/span&gt;\"\n\none_simulation |&gt;\n  ggplot() +\n  geom_density(aes(x = price), color = \"#BF40BF\", linewidth = 3) +\n  geom_density(aes(x = simulated_price), color = \"#7f7f7f\", linewidth = 2) +\n  labs(title = \"Model: price versus year\",\n       subtitle = subtitle_string,\n       caption = \"SML 320\") +\n  theme_minimal() +\n  theme(axis.title.y = element_blank(),\n        axis.text.y  = element_blank(),\n        axis.ticks.y = element_blank(),\n        plot.subtitle = element_markdown()) #use ggtext package"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#different-data-structure",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#different-data-structure",
    "title": "12: Evaluating Regression Models",
    "section": "Different Data Structure",
    "text": "Different Data Structure\n\nNormal\nPoisson\nNegative Binomial\nBinomial\netc."
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#transformations",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#transformations",
    "title": "12: Evaluating Regression Models",
    "section": "Transformations",
    "text": "Transformations\n\ntransform \\(Y\\): \\[g(Y_{i}) | \\beta_{0}, \\beta_{1}, \\sigma \\sim N(\\mu, \\sigma^{2}) \\text{ with } \\mu_{i} = \\beta_{0} + \\beta_{1}X_{i}\\]\ntransform \\(X\\): \\[Y_{i} | \\beta_{0}, \\beta_{1}, \\sigma \\sim N(\\mu, \\sigma^{2}) \\text{ with } \\mu_{i} = \\beta_{0} + \\beta_{1}h(X_{i})\\]\ntransform both \\(X\\) and \\(Y\\): \\[g(Y_{i}) | \\beta_{0}, \\beta_{1}, \\sigma \\sim N(\\mu, \\sigma^{2}) \\text{ with } \\mu_{i} = \\beta_{0} + \\beta_{1}h(X_{i})\\]\n\nwhere functions \\(g,h\\) might take forms like \\(\\log(X), \\sqrt{X}, X^{2}, X^{3},\\) etc."
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#predictions",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#predictions",
    "title": "12: Evaluating Regression Models",
    "section": "Predictions",
    "text": "Predictions\nSimilar to the predict command in base-R, the posterior_predict command in rstanarm takes a model and a data frame.\n\nmodel_area_predictions &lt;- rstanarm::posterior_predict(\n  model_area, newdata = rent_df_for_stan\n)\n\n\nmodel_year_predictions &lt;- rstanarm::posterior_predict(\n  model_year, newdata = rent_df_for_stan\n)\n\n\ndim(model_area_predictions)\n\n[1] 20000  5514"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#metrics",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#metrics",
    "title": "12: Evaluating Regression Models",
    "section": "Metrics",
    "text": "Metrics\n\nmedian absolute error\n\n\\[\\text{MAE} = \\text{median}|Y_{i} - Y_{i}^{'}|\\]\n\nscaled median absolute error\n\n\\[\\text{MAE scaled} = \\text{median}\\frac{|Y_{i} - Y_{i}^{'}|}{\\text{sd}_{i}}\\]\n\nwithin 50: proportion of observed values \\(Y_{i}\\) that fall within their 50% posterior prediction interval\nwithin 95: proportion of observed values \\(Y_{i}\\) that fall within their 95% posterior prediction interval"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#posterior-predictive-summaries",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#posterior-predictive-summaries",
    "title": "12: Evaluating Regression Models",
    "section": "Posterior Predictive Summaries",
    "text": "Posterior Predictive Summaries\nThe textbook authors made another helper function in their bayesrules package called prediction_summary\n\nset.seed(320)\nrent_df_for_ML &lt;- rent_df |&gt;\n  slice_sample(prop = 0.01)\nbayesrules::prediction_summary(model_area, \n                               data = rent_df_for_ML)\n\n       mae mae_scaled within_50 within_95\n1 529.2686   0.649922 0.5172414 0.9491833\n\n\n\nbayesrules::prediction_summary(model_year, \n                               data = rent_df_for_ML)\n\n       mae mae_scaled within_50 within_95\n1 566.7108  0.6837057 0.4936479 0.9455535"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#training-and-testing-sets",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#training-and-testing-sets",
    "title": "12: Evaluating Regression Models",
    "section": "Training and Testing Sets",
    "text": "Training and Testing Sets\n\nallocate about 75% of observations into a training set\n\nbuild models from the training set\n\nallocate other 25% of observations into a testing set\n\nmeasure error on the testing set\n\n\n\nrent_split &lt;- rsample::initial_split(rent_df_for_ML)\nrent_train &lt;- rsample::training(rent_split) #about 75% of observations\nrent_test  &lt;- rsample::testing(rent_split)  #about 25% of observations\n\n\nsubtitle_string &lt;- \"&lt;span style='color:#7f7f7f'&gt;Training data&lt;/span&gt; and &lt;span style='color:#FF0000'&gt;testing data&lt;/span&gt;\"\n\nrent_train |&gt;\n    ggplot(aes(x = sqft, y = price)) +\n    geom_point(color = \"#7f7f7f\") +\n    geom_smooth(method = \"lm\", color = \"black\", se = FALSE) +\n    geom_point(aes(x = sqft, y = price),\n               color = \"#FF0000\", data = rent_test, size = 4) +\n    labs(title = \"Machine Learning on SF Rent Data\",\n         subtitle = subtitle_string,\n         caption = \"Source: Tidy Tuesday\",\n         x = \"area (square feet)\",\n         y = \"price (USD)\") +\n    theme_minimal() +\n    theme(plot.subtitle = element_markdown())"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#bias-variance-trade-off",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#bias-variance-trade-off",
    "title": "12: Evaluating Regression Models",
    "section": "Bias Variance Trade-Off",
    "text": "Bias Variance Trade-Off\n\n\n\nbias-variance trade-off\n\n\nimage source"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#cross-validation",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#cross-validation",
    "title": "12: Evaluating Regression Models",
    "section": "Cross Validation",
    "text": "Cross Validation\nBetter measurement of errors comes from cross-validation\n\n\n\ncross validation\n\n\nimage source"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#training-and-testing-data",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#training-and-testing-data",
    "title": "12: Evaluating Regression Models",
    "section": "Training and Testing Data",
    "text": "Training and Testing Data\n\n\n\ntraining and testing sets\n\n\n\nsubtitle_string &lt;- \"&lt;span style='color:#7f7f7f'&gt;Training data&lt;/span&gt; and &lt;span style='color:#FF0000'&gt;testing data&lt;/span&gt;\"\n\nfor(i in 1:25){\n  \n  rent_split &lt;- rsample::initial_split(rent_df_for_ML)\n  rent_train &lt;- rsample::training(rent_split) #about 75% of observations\n  rent_test  &lt;- rsample::testing(rent_split)  #about 25% of observations\n  \n  this_plot &lt;- rent_train |&gt;\n    ggplot(aes(x = sqft, y = price)) +\n    geom_point(color = \"#7f7f7f\") +\n    geom_smooth(method = \"lm\", color = \"black\", se = FALSE) +\n    geom_point(aes(x = sqft, y = price),\n               color = \"#FF0000\", data = rent_test, size = 4) +\n    labs(title = \"Machine Learning on SF Rent Data\",\n         subtitle = subtitle_string,\n         caption = \"Source: Tidy Tuesday\",\n         x = \"area (square feet)\",\n         y = \"price (USD)\") +\n    theme_minimal() +\n    theme(plot.subtitle = element_markdown()) #use ggtext package\n  \n  ggsave(filename = paste0(\"for_animations/ML_reg_plot_\", i, \".png\"),\n         plot = this_plot)\n}\n\n# then combined using the website https://ezgif.com/"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#cross-validation-through-bayesrules",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#cross-validation-through-bayesrules",
    "title": "12: Evaluating Regression Models",
    "section": "Cross Validation through bayesrules",
    "text": "Cross Validation through bayesrules\n\nset.seed(320)\n\nmodel_area_cv &lt;- bayesrules::prediction_summary_cv(\n  model = model_area, data = rent_df_for_ML, k = 10\n)\n\nmodel_year_cv &lt;- bayesrules::prediction_summary_cv(\n  model = model_year, data = rent_df_for_ML, k = 10\n)\n\nmodel_year_cv$folds\n\n   fold      mae mae_scaled within_50 within_95\n1     1 565.1712  0.6681426 0.5000000 0.9821429\n2     2 483.4587  0.5717974 0.6363636 0.9454545\n3     3 492.0445  0.5862028 0.5454545 0.9272727\n4     4 641.4328  0.7601855 0.4363636 0.9818182\n5     5 666.7055  0.7885234 0.3636364 0.9818182\n6     6 584.7050  0.6943528 0.4909091 0.9454545\n7     7 715.9861  0.8555493 0.4181818 0.9454545\n8     8 483.1197  0.5810148 0.5818182 0.9272727\n9     9 676.2278  0.8031427 0.4363636 0.9272727\n10   10 463.2350  0.5560119 0.5454545 0.9272727"
  },
  {
    "objectID": "posts/12_evaluating_regression/12_evaluating_regression.html#cross-validated",
    "href": "posts/12_evaluating_regression/12_evaluating_regression.html#cross-validated",
    "title": "12: Evaluating Regression Models",
    "section": "Cross-Validated",
    "text": "Cross-Validated\nThe prediction_summary_cv function in the bayesrules package will also average the results across all \\(k\\) folds and store that information into cv.\n\nmodel_area_cv$cv\n\n       mae mae_scaled within_50 within_95\n1 519.9732  0.6470611 0.5244156 0.9474675\n\n\n\nmodel_year_cv$cv\n\n       mae mae_scaled within_50 within_95\n1 577.2086  0.6864923 0.4954545 0.9491234"
  },
  {
    "objectID": "posts/13_extending_regression/13_extending_regression.html",
    "href": "posts/13_extending_regression/13_extending_regression.html",
    "title": "13: Extending Regression Models",
    "section": "",
    "text": "Goal: Explore categorical variables, multivariate models, and interaction terms.\n\n\n\n\n\n\n\n\n\n\n\nsource: TidyTuesday\n2022-07-05\nPennington, Kate (2018). Bay Area Craigslist Rental Housing Posts, 2000-2018. Retrieved from this site.\n\nCraigslist listings\n\n\n\n\n\n\n\nlibrary(\"bayesrules\")\nlibrary(\"bayesplot\")\nlibrary(\"gt\")\nlibrary(\"patchwork\")\nlibrary(\"rstan\")\nlibrary(\"rstanarm\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\nrent_raw &lt;- rent_raw &lt;- readr::read_csv(\"rent.csv\")\nrent_df &lt;- rent_raw |&gt;\n  filter(price &gt;= 800 & price &lt;= 5000) |&gt;\n  filter(sqft &gt;= 500 & sqft &lt;= 2500) |&gt;\n  filter(year &gt;= 2009) |&gt;\n  filter(county %in% c(\"alameda\", \"contra costa\"))\n\nrent_df$county &lt;- factor(rent_df$county,\n                         levels = c(\"alameda\", \"contra costa\"))\n\nset.seed(320)\nrent_df_for_stan &lt;- rent_df |&gt;\n  slice_sample(prop = 0.05)\n\n\n\n\n\n\n\n\n\n\n\\(Y\\): price (monthly rent in USD)\n\n\n\nWhat is the monthly rent for a property that is about 1500 square feet in size?\n\n\n\n\n\n\n\n\\(X_{1}\\): area (in square feet)\n\\(X_{2}\\): year (discrete, numerical)\n\\(X_{3}\\): county\n\\(X_{4}\\): number of bedrooms\n\\(X_{5}\\): number of bathrooms"
  },
  {
    "objectID": "posts/13_extending_regression/13_extending_regression.html#data",
    "href": "posts/13_extending_regression/13_extending_regression.html#data",
    "title": "13: Extending Regression Models",
    "section": "",
    "text": "source: TidyTuesday\n2022-07-05\nPennington, Kate (2018). Bay Area Craigslist Rental Housing Posts, 2000-2018. Retrieved from this site.\n\nCraigslist listings\n\n\n\n\n\n\n\nlibrary(\"bayesrules\")\nlibrary(\"bayesplot\")\nlibrary(\"gt\")\nlibrary(\"patchwork\")\nlibrary(\"rstan\")\nlibrary(\"rstanarm\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\nrent_raw &lt;- rent_raw &lt;- readr::read_csv(\"rent.csv\")\nrent_df &lt;- rent_raw |&gt;\n  filter(price &gt;= 800 & price &lt;= 5000) |&gt;\n  filter(sqft &gt;= 500 & sqft &lt;= 2500) |&gt;\n  filter(year &gt;= 2009) |&gt;\n  filter(county %in% c(\"alameda\", \"contra costa\"))\n\nrent_df$county &lt;- factor(rent_df$county,\n                         levels = c(\"alameda\", \"contra costa\"))\n\nset.seed(320)\nrent_df_for_stan &lt;- rent_df |&gt;\n  slice_sample(prop = 0.05)"
  },
  {
    "objectID": "posts/13_extending_regression/13_extending_regression.html#variables",
    "href": "posts/13_extending_regression/13_extending_regression.html#variables",
    "title": "13: Extending Regression Models",
    "section": "",
    "text": "\\(Y\\): price (monthly rent in USD)\n\n\n\nWhat is the monthly rent for a property that is about 1500 square feet in size?\n\n\n\n\n\n\n\n\\(X_{1}\\): area (in square feet)\n\\(X_{2}\\): year (discrete, numerical)\n\\(X_{3}\\): county\n\\(X_{4}\\): number of bedrooms\n\\(X_{5}\\): number of bathrooms"
  },
  {
    "objectID": "posts/13_extending_regression/13_extending_regression.html#indicator-variable",
    "href": "posts/13_extending_regression/13_extending_regression.html#indicator-variable",
    "title": "13: Extending Regression Models",
    "section": "Indicator Variable",
    "text": "Indicator Variable\n\n\n\\[X_{3} = \\begin{cases}\n  1, & \\text{Contra Costa County} \\\\ 0, & \\text{Alameda County}\n\\end{cases}\\]\n\n\n\n\n\n\nSF Bay Area counties"
  },
  {
    "objectID": "posts/13_extending_regression/13_extending_regression.html#local-mean",
    "href": "posts/13_extending_regression/13_extending_regression.html#local-mean",
    "title": "13: Extending Regression Models",
    "section": "Local Mean",
    "text": "Local Mean\n\\[\\mu_{i} = \\beta_{0} + \\beta_{1}X_{3} \\quad\\text{with}\\quad X_{3} = \\begin{cases}\n  1, & \\text{Contra Costa County} \\\\ 0, & \\text{Alameda County}\n\\end{cases}\\]\n\nAlameda: \\(\\beta_{0} + \\beta_{1}(0) = \\beta_{0}\\)\nContra Costa: \\(\\beta_{0} + \\beta_{1}(1) = \\beta_{0} + \\beta_{1}\\)\nstandard deviation \\(\\sigma\\) still represents variability at a given \\(X_{3}\\) value."
  },
  {
    "objectID": "posts/13_extending_regression/13_extending_regression.html#sf-rental-market",
    "href": "posts/13_extending_regression/13_extending_regression.html#sf-rental-market",
    "title": "13: Extending Regression Models",
    "section": "SF Rental Market",
    "text": "SF Rental Market\n\nDataPlotCode\n\n\n\naverage rent by day\none average price per day\n\\(n = 399\\) data points\n\n\nrent_df &lt;- rent_raw |&gt;\n  filter(price &gt;= 800 & price &lt;= 5000) |&gt;\n  filter(sqft &gt;= 500 & sqft &lt;= 2500) |&gt;\n  filter(year &gt;= 2009)\n\nrent_ts &lt;- rent_df |&gt;\n  select(year, date, price) |&gt;\n  group_by(date) |&gt;\n  mutate(avg_price = mean(price, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(year, date, avg_price) |&gt;\n  distinct() |&gt;\n  arrange(date)\n\n# convert string to YYYY/MM/DD date format\nrent_ts$date &lt;- lubridate::ymd(rent_ts$date) \n\n\n\n\n\n\n\n\n\n\n\nrent_ts |&gt;\n  ggplot(aes(x = date, y = avg_price)) +\n  geom_line() +\n  labs(title = \"SF Bay Area Rental Market\",\n       subtitle = \"2009 to 2018\",\n       caption = \"SML 320\",\n       x = \"date\", y = \"average monthly rent (USD)\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/13_extending_regression/13_extending_regression.html#causal-impact",
    "href": "posts/13_extending_regression/13_extending_regression.html#causal-impact",
    "title": "13: Extending Regression Models",
    "section": "Causal Impact",
    "text": "Causal Impact\n\ntraining over pre-intervention period\ntesting over post-intervention period\nthe CausalImpact function\n\nassembles structural time-series model\nperforms posterior inference\ncomputes estimates for casual effect"
  },
  {
    "objectID": "posts/13_extending_regression/13_extending_regression.html#claim-1-change-in-market-in-2016",
    "href": "posts/13_extending_regression/13_extending_regression.html#claim-1-change-in-market-in-2016",
    "title": "13: Extending Regression Models",
    "section": "Claim 1: Change in Market in 2016",
    "text": "Claim 1: Change in Market in 2016\n\nVizR CodeModel StatsVizInference\n\n\n\n\n\n\n\n\nrent_ts |&gt;\n  ggplot(aes(x = date, y = avg_price)) +\n  geom_line() +\n  geom_vline(xintercept = lubridate::ymd(\"20160101\"),\n             color = \"red\", linetype = 2, linewidth = 3) +\n  labs(title = \"Was there a market intervention in 2016?\",\n       caption = \"SML 320\",\n       x = \"date\", y = \"average monthly rent (USD)\") +\n  theme_minimal()\n\n\n\n\nintervention_index &lt;- which.max(rent_ts$year &gt;= 2016)\npre_intervention &lt;- c(1, intervention_index-1)\npost_intervention &lt;- c(intervention_index, nrow(rent_ts))\n\nts_impact &lt;- CausalImpact::CausalImpact(rent_ts$avg_price,\n                                        pre_intervention,\n                                        post_intervention)\n\n\n\n\nsummary(ts_impact)\n\nPosterior inference {CausalImpact}\n\n                         Average         Cumulative      \nActual                   2821            335689          \nPrediction (s.d.)        2819 (104)      335481 (12414)  \n95% CI                   [2625, 3031]    [312434, 360678]\n                                                         \nAbsolute effect (s.d.)   1.7 (104)       207.6 (12414)   \n95% CI                   [-210, 195]     [-24989, 23255] \n                                                         \nRelative effect (s.d.)   0.2% (3.7%)     0.2% (3.7%)     \n95% CI                   [-6.9%, 7.4%]   [-6.9%, 7.4%]   \n\nPosterior tail-area probability p:   0.48075\nPosterior prob. of a causal effect:  52%\n\nFor more details, type: summary(impact, \"report\")\n\n\n\n\nThe plot of an “impact” object returns a ggplot object!\n\nplot(ts_impact)\n\n\n\n\n\n\n\nsummary(ts_impact, \"report\")\n\nAnalysis report {CausalImpact}\n\n\nDuring the post-intervention period, the response variable had an average value of approx. 2.82K. In the absence of an intervention, we would have expected an average response of 2.82K. The 95% interval of this counterfactual prediction is [2.63K, 3.03K]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 0.00K with a 95% interval of [-0.21K, 0.20K]. For a discussion of the significance of this effect, see below.\n\nSumming up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 335.69K. Had the intervention not taken place, we would have expected a sum of 335.48K. The 95% interval of this prediction is [312.43K, 360.68K].\n\nThe above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +0%. The 95% interval of this percentage is [-7%, +7%].\n\nThis means that, although the intervention appears to have caused a positive effect, this effect is not statistically significant when considering the entire post-intervention period as a whole. Individual days or shorter stretches within the intervention period may of course still have had a significant effect, as indicated whenever the lower limit of the impact time series (lower plot) was above zero. The apparent effect could be the result of random fluctuations that are unrelated to the intervention. This is often the case when the intervention period is very long and includes much of the time when the effect has already worn off. It can also be the case when the intervention period is too short to distinguish the signal from the noise. Finally, failing to find a significant effect can happen when there are not enough control variables or when these variables do not correlate well with the response variable during the learning period.\n\nThe probability of obtaining this effect by chance is p = 0.481. This means the effect may be spurious and would generally not be considered statistically significant."
  },
  {
    "objectID": "posts/13_extending_regression/13_extending_regression.html#claim-2-change-in-market-in-2014",
    "href": "posts/13_extending_regression/13_extending_regression.html#claim-2-change-in-market-in-2014",
    "title": "13: Extending Regression Models",
    "section": "Claim 2: Change in Market in 2014",
    "text": "Claim 2: Change in Market in 2014\n\nVizR CodeModel StatsVizInference\n\n\n\n\n\n\n\n\nrent_ts |&gt;\n  ggplot(aes(x = date, y = avg_price)) +\n  geom_line() +\n  geom_vline(xintercept = lubridate::ymd(\"20140101\"),\n             color = \"red\", linetype = 2, linewidth = 3) +\n  labs(title = \"Was there a market intervention in 2014?\",\n       caption = \"SML 320\",\n       x = \"date\", y = \"average monthly rent (USD)\") +\n  theme_minimal()\n\n\n\n\nintervention_index &lt;- which.max(rent_ts$year &gt;= 2014)\npre_intervention &lt;- c(1, intervention_index-1)\npost_intervention &lt;- c(intervention_index, nrow(rent_ts))\n\nts_impact &lt;- CausalImpact::CausalImpact(rent_ts$avg_price,\n                                        pre_intervention,\n                                        post_intervention)\n\n\n\n\nsummary(ts_impact)\n\nPosterior inference {CausalImpact}\n\n                         Average        Cumulative      \nActual                   2788           521273          \nPrediction (s.d.)        2301 (80)      430286 (14984)  \n95% CI                   [2156, 2469]   [403170, 461701]\n                                                        \nAbsolute effect (s.d.)   487 (80)       90987 (14984)   \n95% CI                   [319, 632]     [59572, 118103] \n                                                        \nRelative effect (s.d.)   21% (4.2%)     21% (4.2%)      \n95% CI                   [13%, 29%]     [13%, 29%]      \n\nPosterior tail-area probability p:   0.0012\nPosterior prob. of a causal effect:  99.88024%\n\nFor more details, type: summary(impact, \"report\")\n\n\n\n\nThe plot of an “impact” object returns a ggplot object!\n\nplot(ts_impact)\n\n\n\n\n\n\n\nsummary(ts_impact, \"report\")\n\nAnalysis report {CausalImpact}\n\n\nDuring the post-intervention period, the response variable had an average value of approx. 2.79K. By contrast, in the absence of an intervention, we would have expected an average response of 2.30K. The 95% interval of this counterfactual prediction is [2.16K, 2.47K]. Subtracting this prediction from the observed response yields an estimate of the causal effect the intervention had on the response variable. This effect is 0.49K with a 95% interval of [0.32K, 0.63K]. For a discussion of the significance of this effect, see below.\n\nSumming up the individual data points during the post-intervention period (which can only sometimes be meaningfully interpreted), the response variable had an overall value of 521.27K. By contrast, had the intervention not taken place, we would have expected a sum of 430.29K. The 95% interval of this prediction is [403.17K, 461.70K].\n\nThe above results are given in terms of absolute numbers. In relative terms, the response variable showed an increase of +21%. The 95% interval of this percentage is [+13%, +29%].\n\nThis means that the positive effect observed during the intervention period is statistically significant and unlikely to be due to random fluctuations. It should be noted, however, that the question of whether this increase also bears substantive significance can only be answered by comparing the absolute effect (0.49K) to the original goal of the underlying intervention.\n\nThe probability of obtaining this effect by chance is very small (Bayesian one-sided tail-area probability p = 0.001). This means the causal effect can be considered statistically significant."
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html",
    "title": "14: Poisson Regression Models",
    "section": "",
    "text": "Goal: Explore Poisson and Negative Binomial Regression Models\n\n\n\n\n\n\n\n\n\n\n\nsource: TidyTuesday\n2019-12-03 edition\nOpen Data Philly\n\nfiltered to year 2017 data that had latitude/longitude\n\n\n\n\n\n\nlibrary(\"bayesrules\")\nlibrary(\"bayesplot\")\nlibrary(\"gt\")\nlibrary(\"patchwork\")\nlibrary(\"rstan\")\nlibrary(\"rstanarm\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# tickets_raw &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-12-03/tickets.csv\")\n\n\n\n\n\n\n\nIdeasCode\n\n\n\nextract month and day from datetime\ncompute number of street sweeping violations per day\nmake categorical variable weekend\n\n\n\n\ntickets_df &lt;- tickets_raw |&gt;\n  separate(issue_datetime, into = c(\"date\", \"time\"), sep = \" \") |&gt;\n  separate(date, into = c(\"year\", \"month\", \"day\"), \n           sep = \"-\", remove = FALSE) |&gt;\n  select(date, month, day, lat, lon) |&gt;\n  group_by(month, day) |&gt;\n  mutate(violations = n(), .before = date) |&gt;\n  ungroup()\n\ntickets_df$day_of_week &lt;- lubridate::wday(tickets_df$date, label = TRUE)\n\ntickets_df &lt;- tickets_df |&gt;\n  mutate(weekend = day_of_week %in% c(\"Sat\", \"Sun\")) |&gt;\n  select(violations, month, day, weekend, lat, lon)\n\ntickets_df$month &lt;- as.numeric(tickets_df$month)\ntickets_df$day &lt;- as.numeric(tickets_df$day)\n\nset.seed(320)\ntickets_df_for_stan &lt;- tickets_df |&gt;\n  slice_sample(prop = 0.05)\n\n\n\n\n\n\n\n\n\n\n\n\\(Y\\): violations\n\ncount variable: number of street sweeping violations per day\n\n\n\n\nHow many street sweeping violations will there be on March 28?\n\n\n\n\n\n\n\n\\(X_{1}\\): month (1, 2, 3, etc.)\n\\(X_{2}\\): day (of month)\n\\(X_{3}\\): weekend (boolean)\n\\(X_{4}\\): latitude\n\\(X_{5}\\): longitude"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#data",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#data",
    "title": "14: Poisson Regression Models",
    "section": "",
    "text": "source: TidyTuesday\n2019-12-03 edition\nOpen Data Philly\n\nfiltered to year 2017 data that had latitude/longitude\n\n\n\n\n\n\nlibrary(\"bayesrules\")\nlibrary(\"bayesplot\")\nlibrary(\"gt\")\nlibrary(\"patchwork\")\nlibrary(\"rstan\")\nlibrary(\"rstanarm\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# tickets_raw &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-12-03/tickets.csv\")"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#data-wrangling",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#data-wrangling",
    "title": "14: Poisson Regression Models",
    "section": "",
    "text": "IdeasCode\n\n\n\nextract month and day from datetime\ncompute number of street sweeping violations per day\nmake categorical variable weekend\n\n\n\n\ntickets_df &lt;- tickets_raw |&gt;\n  separate(issue_datetime, into = c(\"date\", \"time\"), sep = \" \") |&gt;\n  separate(date, into = c(\"year\", \"month\", \"day\"), \n           sep = \"-\", remove = FALSE) |&gt;\n  select(date, month, day, lat, lon) |&gt;\n  group_by(month, day) |&gt;\n  mutate(violations = n(), .before = date) |&gt;\n  ungroup()\n\ntickets_df$day_of_week &lt;- lubridate::wday(tickets_df$date, label = TRUE)\n\ntickets_df &lt;- tickets_df |&gt;\n  mutate(weekend = day_of_week %in% c(\"Sat\", \"Sun\")) |&gt;\n  select(violations, month, day, weekend, lat, lon)\n\ntickets_df$month &lt;- as.numeric(tickets_df$month)\ntickets_df$day &lt;- as.numeric(tickets_df$day)\n\nset.seed(320)\ntickets_df_for_stan &lt;- tickets_df |&gt;\n  slice_sample(prop = 0.05)"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#variables",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#variables",
    "title": "14: Poisson Regression Models",
    "section": "",
    "text": "\\(Y\\): violations\n\ncount variable: number of street sweeping violations per day\n\n\n\n\nHow many street sweeping violations will there be on March 28?\n\n\n\n\n\n\n\n\\(X_{1}\\): month (1, 2, 3, etc.)\n\\(X_{2}\\): day (of month)\n\\(X_{3}\\): weekend (boolean)\n\\(X_{4}\\): latitude\n\\(X_{5}\\): longitude"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#poisson-data-model",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#poisson-data-model",
    "title": "14: Poisson Regression Models",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\ncount response variable \\(Y_{i}\\) (per unit of time)\nrate parameter \\(\\lambda_{i}\\)\nlikelihood\n\n\\[Y_{i} | \\lambda_{i} \\sim \\text{Pois}(\\lambda_{i})\\]"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#expectation",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#expectation",
    "title": "14: Poisson Regression Models",
    "section": "Expectation",
    "text": "Expectation\nThe expected number \\(Y_{i}\\) predicted by values \\(X_{ij}\\) (over \\(j\\) predictor variables) captured by parameter \\(\\lambda_{i}\\) is\n\\[\\text{E}(Y_{i} | \\lambda_{i}) = \\lambda_{i}\\]"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#log_link-function",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#log_link-function",
    "title": "14: Poisson Regression Models",
    "section": "Log_Link Function",
    "text": "Log_Link Function\n\\[Y_{i} | \\beta_{0}, \\beta_{1}, \\beta_{2} \\sim \\text{Pois}(\\lambda_{i}) \\quad\\text{with}\\quad \\ln(\\lambda_{i}) = \\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2}\\] \\[\\text{OR}\\] \\[Y_{i} | \\beta_{0}, \\beta_{1}, \\beta_{2} \\sim \\text{Pois}(\\lambda_{i}) \\quad\\text{with}\\quad \\lambda_{i} = e^{\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2}}\\]"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#model-assumptions",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#model-assumptions",
    "title": "14: Poisson Regression Models",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nStructure of the data: observed data are independent\nStructure of the response variable: \\(Y\\) has a Poisson structure that seeks a discrete count of events (per unit time)\nStructure of the relationship: the logged average \\(Y\\) can be written as a linear combination of the predictors\n\n\\[\\lambda_{i} = e^{\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2}}\\]\n\nStructure of the variability in \\(Y\\):\n\n\\[\\text{E}(Y) = \\text{Var}(Y) = \\lambda\\]\neditor’s note: better visuals by Professors Roback and Legler at St Olaf College"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#interpreting-coefficients",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#interpreting-coefficients",
    "title": "14: Poisson Regression Models",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\nIntercept\nWhen \\(X_{1} = 0, X_{2} = 0, ...\\)\n\\[\\text{E}(Y) = \\lambda = e^{\\beta_{0}} \\quad\\rightarrow\\quad \\beta_{0} = \\ln(\\lambda)\\]\n\n\\(\\beta_{0}\\): logged average\n\\(e^{\\beta_{0}}\\): average value\n\n\n\nRates\nWhen we control other predictors and increase \\(X_{i}\\) by one unit,\n\\[\\beta_{i} = \\ln(\\lambda_{x+1}) - \\ln(\\lambda_{x}) \\quad\\rightarrow\\quad e^{\\beta_{i}} = \\displaystyle\\frac{\\lambda_{x+1}}{\\lambda_{x}}\\]\n\n\\(\\beta_{i}\\): change in logged average\n\\(e^{\\beta_{i}}\\): multiplicative change in average value"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#coefficients",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#coefficients",
    "title": "14: Poisson Regression Models",
    "section": "Coefficients",
    "text": "Coefficients\n\nbroom.mixed::tidy(mod_pois, effects = c(\"fixed\", \"aux\")) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n# A tibble: 4 × 3\n  term        estimate std.error\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   3.58      0.0233\n2 month         0.0357    0.0027\n3 day          -0.0087    0.0009\n4 mean_PPD     39.4       0.467 \n\n\n\\[e^{\\beta_{1}} = e^{0.0357} \\approx 1.0363\\]\nAs we go from one month to the next, the number of street sweeping violations increases by about 3.6 percent.\n\\[e^{\\beta_{2}} = e^{-0.0087} \\approx 0.9913\\]\nAs we go from one day to the next, the number of street sweeping violations decreases by about 0.9 percent."
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#prior-distributions",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#prior-distributions",
    "title": "14: Poisson Regression Models",
    "section": "Prior Distributions",
    "text": "Prior Distributions\n\nCodeInterpretationFull Model\n\n\n\nrstanarm::prior_summary(mod_pois)\n\nPriors for model 'mod_pois' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0], scale = [0.78,0.28])\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\n\nFor the prior distributions,\n\\[e^{\\beta_{0}} \\in (e^{-5}, e^{5}) \\approx (0, 149)\\]\n\nthe average number of street sweeping violations per day is in between zero and 149\n\n\\[e^{\\beta_{1}} \\in (e^{-1.56}, e^{1.56}) \\approx (0.2101, 4.7588)\\]\n\nmoving from one month to the next, the percent change in the number of street sweeping violations per day is in between 21 and 476 percent\n\n\\[e^{\\beta_{2}} \\in (e^{-0.56}, e^{0.56}) \\approx (0.5712, 1.7507)\\]\n\nmoving from one day to the next, the percent change in the number of street sweeping violations per day is in between -43 and 175 percent\n\n\n\n\\[\\begin{array}{rcl}\n  Y_{i}|\\beta_{0},\\beta_{1},\\beta_{2} & \\sim & \\text{Pois}(\\lambda_{i}) \\text{ with }\\ln(\\lambda_{i}) = \\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} \\\\\n  \\beta_{0c} & \\sim & \\text{N}(0, 2.5^{2}) \\\\\n  \\beta_{1} & \\sim & \\text{N}(0, 0.78^{2}) \\\\\n  \\beta_{2} & \\sim & \\text{N}(0, 0.28^{2}) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#posterior-prediction-interval",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#posterior-prediction-interval",
    "title": "14: Poisson Regression Models",
    "section": "Posterior Prediction Interval",
    "text": "Posterior Prediction Interval\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nthese_predictions &lt;- rstanarm::posterior_predict(\n  mod_pois, newdata = data.frame(month = 3, day = 28))\ncredible_interval &lt;- round(quantile(these_predictions, c(0.05, 0.95)))\nsubtitle_string &lt;- paste0(\"90 Percent Credible Interval: (\",\n                          credible_interval[1], \", \",\n                          credible_interval[2], \")\")\nbayesplot::mcmc_areas(these_predictions, prob = 0.9) +\n  labs(title = \"Predictions for March 28\",\n       subtitle = subtitle_string,\n       caption = \"SML 320\",\n       x = \"number of street sweeping violations\")"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#motivation-and-definition",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#motivation-and-definition",
    "title": "14: Poisson Regression Models",
    "section": "Motivation and Definition",
    "text": "Motivation and Definition\n\n\n\n\n\n\nOverdispersion\n\n\n\n\n\nA random variable \\(Y\\) is overdispersed if the observed variability in \\(Y\\) exceeds the variability expected by the assumed probability model of \\(Y\\)\n\n\n\nIf the count data appears to be demonstrating overdispersion, consider using the negative binomial distribution.\n\n\n\n\n\n\nNegative Binomial model\n\n\n\n\n\nLet random variable \\(Y\\) be some count, \\(Y\\in\\{0,1,2,…\\}\\), that can be modeled by the Negative Binomial with mean parameter \\(\\mu\\) and reciprocal dispersion parameter \\(r\\):\n\\[Y|\\mu,r \\sim \\text{NegBin}(\\mu,r)\\]\nThen \\(Y\\) has conditional pmf\n\\[f(y|\\mu,r) = \\binom{y+r-1}{r}\\left( \\displaystyle\\frac{r}{\\mu+r} \\right)^{r}\\left(  \\displaystyle\\frac{\\mu}{\\mu+r}\\right)^{y} \\text{ for } y\\in\\{0,1,2,...\\}\\]\nwith statistics\n\\[\\text{E}(Y|\\mu,r) = \\mu \\quad\\text{and}\\quad \\text{Var}(Y|\\mu,r) = \\mu + \\displaystyle\\frac{\\mu^{2}}{r}\\]\n\n\n\n\n\n\n\n\n\nCommentary\n\n\n\n\n\n\nfor large \\(r\\), this distribution behaves like the Poisson distribution\nfor small \\(r\\), and \\(Y\\) is overdispersed\n\\(\\text{Var}(Y) \\neq \\text{E}(Y)\\)"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#prior-distributions-1",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#prior-distributions-1",
    "title": "14: Poisson Regression Models",
    "section": "Prior Distributions",
    "text": "Prior Distributions\n\nCodeInterpretationFull Model\n\n\n\nmodel_priors &lt;- rstanarm::prior_summary(mod_neg_bin)\nmodel_priors #print\n\nPriors for model 'mod_neg_bin' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0,0,...], scale = [2.5,2.5,2.5,...])\n  Adjusted prior:\n    ~ normal(location = [0,0,0,...], scale = [ 0.78, 0.28,19.58,...])\n\nAuxiliary (reciprocal_dispersion)\n ~ exponential(rate = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\n# model_priors$prior$adjusted_scale #after autoscaling\n\n\n\nFor the prior distributions,\n\\[e^{\\beta_{0}} \\in (e^{-5}, e^{5}) \\approx (0, 149)\\]\n\nthe average number of street sweeping violations per day is in between zero and 149\n\n\\[e^{\\beta_{1}} \\in (e^{-1.56}, e^{1.56}) \\approx (0.2101, 4.7588)\\]\n\nmoving from one month to the next, the percent change in the number of street sweeping violations per day is in between 21 and 476 percent\n\n\\[e^{\\beta_{2}} \\in (e^{-0.56}, e^{0.56}) \\approx (0.5712, 1.7507)\\]\n\nmoving from one day to the next, the percent change in the number of street sweeping violations per day is in between -43 and 175 percent\n\n\\[e^{\\beta_{3}} \\in (e^{-39.16}, e^{39.16})\\]\n\n[We have a vague prior for the coefficient of the categorical variable]\n\n\n\n\\[\\begin{array}{rcl}\n  Y_{i}|\\beta_{0},\\beta_{1},\\beta_{2} & \\sim & \\text{NegBin}(\\mu_{i},r) \\text{ with }\\ln(\\mu_{i}) = \\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} + ... \\\\\n  \\beta_{0c} & \\sim & \\text{N}(0, 2.5^{2}) \\\\\n  \\beta_{1} & \\sim & \\text{N}(0, 0.78^{2}) \\\\\n  \\beta_{2} & \\sim & \\text{N}(0, 0.28^{2}) \\\\\n  \\beta_{3} & \\sim & \\text{N}(0, 19.58^{2}) \\\\\n  \\beta_{4} & \\sim & \\text{N}(0, 1.09^{2}) \\\\\n  r & \\sim & \\text{Exp}(1) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#ppc-3",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#ppc-3",
    "title": "14: Poisson Regression Models",
    "section": "PPC",
    "text": "PPC\n\nbayesplot::pp_check(mod_neg_bin, nreps = 50) +\n  labs(title = \"Posterior Predictive Check\",\n       subtitle = \"Negative Binomial Regression Model\",\n       caption = \"SML 320\")"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#coefficients-1",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#coefficients-1",
    "title": "14: Poisson Regression Models",
    "section": "Coefficients",
    "text": "Coefficients\nLike in the Poisson regression models, the coefficents of negative binomial regression models are also desribed with the logarithmic transformation in mind.\n\nbroom.mixed::tidy(mod_neg_bin, effects = c(\"fixed\", \"aux\"),\n                  conf.int = TRUE, conf.level = 0.90) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n# A tibble: 6 × 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 124.       24.9     82.2     165.    \n2 month         0.022     0.0066   0.0112    0.0329\n3 day          -0.0039    0.0023  -0.0077   -0.0002\n4 weekendTRUE   0.879     0.160    0.627     1.15  \n5 lat:lon       0.04      0.0083   0.0262    0.0536\n6 mean_PPD     39.5       1.16    37.6      41.4   \n\n\n\\[e^{\\beta_{1}} = e^{0.0220} \\approx 1.0222\\]\n\nAs we go from one month to the next, the number of street sweeping violations increases by about 2.2 percent.\n\n\\[e^{\\beta_{2}} = e^{-0.0039} \\approx 0.9961\\]\n\nAs we go from one day to the next, the number of street sweeping violations decreases by about 0.04 percent.\n\n\\[e^{\\beta_{3}} = e^{0.8791} \\approx 2.4087\\]\n\nCompared to a weekday, the expected amount of street sweeping violations on a weekend day is 241 percent higher (caution: sample size issue).\n\n\\[1.0 \\notin (e^{0.0262}, e^{0.0536})\\]\n\nWe may have some evidence that \\(\\beta_{4} &gt; 1.0\\), or the notion that there may be an interaction effect between latitude and longitude"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#posterior-prediction-interval-1",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#posterior-prediction-interval-1",
    "title": "14: Poisson Regression Models",
    "section": "Posterior Prediction Interval",
    "text": "Posterior Prediction Interval\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nthese_predictions &lt;- rstanarm::posterior_predict(\n  mod_neg_bin, newdata = data.frame(month = 3, \n                                    day = 28,\n                                    weekend = FALSE,\n                                    lat = mean(tickets_df$lat),\n                                    lon = mean(tickets_df$lon)))\ncredible_interval &lt;- round(quantile(these_predictions, c(0.05, 0.95)))\nsubtitle_string &lt;- paste0(\"90 Percent Credible Interval: (\",\n                          credible_interval[1], \", \",\n                          credible_interval[2], \")\")\nbayesplot::mcmc_areas(these_predictions, prob = 0.9) +\n  labs(title = \"Predictions for March 28\",\n       subtitle = subtitle_string,\n       caption = \"SML 320\",\n       x = \"number of street sweeping violations\")"
  },
  {
    "objectID": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#dispersion-1",
    "href": "posts/14_poisson_neg_binomial/14_poisson_neg_binomial.html#dispersion-1",
    "title": "14: Poisson Regression Models",
    "section": "Dispersion",
    "text": "Dispersion\n\\[\\text{E}(Y) = \\text{Var}(Y)??\\]\n\nmean(these_predictions)\n\n[1] 33.85635\n\n\n\nvar(these_predictions)\n\n         1\n1 175.5899"
  },
  {
    "objectID": "posts/15_logistic_regression/15_logistic_regression.html",
    "href": "posts/15_logistic_regression/15_logistic_regression.html",
    "title": "15: Logistic Regression",
    "section": "",
    "text": "Goal: Use Bayesian approaches to classification tasks\nBayes Rules! Exercise 13.14\n\n\n\nAnother Ghostbusters movie came out in 2024\n\n\n\n\n\n\nlibrary(\"bayesrules\")\nlibrary(\"bayesplot\")\nlibrary(\"ggtext\")\nlibrary(\"gt\")\nlibrary(\"janitor\")\nlibrary(\"rstan\")\nlibrary(\"rstanarm\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\ndata(pulse_of_the_nation)\npulse_df &lt;- pulse_of_the_nation\n\npulse_df$education &lt;- factor(pulse_df$education,\n                             levels = c(\"Graduate degree\",\n                                        \"College degree\",\n                                        \"Some college\",\n                                        \"High school\",\n                                        \"Other\"))\npulse_df$robots &lt;- factor(pulse_df$robots,\n                          levels = c(\"Likely\", \"Unlikely\"))\npulse_df$ghosts &lt;- factor(pulse_df$ghosts,\n                          levels = c(\"Yes\", \"No\"))"
  },
  {
    "objectID": "posts/15_logistic_regression/15_logistic_regression.html#exploratory-data-analyses",
    "href": "posts/15_logistic_regression/15_logistic_regression.html#exploratory-data-analyses",
    "title": "15: Logistic Regression",
    "section": "Exploratory Data Analyses",
    "text": "Exploratory Data Analyses\n\nIncomeAgeEducationRobotsGhosts\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = income)) +\n  geom_density(fill = \"green\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Income of participants\",\n       caption = \"SML 320\",\n       x = \"income (thousands of dollars)\",\n       y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = age)) +\n  geom_density(fill = \"purple\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Age of participants\",\n       caption = \"SML 320\",\n       x = \"age\",\n       y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = education, fill = education)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Education attainment of participants\",\n       caption = \"SML 320\",\n       x = \"education\",\n       y = \"count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = robots, fill = robots)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Is it likely that robots would take your jobs within the next decade\",\n       caption = \"September 2017\",\n       x = \"\",\n       y = \"count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = ghosts, fill = ghosts)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Do you believe in ghosts?\",\n       caption = \"September 2017\",\n       x = \"\",\n       y = \"count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/15_logistic_regression/15_logistic_regression.html#variables",
    "href": "posts/15_logistic_regression/15_logistic_regression.html#variables",
    "title": "15: Logistic Regression",
    "section": "Variables",
    "text": "Variables\n\n\n\nResponse Variable\n\\(Y\\): ghosts\n\nbinary variable: did the survey participant believe in ghosts?\n\n\n\nPrediction\nWhat percentage of 21-year-olds who make $75,000 believe in ghosts?\n\n\n\n\n\nPredictor Variables\n\n\\(X_{1}\\): income\n\\(X_{2}\\): age\n\\(X_{3}\\): education\n\\(X_{4}\\): robots"
  },
  {
    "objectID": "posts/15_logistic_regression/15_logistic_regression.html#simulation",
    "href": "posts/15_logistic_regression/15_logistic_regression.html#simulation",
    "title": "15: Logistic Regression",
    "section": "Simulation",
    "text": "Simulation\nWhat percentage of 21-year-olds who make $75,000 believe in ghosts?\n\nset.seed(320)\nlog_reg_mod_1_df &lt;- as.data.frame(log_reg_mod_1) |&gt;\n  mutate(log_odds = `(Intercept)` + income*75 + age*21,\n         odds = exp(log_odds),\n         prob = odds / (1 + odds),\n         Y = rbinom(20000, size = 1, prob = prob))\n\n\nlog_reg_mod_1_df |&gt;\n  mutate_if(is.numeric, round, digits = 4) |&gt;\n  slice_sample(n = 10)\n\n   (Intercept) income     age log_odds   odds   prob Y\n1      -0.0165 0.0011  0.0081   0.2364 1.2667 0.5588 1\n2       0.2884 0.0028 -0.0008   0.4830 1.6209 0.6184 1\n3      -0.2852 0.0034  0.0128   0.2402 1.2715 0.5598 1\n4      -0.2206 0.0018  0.0109   0.1463 1.1576 0.5365 1\n5      -0.3701 0.0024  0.0115   0.0559 1.0575 0.5140 0\n6      -0.1566 0.0024  0.0074   0.1799 1.1971 0.5448 0\n7      -0.1805 0.0029  0.0066   0.1775 1.1942 0.5443 0\n8      -0.1173 0.0003  0.0129   0.1723 1.1880 0.5430 1\n9      -0.4068 0.0028  0.0116   0.0493 1.0505 0.5123 0\n10      0.1703 0.0025  0.0029   0.4165 1.5166 0.6026 1\n\n\nWhat percentage of 21-year-olds who make $75,000 believe in ghosts?\n\nmean(log_reg_mod_1_df$Y)\n\n[1] 0.54835"
  },
  {
    "objectID": "posts/15_logistic_regression/15_logistic_regression.html#cutoff",
    "href": "posts/15_logistic_regression/15_logistic_regression.html#cutoff",
    "title": "15: Logistic Regression",
    "section": "Cutoff",
    "text": "Cutoff\n\nset.seed(320)\n\n# make 20000 predictions\nghost_preds &lt;- rstanarm::posterior_predict(\n  log_reg_mod_1, newdata = pulse_df)\n\nghost_classifications &lt;- pulse_df |&gt;\n  mutate(ghost_prob = colMeans(ghost_preds),\n         ghost_class = ifelse(ghost_prob &gt;= 0.60, 1, 0)) |&gt;\n  select(income, age, ghost_prob, ghost_class, ghosts)\n\nhead(ghost_classifications, 10)\n\n# A tibble: 10 × 5\n   income   age ghost_prob ghost_class ghosts\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt; \n 1      8    64      0.608           1 Yes   \n 2     68    56      0.625           1 No    \n 3     46    63      0.629           1 No    \n 4     51    48      0.601           1 No    \n 5    100    32      0.590           0 Yes   \n 6     54    64      0.634           1 No    \n 7     83    61      0.641           1 Yes   \n 8    114    64      0.666           1 No    \n 9     90    64      0.660           1 No    \n10      5    68      0.621           1 No"
  },
  {
    "objectID": "posts/15_logistic_regression/15_logistic_regression.html#confusion-matrix",
    "href": "posts/15_logistic_regression/15_logistic_regression.html#confusion-matrix",
    "title": "15: Logistic Regression",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nghost_classifications |&gt;\n  janitor::tabyl(ghosts, ghost_class) |&gt;\n  adorn_totals(c(\"row\", \"col\"))\n\n ghosts   0   1 Total\n    Yes 135 244   379\n     No 172 449   621\n  Total 307 693  1000\n\n\n\n\n\n\n\n\nFormulas\n\n\n\n\n\n\\[\\text{accuracy } = \\frac{TP + TN}{TP + FN + FP + TN}\\] \\[\\text{sensitivity } = \\frac{TP}{TP + FN}\\] \\[\\text{specificity } = \\frac{TN}{FP + TN}\\] \\[\\text{F-score } = \\frac{2*TP}{2*TP + FN + FP}\\]\nSource: Wikipedia page on sensitivity and specificity"
  },
  {
    "objectID": "posts/15_logistic_regression/15_logistic_regression.html#classification",
    "href": "posts/15_logistic_regression/15_logistic_regression.html#classification",
    "title": "15: Logistic Regression",
    "section": "Classification",
    "text": "Classification\n\nset.seed(320)\nclass_results &lt;- bayesrules::classification_summary(\n  model = log_reg_mod_1,\n  data = pulse_df,\n  cutoff = 0.6\n)\n\n\nclass_results$confusion_matrix\n\n   y   0   1\n Yes 135 244\n  No 172 449\n\n\n\nclass_results$accuracy_rates |&gt; round(digits = 4)\n\n                       \nsensitivity      0.7230\nspecificity      0.3562\noverall_accuracy 0.5840"
  },
  {
    "objectID": "posts/15_logistic_regression/15_logistic_regression.html#cutoff-1",
    "href": "posts/15_logistic_regression/15_logistic_regression.html#cutoff-1",
    "title": "15: Logistic Regression",
    "section": "Cutoff",
    "text": "Cutoff\nHow do we choose a cutoff value?\n\nLoopVizCode\n\n\n\nN &lt;- 11 #resolution\ncutoff_vals &lt;- seq(0.5, 1.0, length.out = N)\n\naccuracies &lt;-    rep(NA, N)\nsensitivities &lt;- rep(NA, N)\nspecificities &lt;- rep(NA, N)\n\nfor(i in 1:length(cutoff_vals)){\n  this_class_result &lt;- bayesrules::classification_summary(\n    model = log_reg_mod_1,\n    data = pulse_df,\n    cutoff = cutoff_vals[i]\n  )\n  \n  these_metrics &lt;- unlist(this_class_result$accuracy_rates)\n  \n  accuracies[i]    &lt;- these_metrics[3]\n  sensitivities[i] &lt;- these_metrics[1]\n  specificities[i] &lt;- these_metrics[2]\n}\n\ncutoff_df &lt;- data.frame(cutoff_vals, accuracies, sensitivities, specificities)\n\n\n\n\n\n\n\n\n\nAs we lower \\(c\\), sensitivity increases, but specificity decreases.\nAs we increase \\(c\\), specificity increases, but sensitivity decreases.\nPerhaps a cutoff value around 0.62 would be good here.\n\n\n\n\nsubtitle_string &lt;- \"&lt;span style='color:#ff00ff'&gt;Accuracy&lt;/span&gt;,&lt;span style='color:#0000ff'&gt;Sensitivity&lt;/span&gt;, and &lt;span style='color:#ff0000'&gt;Specificity&lt;/span&gt;\"\n\ncutoff_df |&gt;\n  ggplot() +\n  geom_point(aes(x = cutoff_vals, y = specificities),\n             color = \"#ff0000\", size = 5) +\n  geom_line(aes(x = cutoff_vals, y = specificities),\n             color = \"#ff0000\", linewidth = 1) +\n  geom_point(aes(x = cutoff_vals, y = sensitivities),\n             color = \"#0000ff\", size = 5) +\n  geom_line(aes(x = cutoff_vals, y = sensitivities),\n             color = \"#0000ff\", linewidth = 1) +\n  geom_point(aes(x = cutoff_vals, y = accuracies),\n             color = \"#ff00ff\", size = 5) +\n  geom_line(aes(x = cutoff_vals, y = accuracies),\n             color = \"#ff00ff\", linewidth = 1) +\n  labs(title = \"Confusion Matrix Metrics\",\n       subtitle = subtitle_string,\n       caption = \"SML 320\",\n       x = \"cutoff value\",\n       y = \"metric value\") +\n  theme_minimal() +\n  theme(plot.subtitle = element_markdown()) #use ggtext package"
  },
  {
    "objectID": "posts/15_logistic_regression/15_logistic_regression.html#cross-validation",
    "href": "posts/15_logistic_regression/15_logistic_regression.html#cross-validation",
    "title": "15: Logistic Regression",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nset.seed(320)\nlog_reg_mod_1_cv &lt;- bayesrules::classification_summary_cv(\n  model = log_reg_mod_1, data = pulse_df,\n  cutoff = 0.62, k = 10)\n\nlog_reg_mod_1_cv$cv\n\n  sensitivity specificity overall_accuracy\n1    0.553233   0.5416136            0.547"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html",
    "href": "posts/16_naive_bayes/16_naive_bayes.html",
    "title": "16: Naive Bayes Classification",
    "section": "",
    "text": "Goal: Use Bayesian approaches to classification tasks\nObjectives:\n\nexplore the pros and cons of naive Bayes classification\ngeneralize classification tasks for more than two categories\n\nBayes Rules! Exercise 13.14\n\n\n\nmeme\n\n\n\nlibrary(\"bayesrules\")\nlibrary(\"e1071\") #for the naiveBayes() function\nlibrary(\"ggtext\")\nlibrary(\"janitor\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\n\n\ndata(pulse_of_the_nation)\npulse_df &lt;- pulse_of_the_nation |&gt;\n  filter(education %in% c(\"Graduate degree\", \"College degree\", \"High school\"))\n\npulse_df$education &lt;- factor(pulse_df$education,\n                             levels = c(\"Graduate degree\",\n                                        \"College degree\",\n                                        \"High school\"))\npulse_df$robots &lt;- factor(pulse_df$robots,\n                          levels = c(\"Likely\", \"Unlikely\"))\npulse_df$ghosts &lt;- factor(pulse_df$ghosts,\n                          levels = c(\"Yes\", \"No\"))\n\n# https://www.color-hex.com/color-palette/46386\nghostbusters_green &lt;- \"#23B63C\"\nghostbusters_red   &lt;- \"#EA0000\"\nghostbusters_gray  &lt;- \"#AB9F8F\""
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#exploratory-data-analyses",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#exploratory-data-analyses",
    "title": "16: Naive Bayes Classification",
    "section": "Exploratory Data Analyses",
    "text": "Exploratory Data Analyses\n\nIncomeAgeEducationRobotsGhosts\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = income)) +\n  geom_density(fill = \"green\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Income of participants\",\n       caption = \"SML 320\",\n       x = \"income (thousands of dollars)\",\n       y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = age)) +\n  geom_density(fill = \"purple\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Age of participants\",\n       caption = \"SML 320\",\n       x = \"age\",\n       y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\\[Y = \\begin{cases} G, & \\text{Graduate Degree} \\\\ C, & \\text{College Degree} \\\\ H, & \\text{High School} \\end{cases}\\]\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = education, fill = education)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Education attainment of participants\",\n       caption = \"SML 320\",\n       x = \"education\",\n       y = \"count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = robots, fill = robots)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Is it likely that robots would take your jobs within the next decade\",\n       caption = \"September 2017\",\n       x = \"\",\n       y = \"count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = ghosts, fill = ghosts)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Pulse of the Nation\",\n       subtitle = \"Do you believe in ghosts?\",\n       caption = \"September 2017\",\n       x = \"\",\n       y = \"count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#variables",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#variables",
    "title": "16: Naive Bayes Classification",
    "section": "Variables",
    "text": "Variables\n\n\n\nResponse Variable\n\\(Y\\): education\n\ncategorical variable: education attained\n\n\n\nPrediction\nWhat degree does a 25-year-old who makes $50,000/year have?\n\n\n\n\n\nPredictor Variables\n\n\\(X_{1}\\): ghosts\n\\(X_{2}\\): age\n\\(X_{3}\\): income\n\\(X_{4}\\): robots"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#ideas",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#ideas",
    "title": "16: Naive Bayes Classification",
    "section": "Ideas",
    "text": "Ideas\nHere, we have three categories, whereas logistic regression is limited to classifying binary response variables. As an alternative, naive Bayes classification\n\ncan classify categorical response variables \\(Y\\) with two or more categories\ndoesn’t require much theory beyond Bayes’ Rule\nit’s computationally efficient, i.e., doesn’t require MCMC simulation\n\nBut why is it called “naive”?"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#recall-bayes-rule",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#recall-bayes-rule",
    "title": "16: Naive Bayes Classification",
    "section": "Recall: Bayes Rule",
    "text": "Recall: Bayes Rule\n\\[f(y|x_{1}) = \\frac{\\text{prior}\\cdot\\text{likelihood}}{\\text{normalizing constant}} = \\frac{f(y) \\cdot L(y|x_{1})}{f(x_{1})}\\] where, by the Law of Total Probability,\n\\[\\begin{array}{rcl}\nf(x_{1}) & = & \\displaystyle\\sum_{\\text{all } y'} f(y')L(y'|x_{1}) \\\\\n~ & = & f(y' = G)L(y' = G|x_{1}) + f(y' = C)L(y' = C|x_{1}) + f(y' = H)L(y' = H|x_{1}) \\\\\n\\end{array}\\]\nover our three education levels."
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#calculation",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#calculation",
    "title": "16: Naive Bayes Classification",
    "section": "Calculation",
    "text": "Calculation\n\npulse_df |&gt;\n  janitor::tabyl(education, ghosts) |&gt;\n  janitor::adorn_totals(c(\"row\", \"col\"))\n\n       education Yes  No Total\n Graduate degree  58 145   203\n  College degree 112 201   313\n     High school  84 107   191\n           Total 254 453   707\n\n\nPrior probabilities:\n\\[f(y = G) = \\frac{203}{707}, \\quad f(y = C) = \\frac{313}{707}, \\quad f(y = H) = \\frac{191}{707}\\]\nLikelihoods:\n\\[\\begin{array}{rcccl}\n  L(y = G | x_{1} = 1) & = & \\frac{58}{203} & \\approx & 0.2857 \\\\\n  L(y = C | x_{1} = 1) & = & \\frac{112}{313} & \\approx & 0.3578 \\\\\n  L(y = H | x_{1} = 1) & = & \\frac{84}{191} & \\approx & 0.4398 \\\\\n\\end{array}\\] Total probability:\n\\[f(x_{1} = 1) = \\frac{203}{707}\\cdot\\frac{58}{203} + \\frac{313}{707}\\cdot\\frac{112}{313} + \\frac{191}{707}\\cdot\\frac{84}{191} = \\frac{97}{270}\\]\nBayes’ Rules:\n\\[\\begin{array}{rcccccl}\n  f(y = G | x_{1} = 1) & = & \\frac{f(y = G) \\cdot L(y = G | x_{1} = 1)}{f(x_{1} = 1)} = \\frac{\\frac{203}{707}\\cdot\\frac{58}{203}}{\\frac{97}{270}} & \\approx & 0.2283 \\\\\n  f(y = C | x_{1} = 1) & = & \\frac{f(y = C) \\cdot L(y = C | x_{1} = 1)}{f(x_{1} = 1)} = \\frac{\\frac{313}{707}\\cdot\\frac{112}{313}}{\\frac{97}{270}} & \\approx & 0.4409 \\\\\n  f(y = H | x_{1} = 1) & = & \\frac{f(y = H) \\cdot L(y = H | x_{1} = 1)}{f(x_{1} = 1)} = \\frac{\\frac{191}{707}\\cdot\\frac{84}{191}}{\\frac{97}{270}} & \\approx & 0.3307 \\\\\n\\end{array}\\]\nWhile the likelihoods pointed at the high school level, the survey had more college degree participants, so the posterior distribution balance, so far, assigns the highest probability to the college degree level"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#naivety",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#naivety",
    "title": "16: Naive Bayes Classification",
    "section": "Naivety",
    "text": "Naivety\nThis is where one “naive” part of naive Bayes classification comes into play. The naive Bayes method typically assumes that any quantitative predictor, here \\(X_{2}\\), is continuous and conditionally normal:\n\\[\\begin{array}{rcl}\n  X_{2} | (Y = G) & \\sim & N(\\mu_{G}, \\sigma_{G}^{2}) \\\\\n  X_{2} | (Y = C) & \\sim & N(\\mu_{C}, \\sigma_{C}^{2}) \\\\\n  X_{2} | (Y = H) & \\sim & N(\\mu_{H}, \\sigma_{H}^{2}) \\\\\n\\end{array}\\]\n\n# Calculate sample mean and sd for each Y group\npulse_df |&gt;\n  group_by(education) |&gt;\n  summarize(mean = mean(age, na.rm = TRUE), \n            sd = sd(age, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  education        mean    sd\n  &lt;fct&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 Graduate degree  51.3  15.3\n2 College degree   49.0  16.1\n3 High school      50.2  17.2"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#priors",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#priors",
    "title": "16: Naive Bayes Classification",
    "section": "Priors",
    "text": "Priors\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = age, color = education)) + \n  stat_function(fun = dnorm, args = list(mean = 51.34975, sd = 15.27409), \n                aes(color = \"Graduate degree\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 50.17277, sd = 17.18495 ),\n                aes(color = \"High school\"), linewidth = 3) + \n  stat_function(fun = dnorm, args = list(mean = 48.98403, sd = 16.10412),\n                aes(color = \"College degree\"), linewidth = 3) +\n  geom_vline(xintercept = 25, linetype = \"dashed\") + \n  labs(title = \"&lt;span style = 'color:#23B63C'&gt;Prior Probabilities&lt;/span&gt;\",\n       subtitle = \"conditionally normal\",\n       caption = \"R4DS Book Club\") +\n  scale_color_manual(values = c(ghostbusters_green, ghostbusters_red , ghostbusters_gray)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#likelihoods",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#likelihoods",
    "title": "16: Naive Bayes Classification",
    "section": "Likelihoods",
    "text": "Likelihoods\nComputing the likelihoods in R:\n\n# L(y = G | x_2 = 25) = 0.0060\ndnorm(25, mean = 51.3, sd = 15.3)\n\n# L(y = C | x_2 = 25) = 0.0082\ndnorm(25, mean = 49.0, sd = 16.1)\n\n# L(y = H | x_2 = 25) = 0.0079\ndnorm(25, mean = 50.2, sd = 17.2)\n\nTotal probability:\n\\[f(x_{2} = 25) = \\frac{203}{707}\\cdot(0.0060) + \\frac{313}{707}\\cdot(0.0082) + \\frac{191}{707}\\cdot(0.0079) \\approx 0.0074\\]"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#bayes-rule",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#bayes-rule",
    "title": "16: Naive Bayes Classification",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\\[\\begin{array}{rcccccl}\n  f(y = G | x_{2} = 25) & = & \\frac{f(y = G) \\cdot L(y = G | x_{2} = 25)}{f(x_{2} = 25)} = \\frac{\\frac{203}{707}\\cdot(0.0060)}{0.0074} & \\approx & 0.2328 \\\\\n  f(y = C | x_{2} = 25) & = & \\frac{f(y = C) \\cdot L(y = C | x_{2} = 25)}{f(x_{2} = 25)} = \\frac{\\frac{313}{707}\\cdot(0.0082)}{0.0074} & \\approx & 0.4906 \\\\\n  f(y = H | x_{2} = 25) & = & \\frac{f(y = H) \\cdot L(y = H | x_{2} = 25)}{f(x_{2} = 25)} = \\frac{\\frac{191}{707}\\cdot(0.0079)}{0.0074} & \\approx & 0.2884 \\\\\n\\end{array}\\]\nThis model also places the most probability the the participant at the college degree education level."
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#naivety-1",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#naivety-1",
    "title": "16: Naive Bayes Classification",
    "section": "Naivety",
    "text": "Naivety\nThis is where one “naive” part of naive Bayes classification comes into play. The naive Bayes method typically assumes that any quantitative predictor, here \\(X_{3}\\), is continuous and conditionally normal:\n\\[\\begin{array}{rcl}\n  X_{3} | (Y = G) & \\sim & N(\\mu_{G}, \\sigma_{G}^{2}) \\\\\n  X_{3} | (Y = C) & \\sim & N(\\mu_{C}, \\sigma_{C}^{2}) \\\\\n  X_{3} | (Y = H) & \\sim & N(\\mu_{H}, \\sigma_{H}^{2}) \\\\\n\\end{array}\\]\n\n# Calculate sample mean and sd for each Y group\npulse_df |&gt;\n  group_by(education) |&gt;\n  summarize(mean = mean(income, na.rm = TRUE), \n            sd = sd(income, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  education        mean    sd\n  &lt;fct&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 Graduate degree 114.   71.0\n2 College degree  102.   59.7\n3 High school      63.9  35.5"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#priors-1",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#priors-1",
    "title": "16: Naive Bayes Classification",
    "section": "Priors",
    "text": "Priors\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npulse_df |&gt;\n  ggplot(aes(x = income, color = education)) + \n  stat_function(fun = dnorm, args = list(mean = 114, sd = 71), \n                aes(color = \"Graduate degree\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 102, sd = 59.7),\n                aes(color = \"College degree\"), linewidth = 3) +\n  stat_function(fun = dnorm, args = list(mean = 63.9, sd = 35.5),\n                aes(color = \"High school\"), linewidth = 3) + \n  geom_vline(xintercept = 50, linetype = \"dashed\", linewidth = 2) + \n  labs(title = \"&lt;span style = 'color:#AB9F8F'&gt;Prior Probabilities&lt;/span&gt;\",\n       subtitle = \"conditionally normal\",\n       caption = \"R4DS Book Club\") +\n  scale_color_manual(values = c(ghostbusters_green, ghostbusters_red , ghostbusters_gray)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown(face = \"bold\", size = 24),\n        plot.subtitle = element_markdown(size = 16))"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#likelihoods-1",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#likelihoods-1",
    "title": "16: Naive Bayes Classification",
    "section": "Likelihoods",
    "text": "Likelihoods\nComputing the likelihoods in R:\n\n# L(y = G | x_3 = 50) = 0.0037\ndnorm(50, mean = 114, sd = 71)\n\n# L(y = C | x_3 = 50) = 0.0046\ndnorm(50, mean = 102, sd = 59.7)\n\n# L(y = H | x_3 = 50) = 0.0104\ndnorm(50, mean = 63.9, sd = 35.5)\n\nTotal probability:\n\\[f(x_{3} = 50) = \\frac{203}{707}\\cdot(0.0037) + \\frac{313}{707}\\cdot(0.0046) + \\frac{191}{707}\\cdot(0.0104) \\approx 0.0059\\]"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#bayes-rule-1",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#bayes-rule-1",
    "title": "16: Naive Bayes Classification",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\\[\\begin{array}{rcccccl}\n  f(y = G | x_{3} = 50) & = & \\frac{f(y = G) \\cdot L(y = G | x_{3} = 50)}{f(x_{3} = 50)} = \\frac{\\frac{203}{707}\\cdot(0.0037)}{0.0059} & \\approx & 0.1801 \\\\\n  f(y = C | x_{3} = 50) & = & \\frac{f(y = C) \\cdot L(y = C | x_{3} = 50)}{f(x_{3} = 50)} = \\frac{\\frac{313}{707}\\cdot(0.0046)}{0.0059} & \\approx & 0.3452 \\\\\n  f(y = H | x_{3} = 50) & = & \\frac{f(y = H) \\cdot L(y = H | x_{3} = 50)}{f(x_{3} = 50)} = \\frac{\\frac{191}{707}\\cdot(0.0104)}{0.0059} & \\approx & 0.4762 \\\\\n\\end{array}\\]\nThis model places the most probability the the participant at the high school education level."
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#likelihoods-2",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#likelihoods-2",
    "title": "16: Naive Bayes Classification",
    "section": "Likelihoods",
    "text": "Likelihoods\n\npulse_df |&gt;\n  janitor::tabyl(education, robots) |&gt;\n  janitor::adorn_totals(c(\"row\", \"col\"))\n\n       education Likely Unlikely Total\n Graduate degree     26      177   203\n  College degree     45      268   313\n     High school     69      122   191\n           Total    140      567   707\n\n\n\\[\\begin{array}{rcccl}\n  L(y = G | x_{4} = 1) & = & \\frac{26}{203} & \\approx & 0.1281 \\\\\n  L(y = C | x_{4} = 1) & = & \\frac{45}{313} & \\approx & 0.1438 \\\\\n  L(y = H | x_{4} = 1) & = & \\frac{69}{191} & \\approx & 0.3089 \\\\\n\\end{array}\\]\nTotal probability:\n\\[f(x_{4} = 1) = \\frac{203}{707}\\cdot\\frac{26}{203} + \\frac{313}{707}\\cdot\\frac{45}{313} + \\frac{191}{707}\\cdot\\frac{69}{191} = \\frac{143}{707}\\]"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#bayes-rule-2",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#bayes-rule-2",
    "title": "16: Naive Bayes Classification",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\\[\\begin{array}{rcccccl}\n  f(y = G | x_{4} = 1) & = & \\frac{f(y = G) \\cdot L(y = G | x_{4} = 1)}{f(x_{4} = 1)} = \\frac{\\frac{203}{707}\\cdot\\frac{26}{203}}{\\frac{143}{707}} & \\approx & 0.1818 \\\\\n  f(y = C | x_{4} = 1) & = & \\frac{f(y = C) \\cdot L(y = C | x_{4} = 1)}{f(x_{4} = 1)} = \\frac{\\frac{313}{707}\\cdot\\frac{45}{313}}{\\frac{143}{707}} & \\approx & 0.3147 \\\\\n  f(y = H | x_{4} = 1) & = & \\frac{f(y = H) \\cdot L(y = H | x_{4} = 1)}{f(x_{4} = 1)} = \\frac{\\frac{191}{707}\\cdot\\frac{69}{191}}{\\frac{143}{707}} & \\approx & 0.4825 \\\\\n\\end{array}\\]\nThis model places the most probability the the participant at the high school education level."
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#generalizing-bayes-rule",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#generalizing-bayes-rule",
    "title": "16: Naive Bayes Classification",
    "section": "Generalizing Bayes’ Rule:",
    "text": "Generalizing Bayes’ Rule:\n\\[f(y | x_{2}, x_{3}) = \\frac{f(y) \\cdot L(y | x_{2}, x_{3})}{\\sum_{y'} f(y') \\cdot L(y' | x_{2}, x_{3})}\\]\nAnother “naive” assumption of conditionally independent:\n\\[L(y | x_{2}, x_{3}) = f(x_{2}, x_{3} | y) = f(x_{2} | y) \\cdot f(x_{3} | y)\\]\n\nmathematically efficient\nbut what about correlation?"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#likelihoods-3",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#likelihoods-3",
    "title": "16: Naive Bayes Classification",
    "section": "Likelihoods",
    "text": "Likelihoods\n\n\n\n# L(y = G | x_2 = 25) = 0.0060\ndnorm(25, mean = 51.3, sd = 15.3)\n\n# L(y = C | x_2 = 25) = 0.0082\ndnorm(25, mean = 49.0, sd = 16.1)\n\n# L(y = H | x_2 = 25) = 0.0079\ndnorm(25, mean = 50.2, sd = 17.2)\n\n\n\n\n\n# L(y = G | x_3 = 50) = 0.0037\ndnorm(50, mean = 114, sd = 71)\n\n# L(y = C | x_3 = 50) = 0.0046\ndnorm(50, mean = 102, sd = 59.7)\n\n# L(y = H | x_3 = 50) = 0.0104\ndnorm(50, mean = 63.9, sd = 35.5)\n\n\n\n\\[f(x_2 = 50, x_{3} = 50) = \\frac{203}{707}\\cdot(0.0060)(0.0037) + \\frac{313}{707}\\cdot(0.0082)(0.0046) + \\frac{191}{707}\\cdot(0.0079)(0.0104) \\approx 0.00004527\\]"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#bayes-rule-3",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#bayes-rule-3",
    "title": "16: Naive Bayes Classification",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\\[\\begin{array}{rcccccl}\n  f(y = G | x_2 = 50, x_{3} = 50) & = & \\frac{f(y = G) \\cdot L(y = G | x_2 = 50, x_{3} = 50)}{f(x_2 = 50, x_{3} = 50)} = \\frac{\\frac{203}{707}\\cdot(0.0060)(0.0037)}{0.00004527} & \\approx & 0.1398 \\\\\n  f(y = C | x_2 = 50, x_{3} = 50) & = & \\frac{f(y = C) \\cdot L(y = C | x_2 = 50, x_{3} = 50)}{f(x_2 = 50, x_{3} = 50)} = \\frac{\\frac{313}{707}\\cdot(0.0082)(0.0046)}{0.00004527} & \\approx & 0.3660 \\\\\n  f(y = H | x_2 = 50, x_{3} = 50) & = & \\frac{f(y = H) \\cdot L(y = H | x_2 = 50, x_{3} = 50)}{f(x_2 = 50, x_{3} = 50)} = \\frac{\\frac{191}{707}\\cdot(0.0079)(0.0104)}{0.00004527} & \\approx & 0.4942 \\\\\n\\end{array}\\]\nThis model places the most probability the the participant at the high school education level."
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#cross-validation",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#cross-validation",
    "title": "16: Naive Bayes Classification",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nset.seed(320)\nnaive_model_1_cv &lt;- bayesrules::naive_classification_summary_cv(\n  model = naive_model_1, data = pulse_df, y = \"education\", k = 10)\n\nnaive_model_2_cv &lt;- bayesrules::naive_classification_summary_cv(\n  model = naive_model_2, data = pulse_df, y = \"education\", k = 10)\n\nnaive_model_3_cv &lt;- bayesrules::naive_classification_summary_cv(\n  model = naive_model_3, data = pulse_df, y = \"education\", k = 10)\n\nnaive_model_4_cv &lt;- bayesrules::naive_classification_summary_cv(\n  model = naive_model_4, data = pulse_df, y = \"education\", k = 10)\n\nnaive_model_5_cv &lt;- bayesrules::naive_classification_summary_cv(\n  model = naive_model_5, data = pulse_df, y = \"education\", k = 10)\n\nnaive_model_6_cv &lt;- bayesrules::naive_classification_summary_cv(\n  model = naive_model_6, data = pulse_df, y = \"education\", k = 10)\n\n\n\n\n\n\n\nAccuracy of a Confusion Matrix\n\n\n\n\n\n\nConfusion_Accuracy &lt;- function(conf_mat){\n  # This function will compute the accuracy of a confusion matrix\n  \n  m &lt;- nrow(conf_mat)\n  numbers_list &lt;- stringr::str_extract_all(unlist(conf_mat), \n                                              \"\\\\([0-9]+\\\\)\")\n  numbers_string &lt;- paste(unlist(numbers_list))\n  numbers_string &lt;- stringr::str_replace_all(numbers_string, \"\\\\(\", \"\")\n  numbers_string &lt;- stringr::str_replace_all(numbers_string, \"\\\\)\", \"\")\n  numbers &lt;- as.numeric(numbers_string)\n  M &lt;- matrix(numbers, nrow = m, ncol = m)\n  \n  # return\n  sum(diag(M)) / sum(M)\n}\n\n\n\n\n\nModel 1Model 2Model 3Model 4Model 5Model 6\n\n\n\nnaive_model_1_cv$cv\n\n       education Graduate degree College degree High school\n Graduate degree       0.00% (0)  100.00% (203)   0.00% (0)\n  College degree       0.00% (0)  100.00% (313)   0.00% (0)\n     High school       0.00% (0)  100.00% (191)   0.00% (0)\n\n\n\nConfusion_Accuracy(naive_model_1_cv$cv)\n\n[1] 0.4427157\n\n\n\n\n\nnaive_model_2_cv$cv\n\n       education Graduate degree College degree High school\n Graduate degree       0.00% (0)   99.01% (201)   0.99% (2)\n  College degree       0.00% (0)   98.40% (308)   1.60% (5)\n     High school       0.00% (0)   99.48% (190)   0.52% (1)\n\n\n\nConfusion_Accuracy(naive_model_2_cv$cv)\n\n[1] 0.437058\n\n\n\n\n\nnaive_model_3_cv$cv\n\n       education Graduate degree College degree  High school\n Graduate degree      6.40% (13)   65.02% (132) 28.57%  (58)\n  College degree      3.19% (10)   57.83% (181) 38.98% (122)\n     High school      0.00%  (0)   30.89%  (59) 69.11% (132)\n\n\n\nConfusion_Accuracy(naive_model_3_cv$cv)\n\n[1] 0.4611033\n\n\n\n\n\nnaive_model_4_cv$cv\n\n       education Graduate degree College degree High school\n Graduate degree       0.00% (0)   87.19% (177) 12.81% (26)\n  College degree       0.00% (0)   85.62% (268) 14.38% (45)\n     High school       0.00% (0)   63.87% (122) 36.13% (69)\n\n\n\nConfusion_Accuracy(naive_model_4_cv$cv)\n\n[1] 0.476662\n\n\n\n\n\nnaive_model_5_cv$cv\n\n       education Graduate degree College degree  High school\n Graduate degree      5.42% (11)   66.50% (135) 28.08%  (57)\n  College degree      4.15% (13)   58.47% (183) 37.38% (117)\n     High school      0.00%  (0)   32.98%  (63) 67.02% (128)\n\n\n\nConfusion_Accuracy(naive_model_5_cv$cv)\n\n[1] 0.4554455\n\n\n\n\n\nnaive_model_6_cv$cv\n\n       education Graduate degree College degree  High school\n Graduate degree      7.39% (15)   78.33% (159) 14.29%  (29)\n  College degree      5.75% (18)   69.97% (219) 24.28%  (76)\n     High school      0.52%  (1)   45.55%  (87) 53.93% (103)\n\n\n\nConfusion_Accuracy(naive_model_6_cv$cv)\n\n[1] 0.476662"
  },
  {
    "objectID": "posts/16_naive_bayes/16_naive_bayes.html#model-selection",
    "href": "posts/16_naive_bayes/16_naive_bayes.html#model-selection",
    "title": "16: Naive Bayes Classification",
    "section": "Model Selection",
    "text": "Model Selection\n\n\n\n\n\n\n  \n    \n      Naive Bayes on Pulse Survey Data\n    \n    \n      Cross validation metrics\n    \n    \n      model\n      Accuracy\n    \n  \n  \n    1\n0.442716\n    2\n0.437058\n    3\n0.461103\n    4\n0.476662\n    5\n0.455446\n    6\n0.476662"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html",
    "title": "17: Hierarchical Models",
    "section": "",
    "text": "Goal: Apply group classifications to models\n\n\n\nJersey City, NJ\n\n\n\n\n\n\nlibrary(\"bayesplot\")\nlibrary(\"bayesrules\")\nlibrary(\"ggtext\")\nlibrary(\"janitor\")\nlibrary(\"patchwork\")\nlibrary(\"rstan\")\nlibrary(\"rstanarm\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\nairbnb_raw &lt;- readr::read_csv(\"listings.csv\")\n\n# brand colors\n# https://pickcoloronline.com/brands/airbnb/\nairbnb_red &lt;- \"#FF5A5F\"\nairbnb_green &lt;- \"#00A699\"\nairbnb_orange &lt;- \"#FC642D\"\nairbnb_black &lt;- \"#484848\"\nairbnb_gray &lt;- \"#767676\"\n\n\n\n\n\n\n\n\nsource: Airbnb listings\n22 December, 2023\n“Summary information and metrics for listings in Jersey City (good for visualisations).”\n\nBefore data wrangling:\n\n1549 observations\n18 variables\n\n\n\n\n\n\n\nJersey City, NJ\n\n\n\n\n\n\n\n\n\n\nextracted ward classification\nnonzero availablity_365\nexcluded rare “Hotel” or “Shared Room” listing\nremoved observations with missing values\n\nAfter data wrangling:\n\n1348 observations\n7 variables\n\n\n\n\n\n\nairbnb_df &lt;- airbnb_raw |&gt;\n  mutate(ward = stringr::str_sub(neighbourhood,6,6)) |&gt;\n  select(price, host_id, number_of_reviews, availability_365, \n         room_type, ward) |&gt;\n  filter(availability_365 &gt; 0) |&gt;\n  filter(room_type %in% c(\"Entire home/apt\", \"Private room\")) |&gt;\n  na.omit() |&gt;\n  group_by(host_id) |&gt;\n  mutate(listings = n()) |&gt;\n  ungroup()\n\n\n\n\n\n\n\nReviewsAvailabilityTypeWard\n\n\n\n\n\n\n\n\nairbnb_df |&gt;\n  ggplot(aes(x = number_of_reviews, y = price)) +\n  geom_point(color = \"#767676\") +\n  labs(title = \"Airbnb Data\",\n       subtitle = \"Price versus number of reviews\",\n       caption = \"SML 320\",\n       x = \"number of reviews\",\n       y = \"price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nairbnb_df |&gt;\n  ggplot(aes(x = availability_365)) +\n  geom_density(color = \"#484848\", fill = \"#FC642D\") +\n  labs(title = \"Airbnb Data\",\n       subtitle = \"Availability over calendar year\",\n       caption = \"SML 320\",\n       x = \"availability\",\n       y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\nairbnb_df |&gt;\n  ggplot(aes(x = room_type, fill = room_type)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Airbnb Data\",\n       subtitle = \"Listing Types\",\n       caption = \"SML 320\",\n       x = \"\",\n       y = \"count\") +\n  scale_fill_manual(values = c(\"#FF5A5F\", \"#00A699\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nairbnb_df |&gt;\n  ggplot(aes(x = ward, fill = ward)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Airbnb Data\",\n       subtitle = \"ward\",\n       caption = \"SML 320\",\n       x = \"\",\n       y = \"count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html#data",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html#data",
    "title": "17: Hierarchical Models",
    "section": "",
    "text": "source: Airbnb listings\n22 December, 2023\n“Summary information and metrics for listings in Jersey City (good for visualisations).”\n\nBefore data wrangling:\n\n1549 observations\n18 variables\n\n\n\n\n\n\n\nJersey City, NJ"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html#data-wrangling",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html#data-wrangling",
    "title": "17: Hierarchical Models",
    "section": "",
    "text": "extracted ward classification\nnonzero availablity_365\nexcluded rare “Hotel” or “Shared Room” listing\nremoved observations with missing values\n\nAfter data wrangling:\n\n1348 observations\n7 variables\n\n\n\n\n\n\nairbnb_df &lt;- airbnb_raw |&gt;\n  mutate(ward = stringr::str_sub(neighbourhood,6,6)) |&gt;\n  select(price, host_id, number_of_reviews, availability_365, \n         room_type, ward) |&gt;\n  filter(availability_365 &gt; 0) |&gt;\n  filter(room_type %in% c(\"Entire home/apt\", \"Private room\")) |&gt;\n  na.omit() |&gt;\n  group_by(host_id) |&gt;\n  mutate(listings = n()) |&gt;\n  ungroup()"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html#exploratory-data-analyses",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html#exploratory-data-analyses",
    "title": "17: Hierarchical Models",
    "section": "",
    "text": "ReviewsAvailabilityTypeWard\n\n\n\n\n\n\n\n\nairbnb_df |&gt;\n  ggplot(aes(x = number_of_reviews, y = price)) +\n  geom_point(color = \"#767676\") +\n  labs(title = \"Airbnb Data\",\n       subtitle = \"Price versus number of reviews\",\n       caption = \"SML 320\",\n       x = \"number of reviews\",\n       y = \"price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nairbnb_df |&gt;\n  ggplot(aes(x = availability_365)) +\n  geom_density(color = \"#484848\", fill = \"#FC642D\") +\n  labs(title = \"Airbnb Data\",\n       subtitle = \"Availability over calendar year\",\n       caption = \"SML 320\",\n       x = \"availability\",\n       y = \"\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\nairbnb_df |&gt;\n  ggplot(aes(x = room_type, fill = room_type)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Airbnb Data\",\n       subtitle = \"Listing Types\",\n       caption = \"SML 320\",\n       x = \"\",\n       y = \"count\") +\n  scale_fill_manual(values = c(\"#FF5A5F\", \"#00A699\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nairbnb_df |&gt;\n  ggplot(aes(x = ward, fill = ward)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Airbnb Data\",\n       subtitle = \"ward\",\n       caption = \"SML 320\",\n       x = \"\",\n       y = \"count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html#predictions",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html#predictions",
    "title": "17: Hierarchical Models",
    "section": "Predictions",
    "text": "Predictions\n\nVizCode\n\n\n\n\n\n\n\n\n\n\nset.seed(320)\nward_preds &lt;- rstanarm::posterior_predict(\n  ward_hier_model, \n  newdata = data.frame(ward = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\")))\n\nbayesplot::mcmc_areas(ward_preds, prob = 0.8) +\n  labs(title = \"Posterior Distributions for Airbnb Prices\",\n       subtitle = \"For each ward in Jersey City\",\n       caption = \"SML 320\",\n       x = \"price\", y = \"ward\") +\n  scale_y_discrete(labels = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"))"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html#predictions-1",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html#predictions-1",
    "title": "17: Hierarchical Models",
    "section": "Predictions",
    "text": "Predictions\n\nVizCode\n\n\n\n\n\n\n\n\n\n\nset.seed(320)\ntype_preds &lt;- rstanarm::posterior_predict(\n  type_hier_model, \n  newdata = data.frame(number_of_reviews = 100,\n                       availability_365 = 200,\n                       room_type = c(\"Entire home/apt\", \"Private room \")))\n\nbayesplot::mcmc_areas(type_preds, prob = 0.8) +\n  labs(title = \"Posterior Distributions for Airbnb Prices\",\n       subtitle = \"Host with 100 reviews and 200 days/year availability\",\n       caption = \"SML 320\",\n       x = \"price\", y = \"room type\") +\n  scale_y_discrete(labels = c(\"Entire home/apt\", \"Private room \"))"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html#means",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html#means",
    "title": "17: Hierarchical Models",
    "section": "Means",
    "text": "Means\n\nglobal mean\n\n\\[\\bar{y}_{\\text{global}} = \\displaystyle\\frac{1}{n}\\sum_{i,j} y_{ij}\\]\n\ngroup mean\n\n\\[\\bar{y}_{j} = \\displaystyle\\frac{1}{n_{j}}\\sum_{i=1}^{n_{j}} y_{ij}\\]"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html#definition",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html#definition",
    "title": "17: Hierarchical Models",
    "section": "Definition",
    "text": "Definition\nShrinkage refers to the phenomenon in which the group-specific local trends in a hierarchical model are pulled or shrunk toward the global trends.\n\\[\\displaystyle\\frac{\\sigma_{y}^{2}}{\\sigma_{y}^{2} + n_{j}\\sigma_{\\mu}^{2}} \\cdot \\bar{y}_{\\text{global}} + \\displaystyle\\frac{n_{j}\\sigma_{\\mu}^{2}}{\\sigma_{y}^{2} + n_{j}\\sigma_{\\mu}^{2}} \\cdot \\bar{y}_{j}\\]\n\n\\(n_{j}\\) decreases \\(\\rightarrow\\) shrinkage increases (i.e. rely more on global trends)\n\\(\\sigma_{y} &gt; \\sigma_{\\mu} \\rightarrow\\) shrinkage increases (i.e. more within group variance than between group variance)"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html#posterior-predictive-check",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html#posterior-predictive-check",
    "title": "17: Hierarchical Models",
    "section": "Posterior Predictive Check",
    "text": "Posterior Predictive Check\n\nVizCode\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesplot::pp_check(ward_hier_model) +\n  labs(title = \"Ward Hierarchical Model\", x = \"price\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.position = \"none\")\np2 &lt;- bayesplot::pp_check(type_hier_model) +\n  labs(title = \"Room Type Hierarchical Model\", x = \"price\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.position = \"bottom\")\n\np1 + p2"
  },
  {
    "objectID": "posts/17_hierarchical_models/17_hierarchical_models.html#median-absolute-error",
    "href": "posts/17_hierarchical_models/17_hierarchical_models.html#median-absolute-error",
    "title": "17: Hierarchical Models",
    "section": "Median Absolute Error",
    "text": "Median Absolute Error\n\nset.seed(320)\nbayesrules::prediction_summary(ward_hier_model, data = airbnb_df) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      mae mae_scaled within_50 within_95\n1 72.5942     0.4707    0.7181    0.9592\n\n\n\nset.seed(320)\nbayesrules::prediction_summary(type_hier_model, data = airbnb_df) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      mae mae_scaled within_50 within_95\n1 53.0925     0.3673    0.7871    0.9651"
  }
]