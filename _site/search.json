[
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html",
    "href": "posts/01_conditional_probability/01_conditional_probability.html",
    "title": "1: Conditional Probability",
    "section": "",
    "text": "Spring 2024\nTuTh, 11 AM to 1220 PM\nBendheim House 103\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides an introduction to Bayesian analysis—a powerful statistical framework for making inferences and modeling uncertainty in a wide range of applications. Students will explore the fundamental principles of Bayesian statistics, probability theory, Bayesian inference, and practical applications of Bayesian modeling. The course will cover both the theory and hands-on implementation using data science software and the R programming language."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SML 320: Bayesian Analysis",
    "section": "",
    "text": "5: Conjugate Families\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n4: Balance and Sequentiality\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n3: Beta-Binomial Models\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n2: Bayes’ Rule\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n1: Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html",
    "href": "posts/02_bayes_rule/02_bayes_rule.html",
    "title": "2: Bayes’ Rule",
    "section": "",
    "text": "In the previous section, we studied conditional probability \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] and we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is \\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\] This is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is \\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] Bayes’ Rule combines the ideas of conditioned probability and total probability as \\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#bayes-rule",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#bayes-rule",
    "title": "2: Bayes’ Rule",
    "section": "",
    "text": "In the previous section, we studied conditional probability \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] and we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is \\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\] This is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is \\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] Bayes’ Rule combines the ideas of conditioned probability and total probability as \\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#a-deep-dive",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#a-deep-dive",
    "title": "2: Bayes’ Rule",
    "section": "A Deep Dive",
    "text": "A Deep Dive\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T)\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#more-practice",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#more-practice",
    "title": "2: Bayes’ Rule",
    "section": "More Practice",
    "text": "More Practice\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T^{c})\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-spam-filtering",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-spam-filtering",
    "title": "2: Bayes’ Rule",
    "section": "Example: Spam Filtering",
    "text": "Example: Spam Filtering\nIn 2002, Paul Graham used Bayes’ Rule as part of his algorithms to greatly decrease false positive rates of unwanted e-mails (“spam”). Let \\(H^{c}\\) be the event that an e-mail is “spam”. Let \\(W\\) be the event that an e-mail contains a trigger word such as “watches”. Suppose that\n\nthe probability that an e-mail contains that word given that it is spam is 17%\nthe probability that an e-mail contains that word given that it is not spam is 9%\nthe probability that a randomly selected e-mail message is spam is 80%\n\nFind the probability that an e-mail message is spam, given that the trigger word appears."
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-quality-control",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-quality-control",
    "title": "2: Bayes’ Rule",
    "section": "Example: Quality Control",
    "text": "Example: Quality Control\nA manufacturing process produces integrated circuit chips. Over the long run the fraction of bad chips produced by the process is around 20%. Thoroughly testing a chip to determine whether it is good or bad is rather expensive, so a cheap test is tried. All good chips will pass the cheap test, but so will 10% of the bad chips. Given that a chip passes the test, what is the probability that the chip was defective?"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-dui-checkpoint",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-dui-checkpoint",
    "title": "2: Bayes’ Rule",
    "section": "Example: DUI Checkpoint",
    "text": "Example: DUI Checkpoint\nA breath analyzer, used by the police to test whether drivers exceed the legal limit set for the blood alcohol percentage while driving, is known to satisfy\n\\[P(A|B) = P(A^{c}|B^{c}) = x\\]\nwhere \\(A\\) is the event “breath analyzer indicates that legal limit is exceeded” and \\(B\\) “driver’s blood alcohol percentage exceeds legal limit.” On Saturday nights, about 4% of the drivers are known to exceed the limit.\n\nDescribe in words the meaning of \\(P(B|A)\\)\nDetermine \\(P(B|A)\\) if \\(x = 0.90\\)\nHow big should \\(x\\) be so that \\(P(B|A) \\geq 0.95\\)?"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-monty-hall-problem",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-monty-hall-problem",
    "title": "2: Bayes’ Rule",
    "section": "Example: Monty Hall Problem",
    "text": "Example: Monty Hall Problem\n\n\n\n\nMonty Hall asks you to choose one of three doors. One of the doors hides a prize and the other two doors have no prize. You state out loud which door you pick, but you don’t open it right away.\n“Monty opens one of the other two doors, and there is no prize behind it.\n“At this moment, there are two closed doors, one of which you picked. The prize is behind one of the closed doors, but you don’t know which one. Monty asks you, ‘Do you want to switch doors?’”\n\nswitch doors\ndo not switch doors"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#generalized-bayes-rule",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#generalized-bayes-rule",
    "title": "2: Bayes’ Rule",
    "section": "Generalized Bayes’ Rule",
    "text": "Generalized Bayes’ Rule\nIf we are conditioning \\(B\\) on an event \\(A\\), where the latter can be partitioned into several subsets,\n\\[A = \\{ A_{1}, A_{2}, ..., A_{j} \\}\\]\nthen the total probability is\n\\[P(B) = P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n})\\]\nand Bayes Rule for computing the probability of \\(A_{i}\\) given \\(B\\) becomes\n\\[P(A_{i}|B) = \\displaystyle\\frac{ P(B|A_{i}) \\cdot P(A_{i}) }{ P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n}) }\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#bayesian-odds",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#bayesian-odds",
    "title": "2: Bayes’ Rule",
    "section": "Bayesian Odds",
    "text": "Bayesian Odds\n\n\n\n\n\n\nNote\n\n\n\nThe Bayesian odds of event \\(A\\) to event \\(B\\) given that event \\(C\\) has already taken place is \\[\\displaystyle\\frac{ P(A|C) }{ P(B|C) }\\]"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-analysis",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-analysis",
    "title": "1: Conditional Probability",
    "section": "",
    "text": "Spring 2024\nTuTh, 11 AM to 1220 PM\nBendheim House 103\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides an introduction to Bayesian analysis—a powerful statistical framework for making inferences and modeling uncertainty in a wide range of applications. Students will explore the fundamental principles of Bayesian statistics, probability theory, Bayesian inference, and practical applications of Bayesian modeling. The course will cover both the theory and hands-on implementation using data science software and the R programming language."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#lecturer",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#lecturer",
    "title": "1: Conditional Probability",
    "section": "Lecturer",
    "text": "Lecturer"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#current-research-in-pedagogy",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#current-research-in-pedagogy",
    "title": "1: Conditional Probability",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\nflipped classrooms"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#identity-statement",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#identity-statement",
    "title": "1: Conditional Probability",
    "section": "Identity Statement",
    "text": "Identity Statement\n\n\n\nOriginally from Los Angeles\nMath: easier to understand through graphs\nComputer Programming: years of experience with R, Python, MATLAB, PHP, HTML, etc.\nLearning: drawn to puzzles and manageable tasks\nPersonality: shy, introvert"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#textbook",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#textbook",
    "title": "1: Conditional Probability",
    "section": "Textbook",
    "text": "Textbook\n\n\nThis course will closely follow the Bayes Rules! textbook by Alicia A Johnson, Miles Q Ott, and Mine Dogucu. It is, in my opinion, the best blend of Bayesian thought, mathematical background, computer processes, and relevant applications. The authors have made the materials of their textbook available online at https://www.bayesrulesbook.com/\n\n\n\n\nBayes Rules"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#additional-reading",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#additional-reading",
    "title": "1: Conditional Probability",
    "section": "Additional Reading",
    "text": "Additional Reading\nThe following list of books is optional for student studies, but the instructor may use some materials to add depth and interest to the course.\n\n\n\n\n\n\nAdditional Reading\n\n\n\n\n\n\nStatistical Rethinking by Richard McElreath is the premier body of work in the field of Bayesian analysis. This resource is great for people who want to build a strong foundation in philosophy and theory in this branch of mathematics.\nBayesian Data Analysis by Andrew Gelman, et al., is the classic textbook (available online) in this field that is used in several university courses. The authors’ approach work well for people looking to quickly add Bayesian approaches to their research skills.\nBayesian Statistics the Fun Way by Will Kurt brings Bayesian notions to a broad audience and its presentation blends will with an introductory course in statistics.\nBayesian Thinking in Biostatistics by Gary L Rosner, et al., provides rigorous applications in bioinformatics along with strong software use."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#cooperative-classroom",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#cooperative-classroom",
    "title": "1: Conditional Probability",
    "section": "Cooperative Classroom",
    "text": "Cooperative Classroom\nLearning in a cooperative environment should be stimulating, demanding, and fair. Because this approach to learning is different from the competitive classroom structure that many other courses used to be based on, it is important for us to be clear about mutual expectations. Below are my expectations for students in this class. This set of expectations is intended to maximize debate and exchange of ideas in an atmosphere of mutual respect while preserving individual ownership of ideas and written words. If you feel you do not understand or cannot agree to these expectations, you should discuss this with your instructor and classmates.\n\nStudents are expected to work cooperatively with other members of the class and show respect for the ideas and contributions of other people.\nWhen working as part of a group, students should strive to be good contributors to the group, listen to others, not dominate, and recognize the contributions of others. Students should try to ensure that everyone in the group is welcome to contribute and recognize that everyone contributes in different ways to a group process.\nStudents should explore data, make observations, and develop inferences as part of a group. If you use material from published sources, you must provide appropriate attribution.\n\n\n\n(Students will be asked to acknowledge this document in an online form.)\nThis document has been adapted from Scientific Teaching by Jo Handelsman, Sarah Miller, and Christine Pfund\n\n\n\n\nScientific Teaching"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#pep-talk",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#pep-talk",
    "title": "1: Conditional Probability",
    "section": "Pep Talk",
    "text": "Pep Talk\nLearning R can be difficult at first—it is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you will be using like ggplot2—made this wise observation:\n\n\n\n\n\n\nWisdom from Hadley Wickham\n\n\n\n\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\n\n\nIf you are finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, ask questions … e-mail [Derek], etc. I promise you can do this.\n—Andrew Heiss, Georgia State University"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#inclusion-statement",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#inclusion-statement",
    "title": "1: Conditional Probability",
    "section": "Inclusion Statement",
    "text": "Inclusion Statement\nI value all students regardless of their background, country of origin, race, religion, ethnicity, gender, sexual orientation, disability status, etc. and am committed to providing a climate of excellence and inclusiveness within all aspects of the course. If there are aspects of your culture or identity that you would like to share with me as they relate to your success in this class, I am happy to meet to discuss. Likewise, if you have any concerns in this area or facing any special issues or challenges, you are encouraged to discuss the matter with me (set up a meeting by e-mail) with an assurance of full confidentiality (only exception being mandatory reporting of academic integrity code violations or sexual harassment)."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#setting",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#setting",
    "title": "1: Conditional Probability",
    "section": "Setting",
    "text": "Setting\n\n\nLet us visit the lands of Faerûn. To grossly simplify and introduce notions from Dungeons and Dragons, let us define the following random variables:\n\n\\(H\\): human\n\\(E\\): evil\n\nso that \\(H^{c}\\) is “non-human” and \\(E^c\\) is “not evil”.\n\n\n\n\nBaldur’s Gate 3\n\n\n\n\n\n\n\n\n\n\nUnicode Characters\n\n\n\n\n\nDerek wanted to make a note to himself here that the way to make accented letters in a markdown environment is to use unicode characters."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nContingency TableCode\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(E^{c}|H)\\) and \\(P(H^{c}|E)\\)\n\n\n\nalignment &lt;- c(\"evil\", \"not evil\")\nhuman &lt;- c(\"24\", \"30\")\nnon_human &lt;- c(\"88\", \"178\")\n\nbg3_df &lt;- data.frame(alignment, human, non_human)\n\nbg3_gt_table &lt;- bg3_df |&gt;\n  gt(rowname_col = \"alignment\") |&gt;\n  cols_align(align = \"right\", columns = alignment) |&gt;\n  cols_align(align = \"center\",  columns = c(human, non_human)) |&gt;\n  tab_style(locations = cells_body(columns = c(human, non_human)),\n            style = list(cell_fill(color = \"yellow\")))\n\nbg3_gt_table #display table"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#practice",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#practice",
    "title": "1: Conditional Probability",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(H|E)\\) and \\(P(E|H)\\). What do you observe about the results?"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#setting-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#setting-1",
    "title": "1: Conditional Probability",
    "section": "Setting",
    "text": "Setting\n\n\nDuring the Winter of 2024, Kaggle had a competition where programmers were asked to “create an energy prediction model of prosumers to reduce energy imbalance costs” based on data that included property information, historical weather, and forecasted weather.\nFor now, let us pretend to classify the results into “high energy” and “low energy” usage, where “positive” results correspond to the “high energy” prosumers.\n\n\nThis Kaggle competition was called “Enefit”, and it was created by Eesti Energia to model Estonian energy customers."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-1",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nConfusion MatrixCodeMetricsFormulas\n\n\nSuppose that a team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\ntrue positives: 59\ntrue negatives: 97\nfalse positives: 14\nfalse negatives: 28\n\n\n\n\nenergy_levels &lt;- c(\"high\", \"low\")\nhigh &lt;- c(59, 14)\nlow &lt;- c(28, 97)\n\nenefit_df &lt;- data.frame(energy_levels, high, low)\n\nenefit_gt &lt;- enefit_df |&gt;\n  gt(rowname_col = \"energy_levels\") |&gt;\n  cols_align(align = \"right\", columns = energy_levels) |&gt;\n  cols_align(align = \"center\",  columns = c(high, low)) |&gt;\n  tab_spanner(columns = c(high, low),\n              label = \"model predictions\") |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 1),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 2),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 2),\n            style = list(cell_fill(color = \"#FFB3B2\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 1),\n            style = list(cell_fill(color = \"#FFB3B2\")))\n\nenefit_gt #display table\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\n\n\\[\\text{accuracy } = \\frac{TP + TN}{TP + FN + FP + TN}\\] \\[\\text{sensitivity } = \\frac{TP}{TP + FN}\\] \\[\\text{specificity } = \\frac{TN}{FP + TN}\\] \\[\\text{F-score } = \\frac{2*TP}{2*TP + FN + FP}\\]\nSource: Wikipedia page on sensitivity and specificity\n\n\n\n\n\n\n\n\n\nRow Span\n\n\n\n\n\nWhen these lecture notes were written, there might not have been a function to have a label span multiple rows in the gt package. The left side of the confusion matrix should say “ground truth”"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#practice-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#practice-1",
    "title": "1: Conditional Probability",
    "section": "Practice",
    "text": "Practice\nSuppose that another team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n94\n80\n    low\n23\n939\n  \n  \n  \n\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\nProsecutor’s Fallacy\n\n\n\n\n\nFor events \\(A\\) and \\(B\\), the inverse conditional probabilities are almost never equal to each other\n\\[P(A|B) \\neq P(B|A)\\]\n\n\n\n\n\n\n\n\n\n(optional) Additional Resources\n\n\n\n\n\n\nClassical vs Frequentist vs Bayesian Probability by Trefor Bazett"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#classical-probability",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#classical-probability",
    "title": "1: Conditional Probability",
    "section": "Classical Probability",
    "text": "Classical Probability\n\nDefinitionExample\n\n\n\\[P(A) = \\frac{\\text{number of outcomes that are } A}{\\text{number of outcomes total}}\\]\n\n\nOverly simplistic example\nIt snowed during 2 of the 30 days of January in 2024. We can claim that it snows \\(\\frac{2}{30}\\) of the days in Princeton."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#frequentist-probablity",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#frequentist-probablity",
    "title": "1: Conditional Probability",
    "section": "Frequentist Probablity",
    "text": "Frequentist Probablity\n\nDefinitionExample\n\n\n\\[P(A) = \\lim_{n \\to \\infty} \\frac{\\text{number of outcomes that are } A}{\\text{number of outcomes total}}\\]\n\n\nOverly simplistic example\nIf we can model many months of weather in Princeton, we expect it to snow in about 3 percent of the days."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-probability",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-probability",
    "title": "1: Conditional Probability",
    "section": "Bayesian Probability",
    "text": "Bayesian Probability\n\nDefinitionExample\n\n\n\nprior belief: \\(P(A)\\)\nupdated belief: \\(P(A|B)\\)\n\n\n\nOverly simplistic example\nIt snowed during 2 of the 30 days of January in 2024. Could we update our probability calculations for snow?"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-3",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-3",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nContingency TableCode\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(E^{c}|H)\\) and \\(P(H^{c}|E)\\)\n\n\n\nalignment &lt;- c(\"evil\", \"not evil\")\nhuman &lt;- c(\"24\", \"30\")\nnon_human &lt;- c(\"88\", \"178\")\n\nbg3_df &lt;- data.frame(alignment, human, non_human)\n\nbg3_gt_table &lt;- bg3_df |&gt;\n  gt(rowname_col = \"alignment\") |&gt;\n  cols_align(align = \"right\", columns = alignment) |&gt;\n  cols_align(align = \"center\",  columns = c(human, non_human)) |&gt;\n  tab_style(locations = cells_body(columns = c(human, non_human)),\n            style = list(cell_fill(color = \"yellow\")))\n\nbg3_gt_table #display table"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-4",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-4",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nConfusion MatrixCodeMetricsFormulas\n\n\nSuppose that a team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\ntrue positives: 59\ntrue negatives: 97\nfalse positives: 14\nfalse negatives: 28\n\n\n\n\nenergy_levels &lt;- c(\"high\", \"low\")\nhigh &lt;- c(59, 14)\nlow &lt;- c(28, 97)\n\nenefit_df &lt;- data.frame(energy_levels, high, low)\n\nenefit_gt &lt;- enefit_df |&gt;\n  gt(rowname_col = \"energy_levels\") |&gt;\n  cols_align(align = \"right\", columns = energy_levels) |&gt;\n  cols_align(align = \"center\",  columns = c(high, low)) |&gt;\n  tab_spanner(columns = c(high, low),\n              label = \"model predictions\") |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 1),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 2),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 2),\n            style = list(cell_fill(color = \"#FFB3B2\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 1),\n            style = list(cell_fill(color = \"#FFB3B2\")))\n\nenefit_gt #display table\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\n\n\\[\\text{accuracy } = \\frac{TP + TN}{TP + FN + FP + TN}\\] \\[\\text{sensitivity } = \\frac{TP}{TP + FN}\\] \\[\\text{specificity } = \\frac{TN}{FP + TN}\\] \\[\\text{F-score } = \\frac{2*TP}{2*TP + FN + FP}\\]\nSource: Wikipedia page on sensitivity and specificity\n\n\n\n\n\n\n\n\n\nRow Span\n\n\n\n\n\nWhen these lecture notes were written, there might not have been a function to have a label span multiple rows in the gt package. The left side of the confusion matrix should say “ground truth”"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html",
    "href": "posts/03_beta_binomial/03_beta_binomial.html",
    "title": "3: Beta-Binomial Models",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"gt\")\nlibrary(\"janitor\")\nlibrary(\"patchwork\")\nlibrary(\"skimr\")\nlibrary(\"tidyverse\")\n\ntips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#tips-data-set",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#tips-data-set",
    "title": "3: Beta-Binomial Models",
    "section": "Tips Data Set",
    "text": "Tips Data Set\n\nDescriptionGlanceStructureSkim\n\n\n\nsource: Kaggle\n\n“The data was reported in a collection of case studies for business statistics. Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing\n\ncontext: “One waiter recorded information about each tip he received over a period of a few months working in one restaurant. In all he recorded 244 tips.”\n\n\n\n\nhead(tips_df)\n\n# A tibble: 6 × 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n3       21.0  3.5  Male   No     Sun   Dinner     3\n4       23.7  3.31 Male   No     Sun   Dinner     2\n5       24.6  3.61 Female No     Sun   Dinner     4\n6       25.3  4.71 Male   No     Sun   Dinner     4\n\n\n\n\n\nstr(tips_df, give.attr = FALSE)\n\nspc_tbl_ [244 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ total_bill: num [1:244] 17 10.3 21 23.7 24.6 ...\n $ tip       : num [1:244] 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ...\n $ sex       : chr [1:244] \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ smoker    : chr [1:244] \"No\" \"No\" \"No\" \"No\" ...\n $ day       : chr [1:244] \"Sun\" \"Sun\" \"Sun\" \"Sun\" ...\n $ time      : chr [1:244] \"Dinner\" \"Dinner\" \"Dinner\" \"Dinner\" ...\n $ size      : num [1:244] 2 3 3 2 4 4 2 4 2 2 ...\n\n\n\n\n\nskimr::skim(tips_df)\n\n\nData summary\n\n\nName\ntips_df\n\n\nNumber of rows\n244\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\nsmoker\n0\n1\n2\n3\n0\n2\n0\n\n\nday\n0\n1\n3\n4\n0\n4\n0\n\n\ntime\n0\n1\n5\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntotal_bill\n0\n1\n19.79\n8.90\n3.07\n13.35\n17.8\n24.13\n50.81\n▃▇▃▁▁\n\n\ntip\n0\n1\n3.00\n1.38\n1.00\n2.00\n2.9\n3.56\n10.00\n▇▆▂▁▁\n\n\nsize\n0\n1\n2.57\n0.95\n1.00\n2.00\n2.0\n3.00\n6.00\n▇▂▂▁▁"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#prior-model",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#prior-model",
    "title": "3: Beta-Binomial Models",
    "section": "Prior Model",
    "text": "Prior Model\n\n\n\n\\(\\pi\\)\n0.25\n0.50\n0.75\ntotal\n\n\n\n\n\\(f(\\pi)\\)\n1/3\n1/3\n1/3\n1\n\n\n\n\nuniform prior\ne.g. guessing the probability that the percentage of customers that smoked was 75% was \\(\\frac{1}{3}\\)\n\n\n\n\n\n\n\nDiscrete Probability Model\n\n\n\n\n\nLet \\(Y\\) be a discrete random variable. The probability model of \\(Y\\) is specified by a probability mass function (pmf) \\(f(y)\\). This pmf defines the probability of any given outcome \\(y\\),\n\\[f(y) = P(Y = y)\\]\n\n\\(0 \\leq f(y) \\leq 1\\)\n\\(\\sum f(y) = 1\\)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#observed-data",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#observed-data",
    "title": "3: Beta-Binomial Models",
    "section": "Observed Data",
    "text": "Observed Data\n\nObserved SampleCode\n\n\nLooking at the last 9 observations in the data set, 4 of the customers were smokers.\n\n\n\n\n\n\n  \n    \n    \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    10.07\n1.25\nMale\nNo\nSat\nDinner\n2\n    12.60\n1.00\nMale\nYes\nSat\nDinner\n2\n    32.83\n1.17\nMale\nYes\nSat\nDinner\n2\n    35.83\n4.67\nFemale\nNo\nSat\nDinner\n3\n    29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n    27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n    22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n    17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n    18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n  \n  \n  \n\n\n\n\n\n\n\ntail(tips_df, 9) |&gt;\n  gt() |&gt;\n  tab_style(locations = cells_body(columns = smoker),\n            style = list(cell_fill(color = \"gray80\"))) |&gt;\n  tab_style(locations = cells_body(columns = smoker,\n                                   rows = smoker == \"Yes\"),\n            style = list(cell_text(color = \"red\")))"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#binomial-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#binomial-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\n\nLet random variable \\(Y\\) be the number of successes in a fixed number of trials \\(n\\). Assume that the trials are independent and that the probability of success in each trial is \\(\\pi\\). Then the conditional dependence of \\(Y\\) on \\(\\pi\\) can be modeled by the Binomial model with parameters \\(n\\) and \\(\\pi\\). In mathematical notation:\n\\[Y|\\pi \\sim \\text{Bin}(n,\\pi)\\] where \\(\\sim\\) can be read as “modeled by”. Correspondingly, the binomial model is specified by the conditional pmf\n\\[f(y|\\pi) = \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y} \\text{ for } y \\in \\{0, 1, 2, ..., n\\}\\] where \\(\\binom{n}{y} = \\displaystyle\\frac{n!}{y!(n-y)!}\\)\n\n\n\nIn this example of \\(Y\\) smokers in \\(n=9\\) customers with probability \\(\\pi\\) of smokers,\n\\[Y|\\pi \\sim \\text{Bin}(9,\\pi)\\] \\[f(y|\\pi) = \\binom{9}{y}\\pi^{y}(1-\\pi)^{9-y} \\text{ for } y \\in \\{0, 1, 2, ..., 9\\}\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#conditional-pmfs",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#conditional-pmfs",
    "title": "3: Beta-Binomial Models",
    "section": "Conditional PMFs",
    "text": "Conditional PMFs\n\nBased on Observed DataCode\n\n\n\n\n\n\n\n\n\n\nhighlight_col &lt;- 0:9 == 4\ndf_25 &lt;- data.frame(k = 0:9, f_y_pi = dbinom(0:9, 9, 0.25), highlight_col)\ndf_50 &lt;- data.frame(k = 0:9, f_y_pi = dbinom(0:9, 9, 0.50), highlight_col)\ndf_75 &lt;- data.frame(k = 0:9, f_y_pi = dbinom(0:9, 9, 0.75), highlight_col)\n\nplot_25 &lt;- df_25 |&gt;\n  ggplot(aes(x = k, y = f_y_pi, fill = highlight_col)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Bin(9,0.25)\") +\n  scale_x_continuous(name = \"customers\", \n                   breaks = 0:9, \n                   labels = as.character(0:9)) +\n  theme_minimal()\n\nplot_50 &lt;- df_50 |&gt;\n  ggplot(aes(x = k, y = f_y_pi, fill = highlight_col)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Bin(9,0.50)\") +\n  scale_x_continuous(name = \"customers\", \n                   breaks = 0:9, \n                   labels = as.character(0:9)) +\n  theme_minimal()\n\nplot_75 &lt;- df_75 |&gt;\n  ggplot(aes(x = k, y = f_y_pi, fill = highlight_col)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Bin(9,0.75)\") +\n  scale_x_continuous(name = \"customers\", \n                   breaks = 0:9, \n                   labels = as.character(0:9)) +\n  theme_minimal()\n\n# patchwork\nplot_25 + plot_50 + plot_75"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#likelihoods",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#likelihoods",
    "title": "3: Beta-Binomial Models",
    "section": "Likelihoods",
    "text": "Likelihoods\nWith the observed data \\(Y = 4\\) out of \\(n = 9\\) customers, for \\(\\pi = \\{0.25, 0.50, 0.75\\}\\), \\[L(\\pi|y = 4) = f(y = 4|\\pi) = \\binom{9}{4}\\pi^{4}(1-\\pi)^{5}\\]\n\\[L(\\pi = 0.25|y = 4) = \\binom{9}{4}(0.25)^{4}(1-0.25)^{5} \\approx 0.1168\\] \\[L(\\pi = 0.50|y = 4) = \\binom{9}{4}(0.50)^{4}(1-0.50)^{5} \\approx 0.2461\\] \\[L(\\pi = 0.75|y = 4) = \\binom{9}{4}(0.75)^{4}(1-0.75)^{5} \\approx 0.0389\\]\n\n\n\n\\(\\pi\\)\n0.25\n0.50\n0.75\ntotal\n\n\n\n\n\\(f(\\pi)\\)\n1/3\n1/3\n1/3\n1\n\n\n\\(L(\\pi|y=4)\\)\n0.1168\n0.2461\n0.0389\n0.4018"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#bayesian-concepts",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#bayesian-concepts",
    "title": "3: Beta-Binomial Models",
    "section": "Bayesian Concepts",
    "text": "Bayesian Concepts\n\\[\\text{posterior} = \\frac{\\text{prior} * \\text{likelihood}}{\\text{normalizing constant}}\\]\nFor observations \\(\\vec{y}\\) and probabilities \\(\\vec{\\pi}\\),\n\\[f(\\pi|y) = \\frac{f(\\pi)L(\\pi|y)}{f(y)} \\propto f(\\pi)L(\\pi|y)\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#normalizing-constant",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#normalizing-constant",
    "title": "3: Beta-Binomial Models",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\\[f(y = 4) = \\displaystyle\\sum_{\\pi\\in\\{0.25, 0.50, 0.75\\}} L(\\pi|y=4) \\cdot f(\\pi)\\]\n\\[f(y = 4) = \\displaystyle\\frac{0.1168}{3} + \\displaystyle\\frac{0.2461}{3} + \\displaystyle\\frac{0.0389}{3} \\approx 0.1339\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#posterior-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#posterior-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\\[f(\\pi|y=4) = \\displaystyle\\frac{f(\\pi)L(\\pi|y=4)}{f(y=4)} \\text{ for } \\pi \\in \\{0.25, 0.50, 0.75 \\}\\] \\[f(\\pi=0.25|y=4) = \\displaystyle\\frac{(1/3)(0.1168)}{0.1339} \\approx 0.2907\\] \\[f(\\pi=0.50|y=4) = \\displaystyle\\frac{(1/3)(0.2461)}{0.1339} \\approx 0.6126\\] \\[f(\\pi=0.75|y=4) = \\displaystyle\\frac{(1/3)(0.0389)}{0.1339} \\approx 0.0968\\]\n\n\n\n\\(\\pi\\)\n0.25\n0.50\n0.75\ntotal\n\n\n\n\n\\(f(\\pi)\\)\n1/3\n1/3\n1/3\n1\n\n\n\\(L(\\pi|y=4)\\)\n0.1168\n0.2461\n0.0389\n0.4018\n\n\n\\(f(\\pi|y=4)\\)\n0.2907\n0.6126\n0.0968\n1"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#computer-simulation",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#computer-simulation",
    "title": "3: Beta-Binomial Models",
    "section": "Computer Simulation",
    "text": "Computer Simulation\n\nSimulation SamplesVerify PriorPMFsPosterior Distribution\n\n\n\n# define possible smoker proportions\nsmokers &lt;- data.frame(pi = c(0.25, 0.50, 0.75))\n\n# define prior model\nprior &lt;- c(1/3, 1/3, 1/3)\n\n# simulate 10000 values of pi from the prior\nset.seed(320)\nsmoker_sim &lt;- sample_n(smokers, size = 10000, weight = prior, replace = TRUE)\n\n# simulate 10000 samples of customers\nsmoker_sim &lt;- smoker_sim |&gt;\n  mutate(y = rbinom(10000, size = 9, prob = pi))\n\nSo far, the simulation yields a data frame that looks like\n\nhead(smoker_sim)\n\n    pi y\n1 0.50 5\n2 0.25 2\n3 0.50 6\n4 0.25 3\n5 0.50 4\n6 0.50 4\n\n\n\n\n\n# summarize the prior\nsmoker_sim |&gt;\n  tabyl(pi) |&gt;\n  adorn_totals(\"row\")\n\n    pi     n percent\n  0.25  3283  0.3283\n   0.5  3345  0.3345\n  0.75  3372  0.3372\n Total 10000  1.0000\n\n\n\n\n\n# plot y by pi\nggplot(smoker_sim, aes(x = y)) + \n  stat_count(aes(y = after_stat(prop))) + \n  facet_wrap(~ pi)\n\n\n\n\n\n\n\n# focus on simulations with y = 4\nfour_smokers &lt;- smoker_sim %&gt;% \n  filter(y == 4)\n\n# summarize the posterior approximation\nfour_smokers %&gt;% \n  tabyl(pi) %&gt;% \n  adorn_totals(\"row\")\n\n    pi    n   percent\n  0.25  408 0.3000000\n   0.5  813 0.5977941\n  0.75  139 0.1022059\n Total 1360 1.0000000\n\n\n\n# plot the posterior approximation\nggplot(four_smokers, aes(x = pi)) + \n  geom_bar()"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#beta-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#beta-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Beta Distribution",
    "text": "Beta Distribution\n\n\n\n\n\n\nBeta Distribution\n\n\n\n\n\nLet \\(\\pi \\in [0,1]\\), then the variability in \\(\\pi\\) may be modeled by a Beta distribution with shape hyperparameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\)\n\\[\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\] with probability density function\n\\[f(\\pi) = \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\]\nwhere the gamma function\n\n\\(\\Gamma(z) = \\displaystyle\\int_{0}^{\\infty} \\! x^{z-1}e^{-x} \\, dx\\)\n\\(\\Gamma(z + 1) = z\\Gamma(z)\\)\n\n\n\n\n\n\n\n\n\n\nCorollary\n\n\n\n\n\nWhen \\(z\\) is a positive integer, then \\[\\Gamma(z) = (z-1)!\\] That is, the gamma function is a generalization of the factorial.\n\n\n\n\n\n\n\n\n\nHyperparameters\n\n\n\n\n\nA hyperparameter is a parameter used in a prior model.\n\n\n\n\n\n\n\n\n\nExplore!\n\n\n\n\n\nMatt Bognar at the University of Iowa created this great webapp to explore the beta distribution.\n\n\n\n\n\n\n\n\n\nUniform Distribution\n\n\n\n\n\nWhen it is equally plausible for \\(\\pi\\) to take on any value between zero and one, we can model \\(\\pi\\) by the standard uniform distribution\n\\[\\pi \\sim \\text{Unif}(0,1)\\]\nwith pdf \\(f(\\pi) = 1\\) for \\(\\pi \\in [0,1]\\). The \\(\\text{Unif}(0,1)\\) distribution is a special case of the beta distribution when \\(\\alpha = 1\\) and \\(\\beta = 1\\)\n\\[\\text{Unif}(0,1) = \\text{Beta}(1,1)\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#sample-statistics",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#sample-statistics",
    "title": "3: Beta-Binomial Models",
    "section": "Sample Statistics",
    "text": "Sample Statistics\nFor a beta distribution, \\(\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\)\n\nexpected value: \\(\\text{E}(\\pi) = \\displaystyle\\frac{\\alpha}{\\alpha + \\beta}\\)\nvariance: \\(\\text{Var}(\\pi) = \\displaystyle\\frac{\\alpha\\beta}{(\\alpha + \\beta)^{2}(\\alpha + \\beta + 1)}\\)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#tuning-the-beta-prior",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#tuning-the-beta-prior",
    "title": "3: Beta-Binomial Models",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\nHere, let us use that sample of observations where 4 out of the 9 customers where smokers. We might then try to align this sample proportion \\(\\frac{4}{9}\\) with the expected value\n\\[\\displaystyle\\frac{\\alpha}{\\alpha + \\beta} = \\displaystyle\\frac{4}{9} \\quad\\rightarrow\\quad \\alpha = 4, \\quad \\beta = 5\\]\nto create a beta model \\(\\pi \\sim \\text{Beta}(4, 5)\\)\n\nbayesrules::plot_beta(4,5)\n\n\n\n\nWe can compute the variance\n\\[\\text{Var}(\\pi) = \\displaystyle\\frac{\\alpha\\beta}{(\\alpha + \\beta)^{2}(\\alpha + \\beta + 1)} = \\displaystyle\\frac{4 \\cdot 5}{(4 + 5)^{2}(4 + 5 + 1)} \\approx 0.0247\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#binomial-data-model",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#binomial-data-model",
    "title": "3: Beta-Binomial Models",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\nSuppose that we obtain a larger sample of observations with \\(n = 25\\) customers. The number of smokers, denoted by random variable \\(Y\\), may have a binomial model conditional on probability \\(\\pi\\),\n\\[Y|\\pi \\sim \\text{Bin}(25, \\pi)\\]\nwith conditional pmf over \\(y \\in \\{0, 1, ..., 25\\}\\), \\[f(y|\\pi) = P(Y = y|\\pi) = \\binom{25}{y}\\pi^{y}(1-\\pi)^{25-y}\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#likelihood",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#likelihood",
    "title": "3: Beta-Binomial Models",
    "section": "Likelihood",
    "text": "Likelihood\n\nFunctionPlotCode\n\n\nSuppose that in that sample of \\(n = 25\\) customers, we observe that \\(y = 7\\) of those customers were smokers. Our likelihood function is then\n\\[L(\\pi|y = 7) = \\binom{25}{7}\\pi^{7}(1-\\pi)^{18}\\]\n\n\n\n\n\n\n\n\n\n\npi &lt;- seq(0, 1, 0.01)\nL_pi_y &lt;- dbinom(7, 25, pi)\n\ndf_for_graph &lt;- data.frame(pi, L_pi_y)\n\ndf_for_graph |&gt;\n  ggplot(aes(x = pi, y = L_pi_y)) +\n  geom_line() +\n  labs(title = \"Likelihood function\",\n       subtitle = \"y = 7, n = 25\",\n       caption = \"SML 320\")"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#beta-binomial-model",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#beta-binomial-model",
    "title": "3: Beta-Binomial Models",
    "section": "Beta-Binomial Model",
    "text": "Beta-Binomial Model\nClaim: With probability \\(\\pi \\in [0,1]\\) and random variable \\(Y\\) representing the number of “successes” in \\(n\\) trials, if the behavior is modeled with prior distribution and likelihood\n\\[\\begin{array}{rcl}\n  \\pi & \\sim & \\text{Beta}(\\alpha, \\beta) \\\\\n  Y|\\pi & \\sim & \\text{Bin}(n,\\pi) \\\\\n\\end{array}\\]\nthen the posterior distribution can be modeled with an updated beta distribution\n\\[\\pi|(Y=y) \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\]\nwith sample statistics\n\\[\\begin{array}{rcl}\n  \\text{E}(\\pi|Y=y) & = & \\displaystyle\\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\n  \\text{Var}(\\pi|Y=y) & = & \\displaystyle\\frac{(\\alpha  +y)(\\beta + n - y)}{(\\alpha + \\beta + n)^{2}(\\alpha + \\beta + n + 1)} \\\\\n\\end{array}\\]\n\n\n\n\n\n\nPartial Proof\n\n\n\n\n\nWith the conditional pmf\n\\[f(\\pi) = \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\] and likelihood function\n\\[L(\\pi|y) = \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y}\\]\nit follows from Bayes’ Rule that the posterior distribution\n\\[\\begin{array}{rcl}\n  f(\\pi|y) & \\propto & f(\\pi)L(\\pi|y) \\\\\n  ~ & = & \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\cdot \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y} \\\\\n  ~ & \\propto & \\pi^{(\\alpha + y)-1}(1-\\pi)^{(\\beta+n-y)-1} \\\\\n\\end{array}\\]\nwhere that last expression is the unnormalized posterior pdf. We observe that it has the same structure of the normalized \\(\\text{Beta}(\\alpha + y, \\beta + n - y)\\) pdf\n\\[f(\\pi|y) = \\displaystyle\\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+y)\\Gamma(\\beta+n-y)} \\pi^{(\\alpha + y)-1}(1-\\pi)^{(\\beta+n-y)-1}\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#beta-posterior",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#beta-posterior",
    "title": "3: Beta-Binomial Models",
    "section": "Beta Posterior",
    "text": "Beta Posterior\n\nUpdated DistributionPlotBoth\n\n\nBy the above theory, having started with a \\(\\text{Beta}{(4,5)}\\) prior, and then observing \\(y = 7\\) smokers among \\(n = 25\\) customers\n\\[\\alpha = 4, \\quad \\beta = 5, \\quad y = 7, \\quad n = 25\\]\nour posterior distribution can be modeled with\n\\[\\pi|(Y=y) \\sim \\text{Beta}(\\alpha + y, \\beta + n - y) = \\text{Beta}(11, 23)\\]\n\n\n\nbayesrules::plot_beta(11,23)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#putting-it-all-together",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#putting-it-all-together",
    "title": "3: Beta-Binomial Models",
    "section": "Putting it All Together",
    "text": "Putting it All Together\n\nHelper FunctionsTablePlot\n\n\nThe bayesrules package (from the textbook authors) provide additonal helper functions for this procedure of modeling with a beta-binomial model.\nbayesrules::summarize_beta_binomial(alpha, beta, y, n)\nbayesrules::plot_beta_binomial(alpha, beta, y, n)\n\n\n\nsummarize_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    5 0.4444 0.4286 0.0247 0.1571\n2 posterior    11   23 0.3235 0.3125 0.0063 0.0791\n\n\n\n\n\nplot_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html",
    "title": "3: Beta-Binomial Models",
    "section": "",
    "text": "Today, let’s see if the following R code runs on your computer.\nRemember to install packages as needed.\nlibrary(\"bayesrules\")\nlibrary(\"gt\")\nlibrary(\"janitor\")\nlibrary(\"patchwork\")\nlibrary(\"skimr\")\nlibrary(\"tidyverse\")\nAlso, place the tips.csv file in the same directory as this script.\ntips_df &lt;- readr::read_csv(\"tips.csv\")\n\nRows: 244 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): sex, smoker, day, time\ndbl (3): total_bill, tip, size\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#simulation-samples",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#simulation-samples",
    "title": "3: Beta-Binomial Models",
    "section": "Simulation Samples",
    "text": "Simulation Samples\n\n# define possible smoker proportions\nsmokers &lt;- data.frame(pi = c(0.25, 0.50, 0.75))\n\n# define prior model\nprior &lt;- c(1/3, 1/3, 1/3)\n\n# simulate 10000 values of pi from the prior\nset.seed(320)\nsmoker_sim &lt;- sample_n(smokers, size = 10000, weight = prior, replace = TRUE)\n\n# simulate 10000 samples of customers\nsmoker_sim &lt;- smoker_sim |&gt;\n  mutate(y = rbinom(10000, size = 9, prob = pi))"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#pmfs",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#pmfs",
    "title": "3: Beta-Binomial Models",
    "section": "PMFs",
    "text": "PMFs\n\n# plot y by pi\nggplot(smoker_sim, aes(x = y)) + \n  stat_count(aes(y = after_stat(prop))) + \n  facet_wrap(~ pi)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#posterior-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#posterior-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\n# focus on simulations with y = 4\nfour_smokers &lt;- smoker_sim %&gt;% \n  filter(y == 4)\n\n# summarize the posterior approximation\nfour_smokers %&gt;% \n  tabyl(pi) %&gt;% \n  adorn_totals(\"row\")\n\n    pi    n   percent\n  0.25  408 0.3000000\n   0.5  813 0.5977941\n  0.75  139 0.1022059\n Total 1360 1.0000000\n\n\n\n# plot the posterior approximation\nggplot(four_smokers, aes(x = pi)) + \n  geom_bar()"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#table",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#table",
    "title": "3: Beta-Binomial Models",
    "section": "Table",
    "text": "Table\n\nsummarize_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    5 0.4444 0.4286 0.0247 0.1571\n2 posterior    11   23 0.3235 0.3125 0.0063 0.0791"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#plot",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#plot",
    "title": "3: Beta-Binomial Models",
    "section": "Plot",
    "text": "Plot\n\nplot_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html",
    "title": "4: Balance and Sequentiality",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\ntips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#journey-so-far",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#journey-so-far",
    "title": "4: Balance and Sequentiality",
    "section": "Journey so far",
    "text": "Journey so far\nWith hopes of learning more about a target probability \\[\\pi \\in [0,1]\\] we have been applying beta-binomial models\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Beta}(\\alpha, \\beta) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Bin}(y, \\pi) \\\\\n  \\text{posterior: } & \\pi|(Y = y) & \\sim & \\text{Beta}(\\alpha + y, \\beta + n - y) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta",
    "title": "4: Balance and Sequentiality",
    "section": "Using plot_beta",
    "text": "Using plot_beta\n\nInclinationsCode\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesrules::plot_beta(4,8) +\n  labs(title = \"Beta(4,8), E(pi) = 1/3\")\np2 &lt;- bayesrules::plot_beta(6,6) +\n  labs(title = \"Beta(6,6), E(pi) = 1/2\")\np3 &lt;- bayesrules::plot_beta(8,4) +\n  labs(title = \"Beta(8,4), E(pi) = 2/3\")\n\n# patchwork\np1 / p2 / p3"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-thursday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-thursday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Thursday Customers",
    "text": "Subset: Thursday Customers\n\nThursday &lt;- tips_df |&gt;\n  filter(day == \"Thur\")\n\nn_Thursday &lt;- nrow(Thursday)\ny_Thursday &lt;- Thursday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta_binomial",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta_binomial",
    "title": "4: Balance and Sequentiality",
    "section": "Using plot_beta_binomial",
    "text": "Using plot_beta_binomial\n\nCodePlotsStatistics\n\n\n\np1 &lt;- bayesrules::plot_beta_binomial(4, 8, y_Thursday, n_Thursday) +\n  labs(title = \"Beta(4,8) prior\")\np2 &lt;- bayesrules::plot_beta_binomial(6, 6, y_Thursday, n_Thursday) +\n  labs(title = \"Beta(6,6) prior\")\np3 &lt;- bayesrules::plot_beta_binomial(8, 4, y_Thursday, n_Thursday) +\n  labs(title = \"Beta(8,4) prior\")\n\n# patchwork\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_beta_binomial(4, 8, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    8 0.3333 0.3000 0.0171 0.1307\n2 posterior    21   53 0.2838 0.2778 0.0027 0.0521\n\n\n\nbayesrules::summarize_beta_binomial(6, 6, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     6    6 0.5000 0.5000 0.0192 0.1387\n2 posterior    23   51 0.3108 0.3056 0.0029 0.0534\n\n\n\nbayesrules::summarize_beta_binomial(8, 4, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     8    4 0.6667 0.7000 0.0171 0.1307\n2 posterior    25   49 0.3378 0.3333 0.0030 0.0546"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#certainty",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#certainty",
    "title": "4: Balance and Sequentiality",
    "section": "Certainty",
    "text": "Certainty\n\nInclinationsCode\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesrules::plot_beta(1,1) +\n  labs(title = \"Uniform Prior: Beta(1,1), var(pi) = 0.0833\")\np2 &lt;- bayesrules::plot_beta(4,4) +\n  labs(title = \"Vague Prior: Beta(4,4), var(pi) = 0.0278\")\np3 &lt;- bayesrules::plot_beta(16,16) +\n  labs(title = \"Informative Prior: Beta(16,16), var(pi) = 0.0076\")\n\n# patchwork\np1 / p2 / p3"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-friday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-friday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Friday Customers",
    "text": "Subset: Friday Customers\n\nFriday &lt;- tips_df |&gt;\n  filter(day == \"Fri\")\n\nn_Friday &lt;- nrow(Friday)\ny_Friday &lt;- Friday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#applying-the-friday-crowd",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#applying-the-friday-crowd",
    "title": "4: Balance and Sequentiality",
    "section": "Applying the Friday Crowd",
    "text": "Applying the Friday Crowd\n\nCodePlotsStatistics\n\n\n\np1 &lt;- bayesrules::plot_beta_binomial(1, 1, y_Friday, n_Friday) +\n  labs(title = \"Uniform Prior: Beta(1,1)\")\np2 &lt;- bayesrules::plot_beta_binomial(4, 4, y_Friday, n_Friday) +\n  labs(title = \"Vague Prior: Beta(4,4)\")\np3 &lt;- bayesrules::plot_beta_binomial(16, 16, y_Friday, n_Friday) +\n  labs(title = \"Informative Prior: Beta(16,16)\")\n\n# patchwork\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_beta_binomial(1, 1, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1    1 0.5000    NaN 0.0833 0.2887\n2 posterior    16    5 0.7619 0.7895 0.0082 0.0908\n\n\n\nbayesrules::summarize_beta_binomial(4, 4, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean mode    var     sd\n1     prior     4    4 0.5000 0.50 0.0278 0.1667\n2 posterior    19    8 0.7037 0.72 0.0074 0.0863\n\n\n\nbayesrules::summarize_beta_binomial(16, 16, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    16   16 0.5000 0.5000 0.0076 0.0870\n2 posterior    31   20 0.6078 0.6122 0.0046 0.0677"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#skewed-prior",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#skewed-prior",
    "title": "4: Balance and Sequentiality",
    "section": "Skewed Prior",
    "text": "Skewed Prior\n\nFixed PriorCode\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_beta(1,32) +\n  labs(title = \"Skewed Prior: Beta(1,32), var(pi) = 0.0009\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-saturday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-saturday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Saturday Customers",
    "text": "Subset: Saturday Customers\n\nSaturday &lt;- tips_df |&gt;\n  filter(day == \"Sat\")\n\nn_Saturday &lt;- nrow(Saturday)\ny_Saturday &lt;- Saturday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#friday-versus-saturday",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#friday-versus-saturday",
    "title": "4: Balance and Sequentiality",
    "section": "Friday versus Saturday",
    "text": "Friday versus Saturday\n\nCodePlotsStatistics\n\n\n\np1 &lt;- bayesrules::plot_beta_binomial(1, 32, y_Friday, n_Friday) +\n  labs(title = \"Friday Customers\")\np2 &lt;- bayesrules::plot_beta_binomial(1, 32, y_Saturday, n_Saturday) +\n  labs(title = \"Saturday Customers\")\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_beta_binomial(1, 32, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean mode   var     sd\n1     prior     1   32 0.0303  0.0 9e-04 0.0294\n2 posterior    16   36 0.3077  0.3 4e-03 0.0634\n\n\n\nbayesrules::summarize_beta_binomial(1, 32, y_Saturday, n_Saturday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1   32 0.0303 0.0000 0.0009 0.0294\n2 posterior    43   77 0.3583 0.3559 0.0019 0.0436"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#vague-prior-revisited",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#vague-prior-revisited",
    "title": "4: Balance and Sequentiality",
    "section": "Vague Prior Revisited",
    "text": "Vague Prior Revisited\n\nVague PriorCode\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_beta(4,4) +\n  labs(title = \"Vague Prior: Beta(4,4), var(pi) = 0.0278\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subsets-lunch-and-dinner",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subsets-lunch-and-dinner",
    "title": "4: Balance and Sequentiality",
    "section": "Subsets: Lunch and Dinner",
    "text": "Subsets: Lunch and Dinner\n\n\n\nLunch &lt;- tips_df |&gt;\n  filter(time == \"Lunch\")\n\nn_Lunch &lt;- nrow(Lunch)\ny_Lunch &lt;- Lunch |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()\n\n\n\nDinner &lt;- tips_df |&gt;\n  filter(time == \"Dinner\")\n\nn_Dinner &lt;- nrow(Dinner)\ny_Dinner &lt;- Dinner |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#lunch-first-then-dinner",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#lunch-first-then-dinner",
    "title": "4: Balance and Sequentiality",
    "section": "Lunch First, then Dinner",
    "text": "Lunch First, then Dinner\n\nCodePlotsStatistics\n\n\n\nalpha_1 &lt;- 4\nbeta_1  &lt;- 4\np1 &lt;- bayesrules::plot_beta_binomial(alpha_1, beta_1, y_Lunch, n_Lunch) +\n  labs(title = \"Lunch First\")\n\nalpha_2 &lt;- alpha_1 + y_Lunch\nbeta_2  &lt;- beta_1 + n_Lunch - y_Lunch\np2 &lt;- bayesrules::plot_beta_binomial(alpha_2, beta_2, y_Dinner, n_Dinner) +\n  labs(title = \"then Dinner\")\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nLunch First\n\nbayesrules::summarize_beta_binomial(alpha_1, beta_1, y_Lunch, n_Lunch) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    4 0.5000 0.5000 0.0278 0.1667\n2 posterior    27   49 0.3553 0.3514 0.0030 0.0545\n\n\n\n\nthen Dinner\n\nbayesrules::summarize_beta_binomial(alpha_2, beta_2, y_Dinner, n_Dinner) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode   var     sd\n1     prior    27   49 0.3553 0.3514 3e-03 0.0545\n2 posterior    97  155 0.3849 0.3840 9e-04 0.0306"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#dinner-first-then-lunch",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#dinner-first-then-lunch",
    "title": "4: Balance and Sequentiality",
    "section": "Dinner First, then Lunch",
    "text": "Dinner First, then Lunch\n\nCodePlotsStatistics\n\n\n\nalpha_1 &lt;- 4\nbeta_1  &lt;- 4\np1 &lt;- bayesrules::plot_beta_binomial(alpha_1, beta_1, y_Dinner, n_Dinner) +\n  labs(title = \"Dinner First\")\n\nalpha_2 &lt;- alpha_1 + y_Dinner\nbeta_2  &lt;- beta_1 + n_Dinner - y_Dinner\np2 &lt;- bayesrules::plot_beta_binomial(alpha_2, beta_2, y_Lunch, n_Lunch) +\n  labs(title = \"then Lunch\")\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nDinner First\n\nbayesrules::summarize_beta_binomial(alpha_1, beta_1, y_Dinner, n_Dinner) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    4 0.5000 0.5000 0.0278 0.1667\n2 posterior    74  110 0.4022 0.4011 0.0013 0.0361\n\n\n\n\nthen Lunch\n\nbayesrules::summarize_beta_binomial(alpha_2, beta_2, y_Lunch, n_Lunch) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    74  110 0.4022 0.4011 0.0013 0.0361\n2 posterior    97  155 0.3849 0.3840 0.0009 0.0306"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#data-invariance",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#data-invariance",
    "title": "4: Balance and Sequentiality",
    "section": "Data Invariance",
    "text": "Data Invariance\nLet \\(\\theta\\) be any parameter of interest with prior pdf \\(f(\\theta)\\). Then a sequential analysis in which we first observe a data point \\(y_{1}\\) and then a second data point \\(y_{2}\\) will produce the same posterior model of \\(\\theta\\) as if we first observe \\(y_{2}\\) and then \\(y_{1}\\):\n\\[f(\\theta|y1,y2)=f(\\theta|y2,y1)\\]\nSimilarly, the posterior model is invariant to whether we observe the data all at once or sequentially.\n\n\n\n\n\n\nproof\n\n\n\n\n\n[Please refer to section 4.5 of the textbook]"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#uniform-prior-revisited",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#uniform-prior-revisited",
    "title": "4: Balance and Sequentiality",
    "section": "Uniform Prior Revisited",
    "text": "Uniform Prior Revisited\n\nUniform PriorCode\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_beta(1,1) +\n  labs(title = \"Uniform Prior: Beta(1,1), var(pi) = 0.0833\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-sunday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-sunday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Sunday Customers",
    "text": "Subset: Sunday Customers\n\nSunday &lt;- tips_df |&gt;\n  filter(day == \"Sun\")\n\nn_Sunday &lt;- nrow(Sunday)\ny_Sunday &lt;- Sunday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#one-day-at-a-time",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#one-day-at-a-time",
    "title": "4: Balance and Sequentiality",
    "section": "One Day at a Time",
    "text": "One Day at a Time\n\nCodePlotsStatistics\n\n\n\nalpha &lt;- 1\nbeta  &lt;- 1\np1 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Thursday, n_Thursday) +\n  labs(title = \"Thursday First\")\n\nalpha &lt;- alpha + y_Thursday\nbeta  &lt;- beta + n_Thursday - y_Thursday\np2 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Friday, n_Friday) +\n  labs(title = \"then Friday\")\n\nalpha &lt;- alpha + y_Friday\nbeta  &lt;- beta + n_Friday - y_Friday\np3 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Saturday, n_Saturday) +\n  labs(title = \"then Saturday\")\n\nalpha &lt;- alpha + y_Saturday\nbeta  &lt;- beta + n_Saturday - y_Saturday\np4 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Sunday, n_Sunday) +\n  labs(title = \"then Sunday\")\n\n# patchwork\np1 / p2 / p3 / p4\n\n\n\n\n\n\n\n\n\n\n\nThursday First\n\nalpha &lt;- 1\nbeta  &lt;- 1\nbayesrules::summarize_beta_binomial(alpha, beta, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1    1 0.5000    NaN 0.0833 0.2887\n2 posterior    18   46 0.2812 0.2742 0.0031 0.0558\n\n\n\n\nthen Friday\n\nalpha &lt;- alpha + y_Thursday\nbeta  &lt;- beta + n_Thursday - y_Thursday\nbayesrules::summarize_beta_binomial(alpha, beta, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    18   46 0.2812 0.2742 0.0031 0.0558\n2 posterior    33   50 0.3976 0.3951 0.0029 0.0534\n\n\n\n\nthen Saturday\n\nalpha &lt;- alpha + y_Friday\nbeta  &lt;- beta + n_Friday - y_Friday\nbayesrules::summarize_beta_binomial(alpha, beta, y_Saturday, n_Saturday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    33   50 0.3976 0.3951 0.0029 0.0534\n2 posterior    75   95 0.4412 0.4405 0.0014 0.0380\n\n\n\n\nthen Sunday\n\nalpha &lt;- alpha + y_Saturday\nbeta  &lt;- beta + n_Saturday - y_Saturday\nbayesrules::summarize_beta_binomial(alpha, beta, y_Sunday, n_Sunday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    75   95 0.4412 0.4405 0.0014 0.0380\n2 posterior    94  152 0.3821 0.3811 0.0010 0.0309"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-all-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-all-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: All Customers",
    "text": "Subset: All Customers\n\nn_all &lt;- nrow(tips_df)\ny_all &lt;- tips_df |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#all-at-once",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#all-at-once",
    "title": "4: Balance and Sequentiality",
    "section": "All at Once",
    "text": "All at Once\n\nCodePlotStatistics\n\n\n\nalpha &lt;- 1\nbeta  &lt;- 1\nbayesrules::plot_beta_binomial(alpha, beta, y_all, n_all) +\n  labs(title = \"All at Once\")\n\n\n\n\n\n\n\n\n\n\n\nalpha &lt;- 1\nbeta  &lt;- 1\nbayesrules::summarize_beta_binomial(alpha, beta, y_all, n_all) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1    1 0.5000    NaN 0.0833 0.2887\n2 posterior    94  152 0.3821 0.3811 0.0010 0.0309\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2  forcats_1.0.0    stringr_1.5.0    dplyr_1.1.3     \n [5] purrr_1.0.2      readr_2.1.4      tidyr_1.3.0      tibble_3.2.1    \n [9] ggplot2_3.4.3    tidyverse_2.0.0  patchwork_1.1.2  bayesrules_0.0.2\n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3       inline_0.3.19       rlang_1.1.1        \n  [4] magrittr_2.0.3      snakecase_0.11.0    matrixStats_1.0.0  \n  [7] e1071_1.7-13        compiler_4.3.0      loo_2.6.0          \n [10] callr_3.7.3         vctrs_0.6.3         reshape2_1.4.4     \n [13] pkgconfig_2.0.3     crayon_1.5.2        fastmap_1.1.1      \n [16] ellipsis_0.3.2      labeling_0.4.3      utf8_1.2.3         \n [19] threejs_0.3.3       promises_1.2.1      rmarkdown_2.24     \n [22] tzdb_0.4.0          markdown_1.8        ps_1.7.5           \n [25] nloptr_2.0.3        bit_4.0.5           xfun_0.40          \n [28] jsonlite_1.8.7      later_1.3.1         parallel_4.3.0     \n [31] prettyunits_1.1.1   R6_2.5.1            dygraphs_1.1.1.6   \n [34] stringi_1.7.12      StanHeaders_2.26.26 boot_1.3-28.1      \n [37] Rcpp_1.0.11         rstan_2.21.8        knitr_1.43         \n [40] zoo_1.8-12          base64enc_0.1-3     bayesplot_1.10.0   \n [43] httpuv_1.6.11       Matrix_1.5-4        splines_4.3.0      \n [46] igraph_1.4.3        timechange_0.2.0    tidyselect_1.2.0   \n [49] rstudioapi_0.15.0   yaml_2.3.7          codetools_0.2-19   \n [52] miniUI_0.1.1.1      processx_3.8.1      pkgbuild_1.4.0     \n [55] lattice_0.21-8      plyr_1.8.8          shiny_1.7.5        \n [58] withr_2.5.2         groupdata2_2.0.2    evaluate_0.21      \n [61] survival_3.5-5      proxy_0.4-27        RcppParallel_5.1.7 \n [64] xts_0.13.1          pillar_1.9.0        DT_0.28            \n [67] stats4_4.3.0        shinyjs_2.1.0       generics_0.1.3     \n [70] vroom_1.6.3         hms_1.1.3           rstantools_2.3.1   \n [73] munsell_0.5.0       scales_1.2.1        minqa_1.2.5        \n [76] gtools_3.9.4        xtable_1.8-4        class_7.3-21       \n [79] glue_1.6.2          janitor_2.2.0       tools_4.3.0        \n [82] shinystan_2.6.0     lme4_1.1-33         colourpicker_1.2.0 \n [85] grid_4.3.0          crosstalk_1.2.0     colorspace_2.1-0   \n [88] nlme_3.1-162        cli_3.6.1           fansi_1.0.4        \n [91] gtable_0.3.4        digest_0.6.33       farver_2.1.1       \n [94] htmlwidgets_1.6.2   htmltools_0.5.6     lifecycle_1.0.4    \n [97] mime_0.12           rstanarm_2.21.4     bit64_4.0.5        \n[100] shinythemes_1.2.0   MASS_7.3-58.4"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html",
    "href": "posts/05_conjugate_families/05_conjugate_families.html",
    "title": "5: Conjugate Families",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"ggtext\")\nlibrary(\"gt\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\ntips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#simple-prior",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#simple-prior",
    "title": "5: Conjugate Families",
    "section": "Simple Prior",
    "text": "Simple Prior\nSuppose that we wanted to estimate a probability \\(\\pi \\in [0,1]\\), but perhaps the beta distribution seems complicated. Instead, we can try an elementary math function like \\(f(\\pi) = 3\\pi^{2}\\), where this is a probability density function since\n\\[\\displaystyle\\int_{0}^{1} \\! 3\\pi^{2} \\, d\\pi = 1 \\text{ and } f(\\pi) \\geq 0 \\text{ for } \\pi \\in [0,1]\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#interpretability",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#interpretability",
    "title": "5: Conjugate Families",
    "section": "Interpretability",
    "text": "Interpretability\n\nPlotCodeInterpretation\n\n\n\n\n\n\n\n\n\n\npi &lt;- seq(0, 1, 0.01)\nf_pi &lt;- 3*pi^2\n\ndf_for_line &lt;- data.frame(pi, f_pi)\ndf_for_shade &lt;- df_for_line |&gt;\n  rbind(c(1,0)) #enforce lower-right corner\n\ndf_for_line |&gt;\n  ggplot(aes(x = pi, y = f_pi)) +\n  geom_polygon(data = df_for_shade, fill = \"#E77500\") +\n  geom_line(color = \"#121212\", linewidth = 3) +\n  labs(title = \"&lt;span style='color:#E77500'&gt;Parabolic Prior&lt;/span&gt;: f(pi) = 3pi^2\",\n       subtitle = \"left-skew\",\n       caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown()) #use ggtext package\n\n\n\nIf we start with this prior, we are perhaps assuming a situation over \\([0,1]\\) where we are expecting the event to likely occur:\n\\[\\text{E}(\\pi) = \\displaystyle\\int_{0}^{1} \\! \\pi \\cdot f(\\pi) \\, d\\pi = \\displaystyle\\frac{3}{4}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#likelihood",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#likelihood",
    "title": "5: Conjugate Families",
    "section": "Likelihood",
    "text": "Likelihood\nSuppose that we observe \\(Y = 17\\) successes in \\(n = 32\\) independent trials, then modeling the likelihood with a binomial model yields\n\\[L(\\pi|y = 17) = \\binom{32}{17}\\pi^{17}(1-\\pi)^{15} \\text{ for } \\pi \\in [0,1]\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#posterior-distribution",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#posterior-distribution",
    "title": "5: Conjugate Families",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\nRecall that the posterior distribution is proportional to the product of the prior distribution and the likelihood\n\\[\\begin{array}{rcl}\n  f(\\pi|y=17) & \\propto & f(\\pi) \\cdot L(\\pi|y=17) \\\\\n  ~ & \\propto & \\pi^{2} \\cdot \\pi^{17}(1-\\pi)^{15} \\\\\n\\end{array}\\]\ndoes not have the same form as our prior \\(f(\\pi) = 3\\pi^{2}\\)"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#normalizing-constant",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#normalizing-constant",
    "title": "5: Conjugate Families",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\\[f(\\pi|y=17) = \\displaystyle\\frac{\\pi^{19}(1-\\pi)^{15}}{ \\int_{0}^{1} \\! \\pi^{19}(1-\\pi)^{15} \\, d\\pi } \\text{ for } \\pi \\in [0,1]\\]\n\nintegrals can be tough to compute, even with numerical methods\nvery low interpretability\ndifficult to compute sample statistics for the posterior distribution (such as mean and variance)"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#conjugate-priors",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#conjugate-priors",
    "title": "5: Conjugate Families",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\nConjugate families have both computational ease and interpretable posterior distributions.\n\n\n\n\n\n\nConjugate Priors\n\n\n\n\n\nLet the prior model for parameter \\(\\theta\\) have pdf \\(f(\\theta)\\) and the model of data Y conditioned on \\(\\theta\\) have likelihood function \\(L(\\theta|y)\\). If the resulting posterior model with pdf \\(f(\\theta|y) \\propto f(\\theta)L(\\theta|y)\\) is of the same model family as the prior, then we say this is a conjugate prior."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#poisson-process",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#poisson-process",
    "title": "5: Conjugate Families",
    "section": "Poisson Process",
    "text": "Poisson Process\n\nMotivationGoalInfinitessimalPartial Proof\n\n\n\nAssume a constant \\(\\lambda\\) of arrivals\nLet \\(N_{t}\\) be the number of arrivals in time interval \\([0,t]\\)\nHomogeneity: \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nIndependence: numbers of arrivals in disjoint time intervals are independent random variables\n\n\n\nerive distribution of number of arrivals\n\nWe expect \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nPartition time interval \\([0,t]\\) into \\(n\\) subintervals\nAssuming \\(n\\) is large enough so that each subinterval has zero or one arrival (i.e. Bernoulli trial)\nProbability of arrival in a random subinterval: \\(p = \\displaystyle\\frac{\\lambda t}{n}\\)\n\nSo far, we are assuming \\(N_{t} \\sim \\text{Bin}(n,p)\\)\n\\[P(N_{t} = k) = \\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k} \\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n-k}\\]\n\n\nHowever,\n\n\\(n\\) was arbitrary\ntime is a continuous variable\n\nSo let’s take the limit as \\(n\\) goes to infinity.\n\\[\\displaystyle\\lim_{n \\to \\infty} P(N_{t} = k) = \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}}\\]\n\n\nHandling the limit by its factors: \\[\\displaystyle\\lim_{n \\to \\infty} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}} = 1, \\quad \\displaystyle\\lim_{n \\to \\infty} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} = e^{-\\lambda t}\\]\n\\[\\begin{array}{rcl}\n  \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\binom{n}{k} \\left(\\displaystyle\\frac{1}{n}\\right)^{k} \\\\\n  ~ & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{k!(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#poisson-distribution",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#poisson-distribution",
    "title": "5: Conjugate Families",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nLet discrete random variable \\(Y\\) be the number of independent events that occur in a fixed amount of time or space, where \\(\\lambda&gt;0\\) is the rate at which these events occur. Then the dependence of \\(Y\\) on parameter \\(\\lambda\\) can be modeled by the Poisson.\n\\[Y|\\lambda \\sim \\text{Pois}(\\lambda)\\]\nwith probability mass function\n\\[f(y|\\lambda) = \\displaystyle\\frac{\\lambda^{y}e^{-\\lambda}}{y!} \\text{ for } y \\in \\{0, 1, 2, ...\\}\\]\n\n\\(f(y|\\lambda) \\geq 0\\)\n\\(\\displaystyle\\sum_{y=0}^{\\infty} \\! f(y|\\lambda) = 1\\)\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\nThe Poisson distribution has the curious property where the randomness has equal mean and variance:\n\\[\\text{E}(Y|\\lambda) = \\text{Var}(Y|\\lambda) = \\lambda\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#guidance",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#guidance",
    "title": "5: Conjugate Families",
    "section": "Guidance",
    "text": "Guidance\n\nPlotCodeGuidance\n\n\n\n\n\n\n\n\n\n\ny_i &lt;- 0:10\nf_y &lt;- dpois(y_i, 1)\ndf_for_plots &lt;- data.frame(y_i,f_y)\n\np1 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 1))) + \n  geom_col() + \n  labs(title = \"lambda = 1\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np2 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 2))) + \n  geom_col() + \n  labs(title = \"lambda = 2\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np3 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 3))) + \n  geom_col() + \n  labs(title = \"lambda = 3\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np4 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 4))) + \n  geom_col() + \n  labs(title = \"lambda = 4\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np5 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 5))) + \n  geom_col() + \n  labs(title = \"lambda = 5\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\n# patchwork\np1 + p2 + p3 + p4 + p5\n\n\n\nThe Poisson distribution is a discrete distribution that tends to be used to model rare events."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#joint-pmf",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#joint-pmf",
    "title": "5: Conjugate Families",
    "section": "Joint PMF",
    "text": "Joint PMF\nLet \\((Y_1,Y_2,…,Y_n)\\) be an independent sample of random variables and \\(\\vec{y} = (y_1,y_2,…,y_n)\\) be the corresponding vector of observed values.\n\n\n\n\n\n\nJoint Probability Mass Function\n\n\n\n\n\nFurther, let \\(f(y_i|\\lambda)\\) denote the pmf of an individual observed data point \\(Y_i=y_i\\). Then by the assumption of independence, the following joint pmf specifies the randomness in and plausibility of the collective sample:\n\\[f(\\vec{y}|\\lambda) = \\displaystyle\\prod_{i=1}^{n} f(y_{i}|\\lambda) = f(y_{1}|\\lambda) \\cdot (y_{2}|\\lambda) \\cdots f(y_{n}|\\lambda)\\]\n\n\n\nThe Poisson probability mass function is then\n\\[\\begin{array}{rcl}\n  f(\\vec{y}|\\lambda) & = & \\displaystyle\\prod_{i=1}^{n} f(y_{i}|\\lambda) \\\\\n  ~ & = & \\displaystyle\\prod_{i=1}^{n} \\displaystyle\\frac{\\lambda^{y_{i}}e^{\\lambda}}{y_{i}!} \\\\\n  ~ & = & \\displaystyle\\frac{\\lambda^{y_{1}}e^{\\lambda}}{y_{1}!} \\cdot \\displaystyle\\frac{\\lambda^{y_{2}}e^{\\lambda}}{y_{2}!} \\cdots \\displaystyle\\frac{\\lambda^{y_{n}}e^{\\lambda}}{y_{n}!} \\\\\n  ~ & = & \\displaystyle\\frac{ [\\lambda^{y_{1}}\\lambda^{y_{2}}\\cdots\\lambda^{y_{n}}][e^{-\\lambda}e^{-\\lambda} \\cdots e^{-\\lambda}] }{ y_{1}!y_{2}! \\cdots y_{n}! } \\\\\n  ~ & = & \\displaystyle\\frac{\\lambda^{\\sum y_{i}}e^{-n\\lambda}}{\\prod y_{i}!} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#poisson-likelihood",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#poisson-likelihood",
    "title": "5: Conjugate Families",
    "section": "Poisson Likelihood",
    "text": "Poisson Likelihood\nThe Poisson likelihood function is then\n\\[L(\\lambda|\\vec{y}) = \\displaystyle\\frac{\\lambda^{\\sum y_{i}}e^{-n\\lambda}}{\\prod y_{i}!}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#parameter-selection",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#parameter-selection",
    "title": "5: Conjugate Families",
    "section": "Parameter Selection",
    "text": "Parameter Selection\nHow do we fit a Poisson model with our data? One idea is to seek the maximum likelihood estimate (MLE).\nClaim: The MLE for the \\(\\text{Pois}(\\lambda)\\) distribution is \\[\\lambda^{*} = \\bar{y} = \\displaystyle\\frac{\\sum y_{i}}{n}\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the Poisson distribution’s PMF \\(f(y) = \\displaystyle\\frac{\\lambda^{y}e^{-\\lambda}}{y!}\\), the likelihood function\n\\[L(\\lambda) = \\displaystyle\\frac{\\lambda^{y_{1}}e^{-\\lambda}}{y_{1}!} \\cdot \\displaystyle\\frac{\\lambda^{y_{2}}e^{-\\lambda}}{y_{2}!} \\cdots \\displaystyle\\frac{\\lambda^{y_{n}}e^{-\\lambda}}{y_{n}!} \\]\nTaking the natural logarithm of both sides, we create the log likelihood function \\(\\ell(\\lambda)\\)\n$$\n\\[\\begin{array}{rcl}\n  \\ln L(\\lambda) & = & \\ln \\left(\\displaystyle\\frac{\\lambda^{y_{1}}e^{-\\lambda}}{y_{1}!} \\cdot \\displaystyle\\frac{\\lambda^{y_{2}}e^{-\\lambda}}{y_{2}!} \\cdots \\displaystyle\\frac{\\lambda^{y_{n}}e^{-\\lambda}}{y_{n}!}\\right) \\\\\n  \\ell(\\lambda) & = & \\ln \\displaystyle\\prod_{i=1}^{n} \\displaystyle\\frac{\\lambda^{y_{i}}e^{-\\lambda}}{y_{i}!} \\\\\n  \\ell(\\lambda) & = & \\displaystyle\\sum_{i=1}^{n} \\ln \\displaystyle\\frac{\\lambda^{y_{i}}e^{-\\lambda}}{y_{i}!} \\\\\n  \n  \\ell(\\lambda) & = & \\displaystyle\\sum_{i=1}^{n} \\left( y_{i}\\ln \\lambda + \\ln e^{-\\lambda} - \\ln y_{i}! \\right) \\\\\n  \n  \\ell(\\lambda) & = & (\\ln \\lambda)\\left(\\displaystyle\\sum_{i=1}^{n} y_{i}\\right) -  \\displaystyle\\sum_{i=1}^{n}\\lambda -  \\displaystyle\\sum_{i=1}^{n} \\ln y_{i}! \\\\\n  \n  \\ell(\\lambda) & = &  (\\ln \\lambda)\\left(\\displaystyle\\sum_{i=1}^{n} y_{i}\\right) - n\\lambda - \\displaystyle\\sum_{i=1}^{n} \\ln (y_{i}!) \\\\\n\\end{array}\\]\n$$\nThe motivation for the logarithm usage is to ease the process of taking the derivative. Here, taking the derivative with respect to \\(\\lambda\\),\n\\[0 = \\ell'(\\lambda)  \\quad\\Rightarrow\\quad 0 = -n + \\displaystyle\\frac{ \\sum_{i=1}^{n} y_{i} }{ \\lambda } \\quad\\Rightarrow\\quad \\lambda = \\displaystyle\\frac{ \\sum_{i=1}^{n} y_{i} }{ n } = \\bar{y}\\]\nThat is, the optimal value for parameter \\(\\lambda\\) is the sample mean \\(\\bar{y}\\)."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety",
    "title": "5: Conjugate Families",
    "section": "Example: Campus Safety",
    "text": "Example: Campus Safety\n\nDataLikelihoodCode\n\n\nThe following data on arrests for drug law violations come from the Princeton University Annual Security and Fire Safety Report (in and around the main campus)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n\n\n\n\narrests\n18\n14\n23\n22\n12\n22\n7\n0\n1\n\n\n\nOur maximum likelihood estimate is\n\\[\\lambda^{*} = \\displaystyle\\frac{\\sum y_{i}}{n} = \\displaystyle\\frac{119}{9} \\approx 13.2222 \\text{ arrests per year}\\]\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_poisson_likelihood(\n  y = c(18, 14, 23, 22, 12, 22, 7, 0, 1),\n  lambda_upper_bound = 20\n) +\n  labs(title = \"Likelihood Curve\",\n       subtitle = \"Arrests per year for drug law violations\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#terminology",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#terminology",
    "title": "5: Conjugate Families",
    "section": "Terminology",
    "text": "Terminology\nLet \\(\\lambda &gt; 0\\) be a continuous random variable. For modeling, we might try a Gamma model \\[\\lambda \\sim \\text{Gamma}(s, r)\\]\n\n\\(s\\): shape parameter\n\\(r\\): rate parameter\n\n\n\n\n\n\n\nExplore!\n\n\n\n\n\nMatt Bognar at the University of Iowa created this great webapp to explore the gamma distribution.\n\n\n\n\n\n\n\n\n\nExponential Model\n\n\n\n\n\nThe Gamma model is a generalization of the exponential model. When the shape parameter \\(s = 1\\), then\n\\[\\lambda \\sim \\text{Gamma}(1,r) = \\text{Exp}(r)\\]\nwhere \\(r\\) is once again the rate parameter."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function",
    "title": "5: Conjugate Families",
    "section": "Probablity Density Function",
    "text": "Probablity Density Function\nThe Gamma model has a continuous pdf\n\\[f(\\lambda) = \\displaystyle\\frac{r^{s}}{\\Gamma(s)} \\lambda^{s-1}e^{-r\\lambda} \\text{ for } \\lambda &gt; 0\\]\nwhere the gamma function\n\n\\(\\Gamma(z) = \\displaystyle\\int_{0}^{\\infty} \\! x^{z-1}e^{-x} \\, dx\\)\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\nFormulas for the Gamma model include\n\\[\\begin{array}{rcl}\n  \\text{E}(\\lambda) & = & \\displaystyle\\frac{s}{r} \\\\\n  \\text{Mode}(\\lambda) & = & \\displaystyle\\frac{s-1}{r} \\\\\n  \\text{Var}(\\lambda) & = & \\displaystyle\\frac{s}{r^{2}} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior",
    "title": "5: Conjugate Families",
    "section": "Tuning the Prior",
    "text": "Tuning the Prior\n\nExample: Campus SafetyStatisticsPlotCode\n\n\nSuppose that a parent of an university applicant feels that the university has arrests for drug law violations with counts between 10 and 30 per year. Matching some statistics formulas\n\\[[\\mu - 2\\sigma, \\mu + 2\\sigma] = [10, 30] \\quad\\rightarrow\\quad \\mu = 20, \\quad \\sigma = 5\\]\n\n\n\\[\\text{E}(\\lambda) = \\displaystyle\\frac{s}{r} = 20 \\text{ and } \\text{Var}(\\lambda) = \\displaystyle\\frac{s}{r^{2}} = 5^{2}\\]\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_gamma(16, 0.8, mean = TRUE) +\n  labs(title = \"Gamma(16, 0.8) Prior\",\n       subtitle = \"mean = 20, sd = 5\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#gamma-poisson-bayesian-model",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#gamma-poisson-bayesian-model",
    "title": "5: Conjugate Families",
    "section": "Gamma-Poisson Bayesian Model",
    "text": "Gamma-Poisson Bayesian Model\nLet \\(\\lambda &gt; 0\\) be an unknown rate parameter and let \\(\\{Y_{1}, Y_{2}, ..., Y_{n}\\}\\) be an i.i.d. sample from a \\(\\text{Pois}(\\lambda)\\) distribution. With a setup of a Gamma prior and Poisson likelihood\n\\[\\begin{array}{rcl}\n  \\lambda & \\sim & \\text{Gamma}(s,r) \\\\\n  Y_{i}|\\lambda & \\sim & \\text{Pois}(\\lambda) \\\\\n\\end{array}\\]\nand observing data \\(\\vec{y} = \\{y_{1}, y_{2}, ..., y_{n}\\}\\), the posterior distribution also has a Gamma structure with updated parameters\n\\[\\lambda|\\vec{y} \\sim \\text{Gamma}\\left( s + \\displaystyle\\sum_{i=1}^{n} y_{i}, r + n \\right)\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\\begin{array}{rcl}\n  f(\\lambda|\\vec{y}) & \\propto & f(\\lambda) \\cdot L(\\lambda|\\vec{y}) \\\\\n  ~ & = & \\displaystyle\\frac{r^{s}}{\\Gamma(s)}\\lambda^{s-1}e^{-r\\lambda} \\cdot \\displaystyle\\frac{\\lambda^{\\sum y_{i}}e^{-n\\lambda}}{\\prod y_{i}!} \\\\\n  ~ & \\propto & \\lambda^{s-1}e^{-r\\lambda} \\cdot \\lambda^{\\sum y_{i}}e^{-n\\lambda} \\\\\n  ~ & = & \\lambda^{s+\\sum y_{i} - 1}e^{-(r+n)\\lambda} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety-2",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety-2",
    "title": "5: Conjugate Families",
    "section": "Example: Campus Safety",
    "text": "Example: Campus Safety\n\nRecapPlotsCodeStatistics\n\n\n\nwe tuned a \\(\\text{Gamma}(16, 0.8)\\) prior\nwe observed 119 arrests for drug law violations over a \\(n = 9\\) year time span\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_gamma_poisson(shape = 16, rate = 0.8,\n                               sum_y = 119, n = 9) +\n  labs(title = \"Gamma-Poisson Model\",\n       subtitle = \"Drug Law Violations Example\",\n       caption = \"SML 320\",\n       x = \"arrests for drug law violations\") +\n  theme_minimal()\n\n\n\n\nbayesrules::summarize_gamma_poisson(shape = 16, rate = 0.8,\n                                    sum_y = 119, n = 9) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model shape rate    mean    mode     var     sd\n1     prior    16  0.8 20.0000 18.7500 25.0000 5.0000\n2 posterior   135  9.8 13.7755 13.6735  1.4057 1.1856"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#terminology-1",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#terminology-1",
    "title": "5: Conjugate Families",
    "section": "Terminology",
    "text": "Terminology\nLet \\(Y &gt; 0\\) be a continuous random variable over all real numbers \\(())-\\infty, \\infty)\\). For modeling, we might try a normal distribution \\[Y \\sim \\text{N}(\\mu, \\sigma^{2})\\]\n\n\\(\\mu\\): mean\n\\(\\sigma\\): standard deviation"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function-1",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function-1",
    "title": "5: Conjugate Families",
    "section": "Probablity Density Function",
    "text": "Probablity Density Function\nThe normal distribution has a continuous probability density function\n\\[f(y) = \\displaystyle\\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} \\text{exp}\\left[ -\\displaystyle\\frac{(y-\\mu)^{2}}{2\\sigma^{2}}\\right] \\text{ for } y \\in (-\\infty, \\infty)\\]\n\n\n\n\n\n\nStatistics\n\n\n\n\n\nDescriptions of normal distributions are dictated by their statistics\n\\[\\begin{array}{rcl}\n  \\text{E}(Y) & = & \\mu \\\\\n  \\text{Mode}(Y) & = & \\mu \\\\\n  \\text{Var}(Y) & = & \\sigma^{2} \\\\\n  \\text{SD}(Y) & = & \\sigma \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior-1",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior-1",
    "title": "5: Conjugate Families",
    "section": "Tuning the Prior",
    "text": "Tuning the Prior\n\nExample: TipsStatisticsPlotCode\n\n\n\nhead(tips_df)\n\n# A tibble: 6 × 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n3       21.0  3.5  Male   No     Sun   Dinner     3\n4       23.7  3.31 Male   No     Sun   Dinner     2\n5       24.6  3.61 Female No     Sun   Dinner     4\n6       25.3  4.71 Male   No     Sun   Dinner     4\n\n\n\n\nLet us guess that Americans tend to tip between 5 and 25 percent of the total bill.\n\\[[\\mu - 2\\sigma, \\mu + 2\\sigma] = [5, 25] \\quad\\rightarrow\\quad \\mu = 15, \\quad \\sigma = 5\\]\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_normal(mean = 15, sd = 5) +\n  labs(title = \"N(15, 25) Prior\",\n       subtitle = \"mean = 15, sd = 5\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#likelihood-2",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#likelihood-2",
    "title": "5: Conjugate Families",
    "section": "Likelihood",
    "text": "Likelihood\nIn this conjugate prior relationship, the likelihood is also modeled as a normal distribution.\n\\[L(\\mu, \\sigma|\\vec{y}) \\propto \\displaystyle\\prod_{i=1}^{n} \\text{exp}\\left[-\\displaystyle\\frac{(y_{i} - \\mu)^{2}}{2\\sigma^{2}}\\right] = \\text{exp}\\left[-\\displaystyle\\frac{\\sum_{i=1}^{n} (y_{i}-\\mu)^{2}}{2\\sigma^{2}}\\right]\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#mles",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#mles",
    "title": "5: Conjugate Families",
    "section": "MLEs",
    "text": "MLEs\nThe likelihood can also be expressed in terms of the sample mean \\(\\bar{y}\\) and the sample size \\(n\\)\n\\[L(\\mu, \\sigma|\\vec{y}) \\propto \\text{exp}\\left[-\\displaystyle\\frac{ (\\bar{y}-\\mu)^{2}}{\\frac{2\\sigma^{2}}{n}}\\right]\\]\nIt follows that the maximum likelihood estimates for the parameters are\n\\[\\begin{array}{rcl}\n  \\mu^{*} & = & \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} y_{i} \\\\\n  \\sigma^{*} & = & \\sqrt{\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (y_{i} - \\mu^{2})^{2}} \\\\\n\\end{array}\\]\nwhich are the sample mean and from the not-corrected population variance (source)."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#dplyr",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#dplyr",
    "title": "5: Conjugate Families",
    "section": "dplyr",
    "text": "dplyr\n\nn &lt;- nrow(tips_df)\ntips_df |&gt;\n  mutate(tips_pct = tip/total_bill * 100) |&gt;\n  summarize(mu = mean(tips_pct, na.rm = TRUE),\n            sigma = sqrt(var(tips_pct, na.rm = TRUE) *(n-1)/(n)))\n\n# A tibble: 1 × 2\n     mu sigma\n  &lt;dbl&gt; &lt;dbl&gt;\n1  16.1  6.09"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#plot-4",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#plot-4",
    "title": "5: Conjugate Families",
    "section": "Plot",
    "text": "Plot"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#code-6",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#code-6",
    "title": "5: Conjugate Families",
    "section": "Code",
    "text": "Code\n\nbayesrules::plot_normal(mean = 16.08026, sd = 6.094693  ) +\n  labs(title = \"Normal \",\n       subtitle = \"MLEs: ybar = 16.08026, sigma = 6.094693\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#normal-normal-conjugacy",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#normal-normal-conjugacy",
    "title": "5: Conjugate Families",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\nLet \\(\\mu \\in (-\\infty, \\infty)\\) be an unknown mean parameter and let \\(\\sigma^{2} &gt; 0\\) be an unknown variance parameter and let \\(\\{Y_{1}, Y_{2}, ..., Y_{n}\\}\\) be an i.i.d. sample from a \\(\\text{N}(\\mu, \\sigma^{2})\\) distribution. With a setup of a normal prior and normal likelihood\n\\[\\begin{array}{rcl}\n  \\mu,\\sigma^{2} & \\sim & \\text{N}(\\theta,\\tau^{2}) \\\\\n  Y_{i}|\\mu, \\sigma^{2} & \\sim & \\text{N}(\\mu,\\sigma^{2}) \\\\\n\\end{array}\\]\nand observing data \\(\\vec{y} = \\{y_{1}, y_{2}, ..., y_{n}\\}\\), the posterior distribution also has a normal structure with updated parameters\n\\[\\mu,\\sigma^{2}|\\vec{y} \\sim \\text{N}\\left( \\displaystyle\\frac{\\sigma^{2}}{n\\tau^{2}+\\sigma^{2}} \\cdot \\theta + \\displaystyle\\frac{n\\tau^{2}}{n\\tau^{2}+\\sigma^{2}} \\cdot \\bar{y}, \\quad \\displaystyle\\frac{\\tau^{2}\\sigma^{2}}{n\\tau^{2}+\\sigma^{2}} \\right)\\]\n\nWhat happens if we have relatively small data sets?\nWhat happens if we have relatively large data sets?"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#example",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#example",
    "title": "5: Conjugate Families",
    "section": "Example",
    "text": "Example\n\nCodePlotsStatistics\n\n\n\nbayesrules::plot_normal_normal(\n  \n  # from prior\n  mean = 15, sd = 5,\n  \n  # from observations\n  y_bar = 16.08026, sigma = 6.094693, n = 244\n) +\n  labs(title = \"Normal-Normal Model\",\n       subtitle = \"Restaurant Tips Example\",\n       caption = \"SML 320\",\n       x = \"percent of total food bill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_normal_normal(\n  \n  # from prior\n  mean = 15, sd = 5,\n  \n  # from observations\n  y_bar = 16.08026, sigma = 6.094693, n = 244\n) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model    mean    mode     var    sd\n1     prior 15.0000 15.0000 25.0000 5.000\n2 posterior 16.0737 16.0737  0.1513 0.389"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#model-selection",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#model-selection",
    "title": "5: Conjugate Families",
    "section": "Model Selection",
    "text": "Model Selection\nWe looked at 3 conjugate families.\n\nBeta-BinomialGamma-PoissonNormal-Normal\n\n\n\nestimate \\(\\pi \\in [0,1]\\)\npro: good for interpretability\ncon: computationally expensive for large \\(n\\)\n\n\n\n\nestimate \\(\\lambda &gt; 0\\)\npro: models rare events and skewed data well\ncon: discussion of rates instead of counts\n\n\n\n\nestimate mean \\(\\mu\\) and variance \\(\\sigma\\)\npro: ubiquitous in scientific communities\ncons:\n\ninfinite support may lead to suboptimal results in larger networks\nwas the data symmetric?"
  }
]