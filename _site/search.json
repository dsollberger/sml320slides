[
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html",
    "href": "posts/01_conditional_probability/01_conditional_probability.html",
    "title": "1: Conditional Probability",
    "section": "",
    "text": "Spring 2024\nTuTh, 11 AM to 1220 PM\nBendheim House 103\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides an introduction to Bayesian analysis—a powerful statistical framework for making inferences and modeling uncertainty in a wide range of applications. Students will explore the fundamental principles of Bayesian statistics, probability theory, Bayesian inference, and practical applications of Bayesian modeling. The course will cover both the theory and hands-on implementation using data science software and the R programming language."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SML 320: Bayesian Analysis",
    "section": "",
    "text": "8: MCMC\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n7: MCMC\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n6: Approximating the Posterior\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n5: Conjugate Families\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n4: Balance and Sequentiality\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n3: Beta-Binomial Models\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n2: Bayes’ Rule\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\n  \n\n\n\n\n1: Conditional Probability\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nDerek Sollberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html",
    "href": "posts/02_bayes_rule/02_bayes_rule.html",
    "title": "2: Bayes’ Rule",
    "section": "",
    "text": "In the previous section, we studied conditional probability \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] and we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is \\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\] This is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is \\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] Bayes’ Rule combines the ideas of conditioned probability and total probability as \\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#bayes-rule",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#bayes-rule",
    "title": "2: Bayes’ Rule",
    "section": "",
    "text": "In the previous section, we studied conditional probability \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] and we talked about how the inverse probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are almost never equal. In this section, we discuss how to properly think and calculate that inverse probability.\n\n\n\n\n\n\nTip\n\n\n\nAnother look at conditional probability is \\[P(A \\text{ and } B) = P(B|A) \\cdot P(A)\\] This is read as “The probability of the intersection \\(A\\) and \\(B\\) is the probability of event \\(B\\) conditioned on event \\(A\\).”\n\n\n\n\n\n\n\n\nTip\n\n\n\nMoreover, if we consider how if event \\(B\\) is dependent on event \\(A\\), then sometimes \\(B\\) happens when \\(A\\) happens and sometimes when \\(A\\) does not occur. More succinctly, the total probability of event \\(B\\) is \\[P(B) = P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nStaring with the conditional probability formula \\[P(B|A) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(A)}\\] Bayes’ Rule combines the ideas of conditioned probability and total probability as \\[P(A|B) = \\displaystyle\\frac{P(A \\text{ and } B)}{P(B)} = \\displaystyle\\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^{c}) \\cdot P(A^{c})}\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#a-deep-dive",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#a-deep-dive",
    "title": "2: Bayes’ Rule",
    "section": "A Deep Dive",
    "text": "A Deep Dive\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T)\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#more-practice",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#more-practice",
    "title": "2: Bayes’ Rule",
    "section": "More Practice",
    "text": "More Practice\n\nExampleTree DiagramNumeratorDenominatorBayes’ Rule\n\n\nAn executive has their blood tested for boneitis. Let \\(B\\) be the event that an executive has the disease, and let \\(T\\) be the event that the test returns positive. Laboratory trials yielded the following information:\n\\[P(T|B) = 0.70 \\quad\\text{and}\\quad P(T|B^{c}) = 0.10\\]\nAssume a prior probability of \\(P(B) = 0.0032\\). Compute \\(P(B|T^{c})\\)\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram\n\n\n\n\n\n\n\ntree diagram"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-spam-filtering",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-spam-filtering",
    "title": "2: Bayes’ Rule",
    "section": "Example: Spam Filtering",
    "text": "Example: Spam Filtering\nIn 2002, Paul Graham used Bayes’ Rule as part of his algorithms to greatly decrease false positive rates of unwanted e-mails (“spam”). Let \\(H^{c}\\) be the event that an e-mail is “spam”. Let \\(W\\) be the event that an e-mail contains a trigger word such as “watches”. Suppose that\n\nthe probability that an e-mail contains that word given that it is spam is 17%\nthe probability that an e-mail contains that word given that it is not spam is 9%\nthe probability that a randomly selected e-mail message is spam is 80%\n\nFind the probability that an e-mail message is spam, given that the trigger word appears."
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-quality-control",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-quality-control",
    "title": "2: Bayes’ Rule",
    "section": "Example: Quality Control",
    "text": "Example: Quality Control\nA manufacturing process produces integrated circuit chips. Over the long run the fraction of bad chips produced by the process is around 20%. Thoroughly testing a chip to determine whether it is good or bad is rather expensive, so a cheap test is tried. All good chips will pass the cheap test, but so will 10% of the bad chips. Given that a chip passes the test, what is the probability that the chip was defective?"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-dui-checkpoint",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-dui-checkpoint",
    "title": "2: Bayes’ Rule",
    "section": "Example: DUI Checkpoint",
    "text": "Example: DUI Checkpoint\nA breath analyzer, used by the police to test whether drivers exceed the legal limit set for the blood alcohol percentage while driving, is known to satisfy\n\\[P(A|B) = P(A^{c}|B^{c}) = x\\]\nwhere \\(A\\) is the event “breath analyzer indicates that legal limit is exceeded” and \\(B\\) “driver’s blood alcohol percentage exceeds legal limit.” On Saturday nights, about 4% of the drivers are known to exceed the limit.\n\nDescribe in words the meaning of \\(P(B|A)\\)\nDetermine \\(P(B|A)\\) if \\(x = 0.90\\)\nHow big should \\(x\\) be so that \\(P(B|A) \\geq 0.95\\)?"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#example-monty-hall-problem",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#example-monty-hall-problem",
    "title": "2: Bayes’ Rule",
    "section": "Example: Monty Hall Problem",
    "text": "Example: Monty Hall Problem\n\n\n\n\nMonty Hall asks you to choose one of three doors. One of the doors hides a prize and the other two doors have no prize. You state out loud which door you pick, but you don’t open it right away.\n“Monty opens one of the other two doors, and there is no prize behind it.\n“At this moment, there are two closed doors, one of which you picked. The prize is behind one of the closed doors, but you don’t know which one. Monty asks you, ‘Do you want to switch doors?’”\n\nswitch doors\ndo not switch doors"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#generalized-bayes-rule",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#generalized-bayes-rule",
    "title": "2: Bayes’ Rule",
    "section": "Generalized Bayes’ Rule",
    "text": "Generalized Bayes’ Rule\nIf we are conditioning \\(B\\) on an event \\(A\\), where the latter can be partitioned into several subsets,\n\\[A = \\{ A_{1}, A_{2}, ..., A_{j} \\}\\]\nthen the total probability is\n\\[P(B) = P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n})\\]\nand Bayes Rule for computing the probability of \\(A_{i}\\) given \\(B\\) becomes\n\\[P(A_{i}|B) = \\displaystyle\\frac{ P(B|A_{i}) \\cdot P(A_{i}) }{ P(B|A_{1}) \\cdot P(A_{1}) + P(B|A_{2}) \\cdot P(A_{2}) + ... + P(B|A_{n}) \\cdot P(A_{n}) }\\]"
  },
  {
    "objectID": "posts/02_bayes_rule/02_bayes_rule.html#bayesian-odds",
    "href": "posts/02_bayes_rule/02_bayes_rule.html#bayesian-odds",
    "title": "2: Bayes’ Rule",
    "section": "Bayesian Odds",
    "text": "Bayesian Odds\n\n\n\n\n\n\nNote\n\n\n\nThe Bayesian odds of event \\(A\\) to event \\(B\\) given that event \\(C\\) has already taken place is \\[\\displaystyle\\frac{ P(A|C) }{ P(B|C) }\\]"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-analysis",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-analysis",
    "title": "1: Conditional Probability",
    "section": "",
    "text": "Spring 2024\nTuTh, 11 AM to 1220 PM\nBendheim House 103\nLecturer: Derek\n\nI go by “Derek” or “teacher”\n\n\n\n\n\n\n\n\nCourse Description\n\n\n\n\n\nThis course provides an introduction to Bayesian analysis—a powerful statistical framework for making inferences and modeling uncertainty in a wide range of applications. Students will explore the fundamental principles of Bayesian statistics, probability theory, Bayesian inference, and practical applications of Bayesian modeling. The course will cover both the theory and hands-on implementation using data science software and the R programming language."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#lecturer",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#lecturer",
    "title": "1: Conditional Probability",
    "section": "Lecturer",
    "text": "Lecturer"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#current-research-in-pedagogy",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#current-research-in-pedagogy",
    "title": "1: Conditional Probability",
    "section": "Current Research in Pedagogy",
    "text": "Current Research in Pedagogy\n\n\n\n\n\nactive learning\ncomputer programming\nflipped classrooms"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#identity-statement",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#identity-statement",
    "title": "1: Conditional Probability",
    "section": "Identity Statement",
    "text": "Identity Statement\n\n\n\nOriginally from Los Angeles\nMath: easier to understand through graphs\nComputer Programming: years of experience with R, Python, MATLAB, PHP, HTML, etc.\nLearning: drawn to puzzles and manageable tasks\nPersonality: shy, introvert"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#textbook",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#textbook",
    "title": "1: Conditional Probability",
    "section": "Textbook",
    "text": "Textbook\n\n\nThis course will closely follow the Bayes Rules! textbook by Alicia A Johnson, Miles Q Ott, and Mine Dogucu. It is, in my opinion, the best blend of Bayesian thought, mathematical background, computer processes, and relevant applications. The authors have made the materials of their textbook available online at https://www.bayesrulesbook.com/\n\n\n\n\nBayes Rules"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#additional-reading",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#additional-reading",
    "title": "1: Conditional Probability",
    "section": "Additional Reading",
    "text": "Additional Reading\nThe following list of books is optional for student studies, but the instructor may use some materials to add depth and interest to the course.\n\n\n\n\n\n\nAdditional Reading\n\n\n\n\n\n\nStatistical Rethinking by Richard McElreath is the premier body of work in the field of Bayesian analysis. This resource is great for people who want to build a strong foundation in philosophy and theory in this branch of mathematics.\nBayesian Data Analysis by Andrew Gelman, et al., is the classic textbook (available online) in this field that is used in several university courses. The authors’ approach work well for people looking to quickly add Bayesian approaches to their research skills.\nBayesian Statistics the Fun Way by Will Kurt brings Bayesian notions to a broad audience and its presentation blends will with an introductory course in statistics.\nBayesian Thinking in Biostatistics by Gary L Rosner, et al., provides rigorous applications in bioinformatics along with strong software use."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#cooperative-classroom",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#cooperative-classroom",
    "title": "1: Conditional Probability",
    "section": "Cooperative Classroom",
    "text": "Cooperative Classroom\nLearning in a cooperative environment should be stimulating, demanding, and fair. Because this approach to learning is different from the competitive classroom structure that many other courses used to be based on, it is important for us to be clear about mutual expectations. Below are my expectations for students in this class. This set of expectations is intended to maximize debate and exchange of ideas in an atmosphere of mutual respect while preserving individual ownership of ideas and written words. If you feel you do not understand or cannot agree to these expectations, you should discuss this with your instructor and classmates.\n\nStudents are expected to work cooperatively with other members of the class and show respect for the ideas and contributions of other people.\nWhen working as part of a group, students should strive to be good contributors to the group, listen to others, not dominate, and recognize the contributions of others. Students should try to ensure that everyone in the group is welcome to contribute and recognize that everyone contributes in different ways to a group process.\nStudents should explore data, make observations, and develop inferences as part of a group. If you use material from published sources, you must provide appropriate attribution.\n\n\n\n(Students will be asked to acknowledge this document in an online form.)\nThis document has been adapted from Scientific Teaching by Jo Handelsman, Sarah Miller, and Christine Pfund\n\n\n\n\nScientific Teaching"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#pep-talk",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#pep-talk",
    "title": "1: Conditional Probability",
    "section": "Pep Talk",
    "text": "Pep Talk\nLearning R can be difficult at first—it is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you will be using like ggplot2—made this wise observation:\n\n\n\n\n\n\nWisdom from Hadley Wickham\n\n\n\n\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\n\n\nIf you are finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, ask questions … e-mail [Derek], etc. I promise you can do this.\n—Andrew Heiss, Georgia State University"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#inclusion-statement",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#inclusion-statement",
    "title": "1: Conditional Probability",
    "section": "Inclusion Statement",
    "text": "Inclusion Statement\nI value all students regardless of their background, country of origin, race, religion, ethnicity, gender, sexual orientation, disability status, etc. and am committed to providing a climate of excellence and inclusiveness within all aspects of the course. If there are aspects of your culture or identity that you would like to share with me as they relate to your success in this class, I am happy to meet to discuss. Likewise, if you have any concerns in this area or facing any special issues or challenges, you are encouraged to discuss the matter with me (set up a meeting by e-mail) with an assurance of full confidentiality (only exception being mandatory reporting of academic integrity code violations or sexual harassment)."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#setting",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#setting",
    "title": "1: Conditional Probability",
    "section": "Setting",
    "text": "Setting\n\n\nLet us visit the lands of Faerûn. To grossly simplify and introduce notions from Dungeons and Dragons, let us define the following random variables:\n\n\\(H\\): human\n\\(E\\): evil\n\nso that \\(H^{c}\\) is “non-human” and \\(E^c\\) is “not evil”.\n\n\n\n\nBaldur’s Gate 3\n\n\n\n\n\n\n\n\n\n\nUnicode Characters\n\n\n\n\n\nDerek wanted to make a note to himself here that the way to make accented letters in a markdown environment is to use unicode characters."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nContingency TableCode\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(E^{c}|H)\\) and \\(P(H^{c}|E)\\)\n\n\n\nalignment &lt;- c(\"evil\", \"not evil\")\nhuman &lt;- c(\"24\", \"30\")\nnon_human &lt;- c(\"88\", \"178\")\n\nbg3_df &lt;- data.frame(alignment, human, non_human)\n\nbg3_gt_table &lt;- bg3_df |&gt;\n  gt(rowname_col = \"alignment\") |&gt;\n  cols_align(align = \"right\", columns = alignment) |&gt;\n  cols_align(align = \"center\",  columns = c(human, non_human)) |&gt;\n  tab_style(locations = cells_body(columns = c(human, non_human)),\n            style = list(cell_fill(color = \"yellow\")))\n\nbg3_gt_table #display table"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#practice",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#practice",
    "title": "1: Conditional Probability",
    "section": "Practice",
    "text": "Practice\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(H|E)\\) and \\(P(E|H)\\). What do you observe about the results?"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#setting-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#setting-1",
    "title": "1: Conditional Probability",
    "section": "Setting",
    "text": "Setting\n\n\nDuring the Winter of 2024, Kaggle had a competition where programmers were asked to “create an energy prediction model of prosumers to reduce energy imbalance costs” based on data that included property information, historical weather, and forecasted weather.\nFor now, let us pretend to classify the results into “high energy” and “low energy” usage, where “positive” results correspond to the “high energy” prosumers.\n\n\nThis Kaggle competition was called “Enefit”, and it was created by Eesti Energia to model Estonian energy customers."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-1",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nConfusion MatrixCodeMetricsFormulas\n\n\nSuppose that a team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\ntrue positives: 59\ntrue negatives: 97\nfalse positives: 14\nfalse negatives: 28\n\n\n\n\nenergy_levels &lt;- c(\"high\", \"low\")\nhigh &lt;- c(59, 14)\nlow &lt;- c(28, 97)\n\nenefit_df &lt;- data.frame(energy_levels, high, low)\n\nenefit_gt &lt;- enefit_df |&gt;\n  gt(rowname_col = \"energy_levels\") |&gt;\n  cols_align(align = \"right\", columns = energy_levels) |&gt;\n  cols_align(align = \"center\",  columns = c(high, low)) |&gt;\n  tab_spanner(columns = c(high, low),\n              label = \"model predictions\") |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 1),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 2),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 2),\n            style = list(cell_fill(color = \"#FFB3B2\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 1),\n            style = list(cell_fill(color = \"#FFB3B2\")))\n\nenefit_gt #display table\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\n\n\\[\\text{accuracy } = \\frac{TP + TN}{TP + FN + FP + TN}\\] \\[\\text{sensitivity } = \\frac{TP}{TP + FN}\\] \\[\\text{specificity } = \\frac{TN}{FP + TN}\\] \\[\\text{F-score } = \\frac{2*TP}{2*TP + FN + FP}\\]\nSource: Wikipedia page on sensitivity and specificity\n\n\n\n\n\n\n\n\n\nRow Span\n\n\n\n\n\nWhen these lecture notes were written, there might not have been a function to have a label span multiple rows in the gt package. The left side of the confusion matrix should say “ground truth”"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#practice-1",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#practice-1",
    "title": "1: Conditional Probability",
    "section": "Practice",
    "text": "Practice\nSuppose that another team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n94\n80\n    low\n23\n939\n  \n  \n  \n\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\nProsecutor’s Fallacy\n\n\n\n\n\nFor events \\(A\\) and \\(B\\), the inverse conditional probabilities are almost never equal to each other\n\\[P(A|B) \\neq P(B|A)\\]\n\n\n\n\n\n\n\n\n\n(optional) Additional Resources\n\n\n\n\n\n\nClassical vs Frequentist vs Bayesian Probability by Trefor Bazett"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#classical-probability",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#classical-probability",
    "title": "1: Conditional Probability",
    "section": "Classical Probability",
    "text": "Classical Probability\n\nDefinitionExample\n\n\n\\[P(A) = \\frac{\\text{number of outcomes that are } A}{\\text{number of outcomes total}}\\]\n\n\nOverly simplistic example\nIt snowed during 2 of the 30 days of January in 2024. We can claim that it snows \\(\\frac{2}{30}\\) of the days in Princeton."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#frequentist-probablity",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#frequentist-probablity",
    "title": "1: Conditional Probability",
    "section": "Frequentist Probablity",
    "text": "Frequentist Probablity\n\nDefinitionExample\n\n\n\\[P(A) = \\lim_{n \\to \\infty} \\frac{\\text{number of outcomes that are } A}{\\text{number of outcomes total}}\\]\n\n\nOverly simplistic example\nIf we can model many months of weather in Princeton, we expect it to snow in about 3 percent of the days."
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-probability",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#bayesian-probability",
    "title": "1: Conditional Probability",
    "section": "Bayesian Probability",
    "text": "Bayesian Probability\n\nDefinitionExample\n\n\n\nprior belief: \\(P(A)\\)\nupdated belief: \\(P(A|B)\\)\n\n\n\nOverly simplistic example\nIt snowed during 2 of the 30 days of January in 2024. Could we update our probability calculations for snow?"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-3",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-3",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nContingency TableCode\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      human\n      non_human\n    \n  \n  \n    evil\n24\n88\n    not evil\n30\n178\n  \n  \n  \n\n\n\n\nCompute \\(P(E^{c}|H)\\) and \\(P(H^{c}|E)\\)\n\n\n\nalignment &lt;- c(\"evil\", \"not evil\")\nhuman &lt;- c(\"24\", \"30\")\nnon_human &lt;- c(\"88\", \"178\")\n\nbg3_df &lt;- data.frame(alignment, human, non_human)\n\nbg3_gt_table &lt;- bg3_df |&gt;\n  gt(rowname_col = \"alignment\") |&gt;\n  cols_align(align = \"right\", columns = alignment) |&gt;\n  cols_align(align = \"center\",  columns = c(human, non_human)) |&gt;\n  tab_style(locations = cells_body(columns = c(human, non_human)),\n            style = list(cell_fill(color = \"yellow\")))\n\nbg3_gt_table #display table"
  },
  {
    "objectID": "posts/01_conditional_probability/01_conditional_probability.html#example-4",
    "href": "posts/01_conditional_probability/01_conditional_probability.html#example-4",
    "title": "1: Conditional Probability",
    "section": "Example",
    "text": "Example\n\nConfusion MatrixCodeMetricsFormulas\n\n\nSuppose that a team of contestants built a machine learning model for this Kaggle competition and achieved the results seen in the confusion matrix below.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\ntrue positives: 59\ntrue negatives: 97\nfalse positives: 14\nfalse negatives: 28\n\n\n\n\nenergy_levels &lt;- c(\"high\", \"low\")\nhigh &lt;- c(59, 14)\nlow &lt;- c(28, 97)\n\nenefit_df &lt;- data.frame(energy_levels, high, low)\n\nenefit_gt &lt;- enefit_df |&gt;\n  gt(rowname_col = \"energy_levels\") |&gt;\n  cols_align(align = \"right\", columns = energy_levels) |&gt;\n  cols_align(align = \"center\",  columns = c(high, low)) |&gt;\n  tab_spanner(columns = c(high, low),\n              label = \"model predictions\") |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 1),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 2),\n            style = list(cell_fill(color = \"lightgreen\"))) |&gt;\n  tab_style(locations = cells_body(columns = high, rows = 2),\n            style = list(cell_fill(color = \"#FFB3B2\"))) |&gt;\n  tab_style(locations = cells_body(columns = low, rows = 1),\n            style = list(cell_fill(color = \"#FFB3B2\")))\n\nenefit_gt #display table\n\n\n\nFor this confusion matrix, compute the accuracy, sensitivity, specificity, and F-score for the results.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        model predictions\n      \n    \n    \n      high\n      low\n    \n  \n  \n    high\n59\n28\n    low\n14\n97\n  \n  \n  \n\n\n\n\n\n\n\\[\\text{accuracy } = \\frac{TP + TN}{TP + FN + FP + TN}\\] \\[\\text{sensitivity } = \\frac{TP}{TP + FN}\\] \\[\\text{specificity } = \\frac{TN}{FP + TN}\\] \\[\\text{F-score } = \\frac{2*TP}{2*TP + FN + FP}\\]\nSource: Wikipedia page on sensitivity and specificity\n\n\n\n\n\n\n\n\n\nRow Span\n\n\n\n\n\nWhen these lecture notes were written, there might not have been a function to have a label span multiple rows in the gt package. The left side of the confusion matrix should say “ground truth”"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html",
    "href": "posts/03_beta_binomial/03_beta_binomial.html",
    "title": "3: Beta-Binomial Models",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"gt\")\nlibrary(\"janitor\")\nlibrary(\"patchwork\")\nlibrary(\"skimr\")\nlibrary(\"tidyverse\")\n\ntips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#tips-data-set",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#tips-data-set",
    "title": "3: Beta-Binomial Models",
    "section": "Tips Data Set",
    "text": "Tips Data Set\n\nDescriptionGlanceStructureSkim\n\n\n\nsource: Kaggle\n\n“The data was reported in a collection of case studies for business statistics. Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing\n\ncontext: “One waiter recorded information about each tip he received over a period of a few months working in one restaurant. In all he recorded 244 tips.”\n\n\n\n\nhead(tips_df)\n\n# A tibble: 6 × 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n3       21.0  3.5  Male   No     Sun   Dinner     3\n4       23.7  3.31 Male   No     Sun   Dinner     2\n5       24.6  3.61 Female No     Sun   Dinner     4\n6       25.3  4.71 Male   No     Sun   Dinner     4\n\n\n\n\n\nstr(tips_df, give.attr = FALSE)\n\nspc_tbl_ [244 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ total_bill: num [1:244] 17 10.3 21 23.7 24.6 ...\n $ tip       : num [1:244] 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ...\n $ sex       : chr [1:244] \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ smoker    : chr [1:244] \"No\" \"No\" \"No\" \"No\" ...\n $ day       : chr [1:244] \"Sun\" \"Sun\" \"Sun\" \"Sun\" ...\n $ time      : chr [1:244] \"Dinner\" \"Dinner\" \"Dinner\" \"Dinner\" ...\n $ size      : num [1:244] 2 3 3 2 4 4 2 4 2 2 ...\n\n\n\n\n\nskimr::skim(tips_df)\n\n\nData summary\n\n\nName\ntips_df\n\n\nNumber of rows\n244\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\nsmoker\n0\n1\n2\n3\n0\n2\n0\n\n\nday\n0\n1\n3\n4\n0\n4\n0\n\n\ntime\n0\n1\n5\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntotal_bill\n0\n1\n19.79\n8.90\n3.07\n13.35\n17.8\n24.13\n50.81\n▃▇▃▁▁\n\n\ntip\n0\n1\n3.00\n1.38\n1.00\n2.00\n2.9\n3.56\n10.00\n▇▆▂▁▁\n\n\nsize\n0\n1\n2.57\n0.95\n1.00\n2.00\n2.0\n3.00\n6.00\n▇▂▂▁▁"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#prior-model",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#prior-model",
    "title": "3: Beta-Binomial Models",
    "section": "Prior Model",
    "text": "Prior Model\n\n\n\n\\(\\pi\\)\n0.25\n0.50\n0.75\ntotal\n\n\n\n\n\\(f(\\pi)\\)\n1/3\n1/3\n1/3\n1\n\n\n\n\nuniform prior\ne.g. guessing the probability that the percentage of customers that smoked was 75% was \\(\\frac{1}{3}\\)\n\n\n\n\n\n\n\nDiscrete Probability Model\n\n\n\n\n\nLet \\(Y\\) be a discrete random variable. The probability model of \\(Y\\) is specified by a probability mass function (pmf) \\(f(y)\\). This pmf defines the probability of any given outcome \\(y\\),\n\\[f(y) = P(Y = y)\\]\n\n\\(0 \\leq f(y) \\leq 1\\)\n\\(\\sum f(y) = 1\\)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#observed-data",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#observed-data",
    "title": "3: Beta-Binomial Models",
    "section": "Observed Data",
    "text": "Observed Data\n\nObserved SampleCode\n\n\nLooking at the last 9 observations in the data set, 4 of the customers were smokers.\n\n\n\n\n\n\n  \n    \n    \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    10.07\n1.25\nMale\nNo\nSat\nDinner\n2\n    12.60\n1.00\nMale\nYes\nSat\nDinner\n2\n    32.83\n1.17\nMale\nYes\nSat\nDinner\n2\n    35.83\n4.67\nFemale\nNo\nSat\nDinner\n3\n    29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n    27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n    22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n    17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n    18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n  \n  \n  \n\n\n\n\n\n\n\ntail(tips_df, 9) |&gt;\n  gt() |&gt;\n  tab_style(locations = cells_body(columns = smoker),\n            style = list(cell_fill(color = \"gray80\"))) |&gt;\n  tab_style(locations = cells_body(columns = smoker,\n                                   rows = smoker == \"Yes\"),\n            style = list(cell_text(color = \"red\")))"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#binomial-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#binomial-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\n\nLet random variable \\(Y\\) be the number of successes in a fixed number of trials \\(n\\). Assume that the trials are independent and that the probability of success in each trial is \\(\\pi\\). Then the conditional dependence of \\(Y\\) on \\(\\pi\\) can be modeled by the Binomial model with parameters \\(n\\) and \\(\\pi\\). In mathematical notation:\n\\[Y|\\pi \\sim \\text{Bin}(n,\\pi)\\] where \\(\\sim\\) can be read as “modeled by”. Correspondingly, the binomial model is specified by the conditional pmf\n\\[f(y|\\pi) = \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y} \\text{ for } y \\in \\{0, 1, 2, ..., n\\}\\] where \\(\\binom{n}{y} = \\displaystyle\\frac{n!}{y!(n-y)!}\\)\n\n\n\nIn this example of \\(Y\\) smokers in \\(n=9\\) customers with probability \\(\\pi\\) of smokers,\n\\[Y|\\pi \\sim \\text{Bin}(9,\\pi)\\] \\[f(y|\\pi) = \\binom{9}{y}\\pi^{y}(1-\\pi)^{9-y} \\text{ for } y \\in \\{0, 1, 2, ..., 9\\}\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#conditional-pmfs",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#conditional-pmfs",
    "title": "3: Beta-Binomial Models",
    "section": "Conditional PMFs",
    "text": "Conditional PMFs\n\nBased on Observed DataCode\n\n\n\n\n\n\n\n\n\n\nhighlight_col &lt;- 0:9 == 4\ndf_25 &lt;- data.frame(k = 0:9, f_y_pi = dbinom(0:9, 9, 0.25), highlight_col)\ndf_50 &lt;- data.frame(k = 0:9, f_y_pi = dbinom(0:9, 9, 0.50), highlight_col)\ndf_75 &lt;- data.frame(k = 0:9, f_y_pi = dbinom(0:9, 9, 0.75), highlight_col)\n\nplot_25 &lt;- df_25 |&gt;\n  ggplot(aes(x = k, y = f_y_pi, fill = highlight_col)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Bin(9,0.25)\") +\n  scale_x_continuous(name = \"customers\", \n                   breaks = 0:9, \n                   labels = as.character(0:9)) +\n  theme_minimal()\n\nplot_50 &lt;- df_50 |&gt;\n  ggplot(aes(x = k, y = f_y_pi, fill = highlight_col)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Bin(9,0.50)\") +\n  scale_x_continuous(name = \"customers\", \n                   breaks = 0:9, \n                   labels = as.character(0:9)) +\n  theme_minimal()\n\nplot_75 &lt;- df_75 |&gt;\n  ggplot(aes(x = k, y = f_y_pi, fill = highlight_col)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Bin(9,0.75)\") +\n  scale_x_continuous(name = \"customers\", \n                   breaks = 0:9, \n                   labels = as.character(0:9)) +\n  theme_minimal()\n\n# patchwork\nplot_25 + plot_50 + plot_75"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#likelihoods",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#likelihoods",
    "title": "3: Beta-Binomial Models",
    "section": "Likelihoods",
    "text": "Likelihoods\nWith the observed data \\(Y = 4\\) out of \\(n = 9\\) customers, for \\(\\pi = \\{0.25, 0.50, 0.75\\}\\), \\[L(\\pi|y = 4) = f(y = 4|\\pi) = \\binom{9}{4}\\pi^{4}(1-\\pi)^{5}\\]\n\\[L(\\pi = 0.25|y = 4) = \\binom{9}{4}(0.25)^{4}(1-0.25)^{5} \\approx 0.1168\\] \\[L(\\pi = 0.50|y = 4) = \\binom{9}{4}(0.50)^{4}(1-0.50)^{5} \\approx 0.2461\\] \\[L(\\pi = 0.75|y = 4) = \\binom{9}{4}(0.75)^{4}(1-0.75)^{5} \\approx 0.0389\\]\n\n\n\n\\(\\pi\\)\n0.25\n0.50\n0.75\ntotal\n\n\n\n\n\\(f(\\pi)\\)\n1/3\n1/3\n1/3\n1\n\n\n\\(L(\\pi|y=4)\\)\n0.1168\n0.2461\n0.0389\n0.4018"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#bayesian-concepts",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#bayesian-concepts",
    "title": "3: Beta-Binomial Models",
    "section": "Bayesian Concepts",
    "text": "Bayesian Concepts\n\\[\\text{posterior} = \\frac{\\text{prior} * \\text{likelihood}}{\\text{normalizing constant}}\\]\nFor observations \\(\\vec{y}\\) and probabilities \\(\\vec{\\pi}\\),\n\\[f(\\pi|y) = \\frac{f(\\pi)L(\\pi|y)}{f(y)} \\propto f(\\pi)L(\\pi|y)\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#normalizing-constant",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#normalizing-constant",
    "title": "3: Beta-Binomial Models",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\\[f(y = 4) = \\displaystyle\\sum_{\\pi\\in\\{0.25, 0.50, 0.75\\}} L(\\pi|y=4) \\cdot f(\\pi)\\]\n\\[f(y = 4) = \\displaystyle\\frac{0.1168}{3} + \\displaystyle\\frac{0.2461}{3} + \\displaystyle\\frac{0.0389}{3} \\approx 0.1339\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#posterior-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#posterior-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\\[f(\\pi|y=4) = \\displaystyle\\frac{f(\\pi)L(\\pi|y=4)}{f(y=4)} \\text{ for } \\pi \\in \\{0.25, 0.50, 0.75 \\}\\] \\[f(\\pi=0.25|y=4) = \\displaystyle\\frac{(1/3)(0.1168)}{0.1339} \\approx 0.2907\\] \\[f(\\pi=0.50|y=4) = \\displaystyle\\frac{(1/3)(0.2461)}{0.1339} \\approx 0.6126\\] \\[f(\\pi=0.75|y=4) = \\displaystyle\\frac{(1/3)(0.0389)}{0.1339} \\approx 0.0968\\]\n\n\n\n\\(\\pi\\)\n0.25\n0.50\n0.75\ntotal\n\n\n\n\n\\(f(\\pi)\\)\n1/3\n1/3\n1/3\n1\n\n\n\\(L(\\pi|y=4)\\)\n0.1168\n0.2461\n0.0389\n0.4018\n\n\n\\(f(\\pi|y=4)\\)\n0.2907\n0.6126\n0.0968\n1"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#computer-simulation",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#computer-simulation",
    "title": "3: Beta-Binomial Models",
    "section": "Computer Simulation",
    "text": "Computer Simulation\n\nSimulation SamplesVerify PriorPMFsPosterior Distribution\n\n\n\n# define possible smoker proportions\nsmokers &lt;- data.frame(pi = c(0.25, 0.50, 0.75))\n\n# define prior model\nprior &lt;- c(1/3, 1/3, 1/3)\n\n# simulate 10000 values of pi from the prior\nset.seed(320)\nsmoker_sim &lt;- sample_n(smokers, size = 10000, weight = prior, replace = TRUE)\n\n# simulate 10000 samples of customers\nsmoker_sim &lt;- smoker_sim |&gt;\n  mutate(y = rbinom(10000, size = 9, prob = pi))\n\nSo far, the simulation yields a data frame that looks like\n\nhead(smoker_sim)\n\n    pi y\n1 0.50 5\n2 0.25 2\n3 0.50 6\n4 0.25 3\n5 0.50 4\n6 0.50 4\n\n\n\n\n\n# summarize the prior\nsmoker_sim |&gt;\n  tabyl(pi) |&gt;\n  adorn_totals(\"row\")\n\n    pi     n percent\n  0.25  3283  0.3283\n   0.5  3345  0.3345\n  0.75  3372  0.3372\n Total 10000  1.0000\n\n\n\n\n\n# plot y by pi\nggplot(smoker_sim, aes(x = y)) + \n  stat_count(aes(y = after_stat(prop))) + \n  facet_wrap(~ pi)\n\n\n\n\n\n\n\n# focus on simulations with y = 4\nfour_smokers &lt;- smoker_sim %&gt;% \n  filter(y == 4)\n\n# summarize the posterior approximation\nfour_smokers %&gt;% \n  tabyl(pi) %&gt;% \n  adorn_totals(\"row\")\n\n    pi    n   percent\n  0.25  408 0.3000000\n   0.5  813 0.5977941\n  0.75  139 0.1022059\n Total 1360 1.0000000\n\n\n\n# plot the posterior approximation\nggplot(four_smokers, aes(x = pi)) + \n  geom_bar()"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#beta-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#beta-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Beta Distribution",
    "text": "Beta Distribution\n\n\n\n\n\n\nBeta Distribution\n\n\n\n\n\nLet \\(\\pi \\in [0,1]\\), then the variability in \\(\\pi\\) may be modeled by a Beta distribution with shape hyperparameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\)\n\\[\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\] with probability density function\n\\[f(\\pi) = \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\]\nwhere the gamma function\n\n\\(\\Gamma(z) = \\displaystyle\\int_{0}^{\\infty} \\! x^{z-1}e^{-x} \\, dx\\)\n\\(\\Gamma(z + 1) = z\\Gamma(z)\\)\n\n\n\n\n\n\n\n\n\n\nCorollary\n\n\n\n\n\nWhen \\(z\\) is a positive integer, then \\[\\Gamma(z) = (z-1)!\\] That is, the gamma function is a generalization of the factorial.\n\n\n\n\n\n\n\n\n\nHyperparameters\n\n\n\n\n\nA hyperparameter is a parameter used in a prior model.\n\n\n\n\n\n\n\n\n\nExplore!\n\n\n\n\n\nMatt Bognar at the University of Iowa created this great webapp to explore the beta distribution.\n\n\n\n\n\n\n\n\n\nUniform Distribution\n\n\n\n\n\nWhen it is equally plausible for \\(\\pi\\) to take on any value between zero and one, we can model \\(\\pi\\) by the standard uniform distribution\n\\[\\pi \\sim \\text{Unif}(0,1)\\]\nwith pdf \\(f(\\pi) = 1\\) for \\(\\pi \\in [0,1]\\). The \\(\\text{Unif}(0,1)\\) distribution is a special case of the beta distribution when \\(\\alpha = 1\\) and \\(\\beta = 1\\)\n\\[\\text{Unif}(0,1) = \\text{Beta}(1,1)\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#sample-statistics",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#sample-statistics",
    "title": "3: Beta-Binomial Models",
    "section": "Sample Statistics",
    "text": "Sample Statistics\nFor a beta distribution, \\(\\pi \\sim \\text{Beta}(\\alpha, \\beta)\\)\n\nexpected value: \\(\\text{E}(\\pi) = \\displaystyle\\frac{\\alpha}{\\alpha + \\beta}\\)\nvariance: \\(\\text{Var}(\\pi) = \\displaystyle\\frac{\\alpha\\beta}{(\\alpha + \\beta)^{2}(\\alpha + \\beta + 1)}\\)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#tuning-the-beta-prior",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#tuning-the-beta-prior",
    "title": "3: Beta-Binomial Models",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\nHere, let us use that sample of observations where 4 out of the 9 customers where smokers. We might then try to align this sample proportion \\(\\frac{4}{9}\\) with the expected value\n\\[\\displaystyle\\frac{\\alpha}{\\alpha + \\beta} = \\displaystyle\\frac{4}{9} \\quad\\rightarrow\\quad \\alpha = 4, \\quad \\beta = 5\\]\nto create a beta model \\(\\pi \\sim \\text{Beta}(4, 5)\\)\n\nbayesrules::plot_beta(4,5)\n\n\n\n\nWe can compute the variance\n\\[\\text{Var}(\\pi) = \\displaystyle\\frac{\\alpha\\beta}{(\\alpha + \\beta)^{2}(\\alpha + \\beta + 1)} = \\displaystyle\\frac{4 \\cdot 5}{(4 + 5)^{2}(4 + 5 + 1)} \\approx 0.0247\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#binomial-data-model",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#binomial-data-model",
    "title": "3: Beta-Binomial Models",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\nSuppose that we obtain a larger sample of observations with \\(n = 25\\) customers. The number of smokers, denoted by random variable \\(Y\\), may have a binomial model conditional on probability \\(\\pi\\),\n\\[Y|\\pi \\sim \\text{Bin}(25, \\pi)\\]\nwith conditional pmf over \\(y \\in \\{0, 1, ..., 25\\}\\), \\[f(y|\\pi) = P(Y = y|\\pi) = \\binom{25}{y}\\pi^{y}(1-\\pi)^{25-y}\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#likelihood",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#likelihood",
    "title": "3: Beta-Binomial Models",
    "section": "Likelihood",
    "text": "Likelihood\n\nFunctionPlotCode\n\n\nSuppose that in that sample of \\(n = 25\\) customers, we observe that \\(y = 7\\) of those customers were smokers. Our likelihood function is then\n\\[L(\\pi|y = 7) = \\binom{25}{7}\\pi^{7}(1-\\pi)^{18}\\]\n\n\n\n\n\n\n\n\n\n\npi &lt;- seq(0, 1, 0.01)\nL_pi_y &lt;- dbinom(7, 25, pi)\n\ndf_for_graph &lt;- data.frame(pi, L_pi_y)\n\ndf_for_graph |&gt;\n  ggplot(aes(x = pi, y = L_pi_y)) +\n  geom_line() +\n  labs(title = \"Likelihood function\",\n       subtitle = \"y = 7, n = 25\",\n       caption = \"SML 320\")"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#beta-binomial-model",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#beta-binomial-model",
    "title": "3: Beta-Binomial Models",
    "section": "Beta-Binomial Model",
    "text": "Beta-Binomial Model\nClaim: With probability \\(\\pi \\in [0,1]\\) and random variable \\(Y\\) representing the number of “successes” in \\(n\\) trials, if the behavior is modeled with prior distribution and likelihood\n\\[\\begin{array}{rcl}\n  \\pi & \\sim & \\text{Beta}(\\alpha, \\beta) \\\\\n  Y|\\pi & \\sim & \\text{Bin}(n,\\pi) \\\\\n\\end{array}\\]\nthen the posterior distribution can be modeled with an updated beta distribution\n\\[\\pi|(Y=y) \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\]\nwith sample statistics\n\\[\\begin{array}{rcl}\n  \\text{E}(\\pi|Y=y) & = & \\displaystyle\\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\n  \\text{Var}(\\pi|Y=y) & = & \\displaystyle\\frac{(\\alpha  +y)(\\beta + n - y)}{(\\alpha + \\beta + n)^{2}(\\alpha + \\beta + n + 1)} \\\\\n\\end{array}\\]\n\n\n\n\n\n\nPartial Proof\n\n\n\n\n\nWith the conditional pmf\n\\[f(\\pi) = \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\] and likelihood function\n\\[L(\\pi|y) = \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y}\\]\nit follows from Bayes’ Rule that the posterior distribution\n\\[\\begin{array}{rcl}\n  f(\\pi|y) & \\propto & f(\\pi)L(\\pi|y) \\\\\n  ~ & = & \\displaystyle\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\cdot \\binom{n}{y}\\pi^{y}(1-\\pi)^{n-y} \\\\\n  ~ & \\propto & \\pi^{(\\alpha + y)-1}(1-\\pi)^{(\\beta+n-y)-1} \\\\\n\\end{array}\\]\nwhere that last expression is the unnormalized posterior pdf. We observe that it has the same structure of the normalized \\(\\text{Beta}(\\alpha + y, \\beta + n - y)\\) pdf\n\\[f(\\pi|y) = \\displaystyle\\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+y)\\Gamma(\\beta+n-y)} \\pi^{(\\alpha + y)-1}(1-\\pi)^{(\\beta+n-y)-1}\\]"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#beta-posterior",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#beta-posterior",
    "title": "3: Beta-Binomial Models",
    "section": "Beta Posterior",
    "text": "Beta Posterior\n\nUpdated DistributionPlotBoth\n\n\nBy the above theory, having started with a \\(\\text{Beta}{(4,5)}\\) prior, and then observing \\(y = 7\\) smokers among \\(n = 25\\) customers\n\\[\\alpha = 4, \\quad \\beta = 5, \\quad y = 7, \\quad n = 25\\]\nour posterior distribution can be modeled with\n\\[\\pi|(Y=y) \\sim \\text{Beta}(\\alpha + y, \\beta + n - y) = \\text{Beta}(11, 23)\\]\n\n\n\nbayesrules::plot_beta(11,23)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial.html#putting-it-all-together",
    "href": "posts/03_beta_binomial/03_beta_binomial.html#putting-it-all-together",
    "title": "3: Beta-Binomial Models",
    "section": "Putting it All Together",
    "text": "Putting it All Together\n\nHelper FunctionsTablePlot\n\n\nThe bayesrules package (from the textbook authors) provide additonal helper functions for this procedure of modeling with a beta-binomial model.\nbayesrules::summarize_beta_binomial(alpha, beta, y, n)\nbayesrules::plot_beta_binomial(alpha, beta, y, n)\n\n\n\nsummarize_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    5 0.4444 0.4286 0.0247 0.1571\n2 posterior    11   23 0.3235 0.3125 0.0063 0.0791\n\n\n\n\n\nplot_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html",
    "title": "3: Beta-Binomial Models",
    "section": "",
    "text": "Today, let’s see if the following R code runs on your computer.\nRemember to install packages as needed.\nlibrary(\"bayesrules\")\nlibrary(\"gt\")\nlibrary(\"janitor\")\nlibrary(\"patchwork\")\nlibrary(\"skimr\")\nlibrary(\"tidyverse\")\nAlso, place the tips.csv file in the same directory as this script.\ntips_df &lt;- readr::read_csv(\"tips.csv\")\n\nRows: 244 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): sex, smoker, day, time\ndbl (3): total_bill, tip, size\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#simulation-samples",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#simulation-samples",
    "title": "3: Beta-Binomial Models",
    "section": "Simulation Samples",
    "text": "Simulation Samples\n\n# define possible smoker proportions\nsmokers &lt;- data.frame(pi = c(0.25, 0.50, 0.75))\n\n# define prior model\nprior &lt;- c(1/3, 1/3, 1/3)\n\n# simulate 10000 values of pi from the prior\nset.seed(320)\nsmoker_sim &lt;- sample_n(smokers, size = 10000, weight = prior, replace = TRUE)\n\n# simulate 10000 samples of customers\nsmoker_sim &lt;- smoker_sim |&gt;\n  mutate(y = rbinom(10000, size = 9, prob = pi))"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#pmfs",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#pmfs",
    "title": "3: Beta-Binomial Models",
    "section": "PMFs",
    "text": "PMFs\n\n# plot y by pi\nggplot(smoker_sim, aes(x = y)) + \n  stat_count(aes(y = after_stat(prop))) + \n  facet_wrap(~ pi)"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#posterior-distribution",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#posterior-distribution",
    "title": "3: Beta-Binomial Models",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\n\n# focus on simulations with y = 4\nfour_smokers &lt;- smoker_sim %&gt;% \n  filter(y == 4)\n\n# summarize the posterior approximation\nfour_smokers %&gt;% \n  tabyl(pi) %&gt;% \n  adorn_totals(\"row\")\n\n    pi    n   percent\n  0.25  408 0.3000000\n   0.5  813 0.5977941\n  0.75  139 0.1022059\n Total 1360 1.0000000\n\n\n\n# plot the posterior approximation\nggplot(four_smokers, aes(x = pi)) + \n  geom_bar()"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#table",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#table",
    "title": "3: Beta-Binomial Models",
    "section": "Table",
    "text": "Table\n\nsummarize_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    5 0.4444 0.4286 0.0247 0.1571\n2 posterior    11   23 0.3235 0.3125 0.0063 0.0791"
  },
  {
    "objectID": "posts/03_beta_binomial/03_beta_binomial_template.html#plot",
    "href": "posts/03_beta_binomial/03_beta_binomial_template.html#plot",
    "title": "3: Beta-Binomial Models",
    "section": "Plot",
    "text": "Plot\n\nplot_beta_binomial(alpha = 4, beta = 5, y = 7, n = 25) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html",
    "title": "4: Balance and Sequentiality",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\ntips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#journey-so-far",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#journey-so-far",
    "title": "4: Balance and Sequentiality",
    "section": "Journey so far",
    "text": "Journey so far\nWith hopes of learning more about a target probability \\[\\pi \\in [0,1]\\] we have been applying beta-binomial models\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Beta}(\\alpha, \\beta) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Bin}(y, \\pi) \\\\\n  \\text{posterior: } & \\pi|(Y = y) & \\sim & \\text{Beta}(\\alpha + y, \\beta + n - y) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta",
    "title": "4: Balance and Sequentiality",
    "section": "Using plot_beta",
    "text": "Using plot_beta\n\nInclinationsCode\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesrules::plot_beta(4,8) +\n  labs(title = \"Beta(4,8), E(pi) = 1/3\")\np2 &lt;- bayesrules::plot_beta(6,6) +\n  labs(title = \"Beta(6,6), E(pi) = 1/2\")\np3 &lt;- bayesrules::plot_beta(8,4) +\n  labs(title = \"Beta(8,4), E(pi) = 2/3\")\n\n# patchwork\np1 / p2 / p3"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-thursday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-thursday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Thursday Customers",
    "text": "Subset: Thursday Customers\n\nThursday &lt;- tips_df |&gt;\n  filter(day == \"Thur\")\n\nn_Thursday &lt;- nrow(Thursday)\ny_Thursday &lt;- Thursday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta_binomial",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#using-plot_beta_binomial",
    "title": "4: Balance and Sequentiality",
    "section": "Using plot_beta_binomial",
    "text": "Using plot_beta_binomial\n\nCodePlotsStatistics\n\n\n\np1 &lt;- bayesrules::plot_beta_binomial(4, 8, y_Thursday, n_Thursday) +\n  labs(title = \"Beta(4,8) prior\")\np2 &lt;- bayesrules::plot_beta_binomial(6, 6, y_Thursday, n_Thursday) +\n  labs(title = \"Beta(6,6) prior\")\np3 &lt;- bayesrules::plot_beta_binomial(8, 4, y_Thursday, n_Thursday) +\n  labs(title = \"Beta(8,4) prior\")\n\n# patchwork\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_beta_binomial(4, 8, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    8 0.3333 0.3000 0.0171 0.1307\n2 posterior    21   53 0.2838 0.2778 0.0027 0.0521\n\n\n\nbayesrules::summarize_beta_binomial(6, 6, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     6    6 0.5000 0.5000 0.0192 0.1387\n2 posterior    23   51 0.3108 0.3056 0.0029 0.0534\n\n\n\nbayesrules::summarize_beta_binomial(8, 4, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     8    4 0.6667 0.7000 0.0171 0.1307\n2 posterior    25   49 0.3378 0.3333 0.0030 0.0546"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#certainty",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#certainty",
    "title": "4: Balance and Sequentiality",
    "section": "Certainty",
    "text": "Certainty\n\nInclinationsCode\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesrules::plot_beta(1,1) +\n  labs(title = \"Uniform Prior: Beta(1,1), var(pi) = 0.0833\")\np2 &lt;- bayesrules::plot_beta(4,4) +\n  labs(title = \"Vague Prior: Beta(4,4), var(pi) = 0.0278\")\np3 &lt;- bayesrules::plot_beta(16,16) +\n  labs(title = \"Informative Prior: Beta(16,16), var(pi) = 0.0076\")\n\n# patchwork\np1 / p2 / p3"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-friday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-friday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Friday Customers",
    "text": "Subset: Friday Customers\n\nFriday &lt;- tips_df |&gt;\n  filter(day == \"Fri\")\n\nn_Friday &lt;- nrow(Friday)\ny_Friday &lt;- Friday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#applying-the-friday-crowd",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#applying-the-friday-crowd",
    "title": "4: Balance and Sequentiality",
    "section": "Applying the Friday Crowd",
    "text": "Applying the Friday Crowd\n\nCodePlotsStatistics\n\n\n\np1 &lt;- bayesrules::plot_beta_binomial(1, 1, y_Friday, n_Friday) +\n  labs(title = \"Uniform Prior: Beta(1,1)\")\np2 &lt;- bayesrules::plot_beta_binomial(4, 4, y_Friday, n_Friday) +\n  labs(title = \"Vague Prior: Beta(4,4)\")\np3 &lt;- bayesrules::plot_beta_binomial(16, 16, y_Friday, n_Friday) +\n  labs(title = \"Informative Prior: Beta(16,16)\")\n\n# patchwork\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_beta_binomial(1, 1, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1    1 0.5000    NaN 0.0833 0.2887\n2 posterior    16    5 0.7619 0.7895 0.0082 0.0908\n\n\n\nbayesrules::summarize_beta_binomial(4, 4, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean mode    var     sd\n1     prior     4    4 0.5000 0.50 0.0278 0.1667\n2 posterior    19    8 0.7037 0.72 0.0074 0.0863\n\n\n\nbayesrules::summarize_beta_binomial(16, 16, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    16   16 0.5000 0.5000 0.0076 0.0870\n2 posterior    31   20 0.6078 0.6122 0.0046 0.0677"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#skewed-prior",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#skewed-prior",
    "title": "4: Balance and Sequentiality",
    "section": "Skewed Prior",
    "text": "Skewed Prior\n\nFixed PriorCode\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_beta(1,32) +\n  labs(title = \"Skewed Prior: Beta(1,32), var(pi) = 0.0009\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-saturday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-saturday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Saturday Customers",
    "text": "Subset: Saturday Customers\n\nSaturday &lt;- tips_df |&gt;\n  filter(day == \"Sat\")\n\nn_Saturday &lt;- nrow(Saturday)\ny_Saturday &lt;- Saturday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#friday-versus-saturday",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#friday-versus-saturday",
    "title": "4: Balance and Sequentiality",
    "section": "Friday versus Saturday",
    "text": "Friday versus Saturday\n\nCodePlotsStatistics\n\n\n\np1 &lt;- bayesrules::plot_beta_binomial(1, 32, y_Friday, n_Friday) +\n  labs(title = \"Friday Customers\")\np2 &lt;- bayesrules::plot_beta_binomial(1, 32, y_Saturday, n_Saturday) +\n  labs(title = \"Saturday Customers\")\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_beta_binomial(1, 32, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean mode   var     sd\n1     prior     1   32 0.0303  0.0 9e-04 0.0294\n2 posterior    16   36 0.3077  0.3 4e-03 0.0634\n\n\n\nbayesrules::summarize_beta_binomial(1, 32, y_Saturday, n_Saturday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1   32 0.0303 0.0000 0.0009 0.0294\n2 posterior    43   77 0.3583 0.3559 0.0019 0.0436"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#vague-prior-revisited",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#vague-prior-revisited",
    "title": "4: Balance and Sequentiality",
    "section": "Vague Prior Revisited",
    "text": "Vague Prior Revisited\n\nVague PriorCode\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_beta(4,4) +\n  labs(title = \"Vague Prior: Beta(4,4), var(pi) = 0.0278\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subsets-lunch-and-dinner",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subsets-lunch-and-dinner",
    "title": "4: Balance and Sequentiality",
    "section": "Subsets: Lunch and Dinner",
    "text": "Subsets: Lunch and Dinner\n\n\n\nLunch &lt;- tips_df |&gt;\n  filter(time == \"Lunch\")\n\nn_Lunch &lt;- nrow(Lunch)\ny_Lunch &lt;- Lunch |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()\n\n\n\nDinner &lt;- tips_df |&gt;\n  filter(time == \"Dinner\")\n\nn_Dinner &lt;- nrow(Dinner)\ny_Dinner &lt;- Dinner |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#lunch-first-then-dinner",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#lunch-first-then-dinner",
    "title": "4: Balance and Sequentiality",
    "section": "Lunch First, then Dinner",
    "text": "Lunch First, then Dinner\n\nCodePlotsStatistics\n\n\n\nalpha_1 &lt;- 4\nbeta_1  &lt;- 4\np1 &lt;- bayesrules::plot_beta_binomial(alpha_1, beta_1, y_Lunch, n_Lunch) +\n  labs(title = \"Lunch First\")\n\nalpha_2 &lt;- alpha_1 + y_Lunch\nbeta_2  &lt;- beta_1 + n_Lunch - y_Lunch\np2 &lt;- bayesrules::plot_beta_binomial(alpha_2, beta_2, y_Dinner, n_Dinner) +\n  labs(title = \"then Dinner\")\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nLunch First\n\nbayesrules::summarize_beta_binomial(alpha_1, beta_1, y_Lunch, n_Lunch) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    4 0.5000 0.5000 0.0278 0.1667\n2 posterior    27   49 0.3553 0.3514 0.0030 0.0545\n\n\n\n\nthen Dinner\n\nbayesrules::summarize_beta_binomial(alpha_2, beta_2, y_Dinner, n_Dinner) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode   var     sd\n1     prior    27   49 0.3553 0.3514 3e-03 0.0545\n2 posterior    97  155 0.3849 0.3840 9e-04 0.0306"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#dinner-first-then-lunch",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#dinner-first-then-lunch",
    "title": "4: Balance and Sequentiality",
    "section": "Dinner First, then Lunch",
    "text": "Dinner First, then Lunch\n\nCodePlotsStatistics\n\n\n\nalpha_1 &lt;- 4\nbeta_1  &lt;- 4\np1 &lt;- bayesrules::plot_beta_binomial(alpha_1, beta_1, y_Dinner, n_Dinner) +\n  labs(title = \"Dinner First\")\n\nalpha_2 &lt;- alpha_1 + y_Dinner\nbeta_2  &lt;- beta_1 + n_Dinner - y_Dinner\np2 &lt;- bayesrules::plot_beta_binomial(alpha_2, beta_2, y_Lunch, n_Lunch) +\n  labs(title = \"then Lunch\")\n\n# patchwork\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\nDinner First\n\nbayesrules::summarize_beta_binomial(alpha_1, beta_1, y_Dinner, n_Dinner) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     4    4 0.5000 0.5000 0.0278 0.1667\n2 posterior    74  110 0.4022 0.4011 0.0013 0.0361\n\n\n\n\nthen Lunch\n\nbayesrules::summarize_beta_binomial(alpha_2, beta_2, y_Lunch, n_Lunch) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    74  110 0.4022 0.4011 0.0013 0.0361\n2 posterior    97  155 0.3849 0.3840 0.0009 0.0306"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#data-invariance",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#data-invariance",
    "title": "4: Balance and Sequentiality",
    "section": "Data Invariance",
    "text": "Data Invariance\nLet \\(\\theta\\) be any parameter of interest with prior pdf \\(f(\\theta)\\). Then a sequential analysis in which we first observe a data point \\(y_{1}\\) and then a second data point \\(y_{2}\\) will produce the same posterior model of \\(\\theta\\) as if we first observe \\(y_{2}\\) and then \\(y_{1}\\):\n\\[f(\\theta|y1,y2)=f(\\theta|y2,y1)\\]\nSimilarly, the posterior model is invariant to whether we observe the data all at once or sequentially.\n\n\n\n\n\n\nproof\n\n\n\n\n\n[Please refer to section 4.5 of the textbook]"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#uniform-prior-revisited",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#uniform-prior-revisited",
    "title": "4: Balance and Sequentiality",
    "section": "Uniform Prior Revisited",
    "text": "Uniform Prior Revisited\n\nUniform PriorCode\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_beta(1,1) +\n  labs(title = \"Uniform Prior: Beta(1,1), var(pi) = 0.0833\")"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-sunday-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-sunday-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: Sunday Customers",
    "text": "Subset: Sunday Customers\n\nSunday &lt;- tips_df |&gt;\n  filter(day == \"Sun\")\n\nn_Sunday &lt;- nrow(Sunday)\ny_Sunday &lt;- Sunday |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#one-day-at-a-time",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#one-day-at-a-time",
    "title": "4: Balance and Sequentiality",
    "section": "One Day at a Time",
    "text": "One Day at a Time\n\nCodePlotsStatistics\n\n\n\nalpha &lt;- 1\nbeta  &lt;- 1\np1 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Thursday, n_Thursday) +\n  labs(title = \"Thursday First\")\n\nalpha &lt;- alpha + y_Thursday\nbeta  &lt;- beta + n_Thursday - y_Thursday\np2 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Friday, n_Friday) +\n  labs(title = \"then Friday\")\n\nalpha &lt;- alpha + y_Friday\nbeta  &lt;- beta + n_Friday - y_Friday\np3 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Saturday, n_Saturday) +\n  labs(title = \"then Saturday\")\n\nalpha &lt;- alpha + y_Saturday\nbeta  &lt;- beta + n_Saturday - y_Saturday\np4 &lt;- bayesrules::plot_beta_binomial(alpha, beta, y_Sunday, n_Sunday) +\n  labs(title = \"then Sunday\")\n\n# patchwork\np1 / p2 / p3 / p4\n\n\n\n\n\n\n\n\n\n\n\nThursday First\n\nalpha &lt;- 1\nbeta  &lt;- 1\nbayesrules::summarize_beta_binomial(alpha, beta, y_Thursday, n_Thursday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1    1 0.5000    NaN 0.0833 0.2887\n2 posterior    18   46 0.2812 0.2742 0.0031 0.0558\n\n\n\n\nthen Friday\n\nalpha &lt;- alpha + y_Thursday\nbeta  &lt;- beta + n_Thursday - y_Thursday\nbayesrules::summarize_beta_binomial(alpha, beta, y_Friday, n_Friday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    18   46 0.2812 0.2742 0.0031 0.0558\n2 posterior    33   50 0.3976 0.3951 0.0029 0.0534\n\n\n\n\nthen Saturday\n\nalpha &lt;- alpha + y_Friday\nbeta  &lt;- beta + n_Friday - y_Friday\nbayesrules::summarize_beta_binomial(alpha, beta, y_Saturday, n_Saturday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    33   50 0.3976 0.3951 0.0029 0.0534\n2 posterior    75   95 0.4412 0.4405 0.0014 0.0380\n\n\n\n\nthen Sunday\n\nalpha &lt;- alpha + y_Saturday\nbeta  &lt;- beta + n_Saturday - y_Saturday\nbayesrules::summarize_beta_binomial(alpha, beta, y_Sunday, n_Sunday) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior    75   95 0.4412 0.4405 0.0014 0.0380\n2 posterior    94  152 0.3821 0.3811 0.0010 0.0309"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-all-customers",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#subset-all-customers",
    "title": "4: Balance and Sequentiality",
    "section": "Subset: All Customers",
    "text": "Subset: All Customers\n\nn_all &lt;- nrow(tips_df)\ny_all &lt;- tips_df |&gt; \n  filter(smoker == \"Yes\") |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/04_balance_sequentiality/04_balance_sequentiality.html#all-at-once",
    "href": "posts/04_balance_sequentiality/04_balance_sequentiality.html#all-at-once",
    "title": "4: Balance and Sequentiality",
    "section": "All at Once",
    "text": "All at Once\n\nCodePlotStatistics\n\n\n\nalpha &lt;- 1\nbeta  &lt;- 1\nbayesrules::plot_beta_binomial(alpha, beta, y_all, n_all) +\n  labs(title = \"All at Once\")\n\n\n\n\n\n\n\n\n\n\n\nalpha &lt;- 1\nbeta  &lt;- 1\nbayesrules::summarize_beta_binomial(alpha, beta, y_all, n_all) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model alpha beta   mean   mode    var     sd\n1     prior     1    1 0.5000    NaN 0.0833 0.2887\n2 posterior    94  152 0.3821 0.3811 0.0010 0.0309\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2  forcats_1.0.0    stringr_1.5.0    dplyr_1.1.3     \n [5] purrr_1.0.2      readr_2.1.4      tidyr_1.3.0      tibble_3.2.1    \n [9] ggplot2_3.4.3    tidyverse_2.0.0  patchwork_1.1.2  bayesrules_0.0.2\n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3       inline_0.3.19       rlang_1.1.1        \n  [4] magrittr_2.0.3      snakecase_0.11.0    matrixStats_1.0.0  \n  [7] e1071_1.7-13        compiler_4.3.0      loo_2.6.0          \n [10] callr_3.7.3         vctrs_0.6.3         reshape2_1.4.4     \n [13] pkgconfig_2.0.3     crayon_1.5.2        fastmap_1.1.1      \n [16] ellipsis_0.3.2      labeling_0.4.3      utf8_1.2.3         \n [19] threejs_0.3.3       promises_1.2.1      rmarkdown_2.24     \n [22] tzdb_0.4.0          markdown_1.8        ps_1.7.5           \n [25] nloptr_2.0.3        bit_4.0.5           xfun_0.40          \n [28] jsonlite_1.8.7      later_1.3.1         parallel_4.3.0     \n [31] prettyunits_1.1.1   R6_2.5.1            dygraphs_1.1.1.6   \n [34] stringi_1.7.12      StanHeaders_2.26.26 boot_1.3-28.1      \n [37] Rcpp_1.0.11         rstan_2.21.8        knitr_1.43         \n [40] zoo_1.8-12          base64enc_0.1-3     bayesplot_1.10.0   \n [43] httpuv_1.6.11       Matrix_1.5-4        splines_4.3.0      \n [46] igraph_1.4.3        timechange_0.2.0    tidyselect_1.2.0   \n [49] rstudioapi_0.15.0   yaml_2.3.7          codetools_0.2-19   \n [52] miniUI_0.1.1.1      processx_3.8.1      pkgbuild_1.4.0     \n [55] lattice_0.21-8      plyr_1.8.8          shiny_1.7.5        \n [58] withr_2.5.2         groupdata2_2.0.2    evaluate_0.21      \n [61] survival_3.5-5      proxy_0.4-27        RcppParallel_5.1.7 \n [64] xts_0.13.1          pillar_1.9.0        DT_0.28            \n [67] stats4_4.3.0        shinyjs_2.1.0       generics_0.1.3     \n [70] vroom_1.6.3         hms_1.1.3           rstantools_2.3.1   \n [73] munsell_0.5.0       scales_1.2.1        minqa_1.2.5        \n [76] gtools_3.9.4        xtable_1.8-4        class_7.3-21       \n [79] glue_1.6.2          janitor_2.2.0       tools_4.3.0        \n [82] shinystan_2.6.0     lme4_1.1-33         colourpicker_1.2.0 \n [85] grid_4.3.0          crosstalk_1.2.0     colorspace_2.1-0   \n [88] nlme_3.1-162        cli_3.6.1           fansi_1.0.4        \n [91] gtable_0.3.4        digest_0.6.33       farver_2.1.1       \n [94] htmlwidgets_1.6.2   htmltools_0.5.6     lifecycle_1.0.4    \n [97] mime_0.12           rstanarm_2.21.4     bit64_4.0.5        \n[100] shinythemes_1.2.0   MASS_7.3-58.4"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html",
    "href": "posts/05_conjugate_families/05_conjugate_families.html",
    "title": "5: Conjugate Families",
    "section": "",
    "text": "library(\"bayesrules\")\nlibrary(\"ggtext\")\nlibrary(\"gt\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\ntips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#simple-prior",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#simple-prior",
    "title": "5: Conjugate Families",
    "section": "Simple Prior",
    "text": "Simple Prior\nSuppose that we wanted to estimate a probability \\(\\pi \\in [0,1]\\), but perhaps the beta distribution seems complicated. Instead, we can try an elementary math function like \\(f(\\pi) = 3\\pi^{2}\\), where this is a probability density function since\n\\[\\displaystyle\\int_{0}^{1} \\! 3\\pi^{2} \\, d\\pi = 1 \\text{ and } f(\\pi) \\geq 0 \\text{ for } \\pi \\in [0,1]\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#interpretability",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#interpretability",
    "title": "5: Conjugate Families",
    "section": "Interpretability",
    "text": "Interpretability\n\nPlotCodeInterpretation\n\n\n\n\n\n\n\n\n\n\npi &lt;- seq(0, 1, 0.01)\nf_pi &lt;- 3*pi^2\n\ndf_for_line &lt;- data.frame(pi, f_pi)\ndf_for_shade &lt;- df_for_line |&gt;\n  rbind(c(1,0)) #enforce lower-right corner\n\ndf_for_line |&gt;\n  ggplot(aes(x = pi, y = f_pi)) +\n  geom_polygon(data = df_for_shade, fill = \"#E77500\") +\n  geom_line(color = \"#121212\", linewidth = 3) +\n  labs(title = \"&lt;span style='color:#E77500'&gt;Parabolic Prior&lt;/span&gt;: f(pi) = 3pi^2\",\n       subtitle = \"left-skew\",\n       caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown()) #use ggtext package\n\n\n\nIf we start with this prior, we are perhaps assuming a situation over \\([0,1]\\) where we are expecting the event to likely occur:\n\\[\\text{E}(\\pi) = \\displaystyle\\int_{0}^{1} \\! \\pi \\cdot f(\\pi) \\, d\\pi = \\displaystyle\\frac{3}{4}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#likelihood",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#likelihood",
    "title": "5: Conjugate Families",
    "section": "Likelihood",
    "text": "Likelihood\nSuppose that we observe \\(Y = 17\\) successes in \\(n = 32\\) independent trials, then modeling the likelihood with a binomial model yields\n\\[L(\\pi|y = 17) = \\binom{32}{17}\\pi^{17}(1-\\pi)^{15} \\text{ for } \\pi \\in [0,1]\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#posterior-distribution",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#posterior-distribution",
    "title": "5: Conjugate Families",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution\nRecall that the posterior distribution is proportional to the product of the prior distribution and the likelihood\n\\[\\begin{array}{rcl}\n  f(\\pi|y=17) & \\propto & f(\\pi) \\cdot L(\\pi|y=17) \\\\\n  ~ & \\propto & \\pi^{2} \\cdot \\pi^{17}(1-\\pi)^{15} \\\\\n\\end{array}\\]\ndoes not have the same form as our prior \\(f(\\pi) = 3\\pi^{2}\\)"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#normalizing-constant",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#normalizing-constant",
    "title": "5: Conjugate Families",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\\[f(\\pi|y=17) = \\displaystyle\\frac{\\pi^{19}(1-\\pi)^{15}}{ \\int_{0}^{1} \\! \\pi^{19}(1-\\pi)^{15} \\, d\\pi } \\text{ for } \\pi \\in [0,1]\\]\n\nintegrals can be tough to compute, even with numerical methods\nvery low interpretability\ndifficult to compute sample statistics for the posterior distribution (such as mean and variance)"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#conjugate-priors",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#conjugate-priors",
    "title": "5: Conjugate Families",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\nConjugate families have both computational ease and interpretable posterior distributions.\n\n\n\n\n\n\nConjugate Priors\n\n\n\n\n\nLet the prior model for parameter \\(\\theta\\) have pdf \\(f(\\theta)\\) and the model of data Y conditioned on \\(\\theta\\) have likelihood function \\(L(\\theta|y)\\). If the resulting posterior model with pdf \\(f(\\theta|y) \\propto f(\\theta)L(\\theta|y)\\) is of the same model family as the prior, then we say this is a conjugate prior."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#poisson-process",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#poisson-process",
    "title": "5: Conjugate Families",
    "section": "Poisson Process",
    "text": "Poisson Process\n\nMotivationGoalInfinitessimalPartial Proof\n\n\n\nAssume a constant \\(\\lambda\\) of arrivals\nLet \\(N_{t}\\) be the number of arrivals in time interval \\([0,t]\\)\nHomogeneity: \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nIndependence: numbers of arrivals in disjoint time intervals are independent random variables\n\n\n\nerive distribution of number of arrivals\n\nWe expect \\(\\text{E}[N_{t}] = \\lambda t\\) (``rate times time’’)\nPartition time interval \\([0,t]\\) into \\(n\\) subintervals\nAssuming \\(n\\) is large enough so that each subinterval has zero or one arrival (i.e. Bernoulli trial)\nProbability of arrival in a random subinterval: \\(p = \\displaystyle\\frac{\\lambda t}{n}\\)\n\nSo far, we are assuming \\(N_{t} \\sim \\text{Bin}(n,p)\\)\n\\[P(N_{t} = k) = \\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k} \\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n-k}\\]\n\n\nHowever,\n\n\\(n\\) was arbitrary\ntime is a continuous variable\n\nSo let’s take the limit as \\(n\\) goes to infinity.\n\\[\\displaystyle\\lim_{n \\to \\infty} P(N_{t} = k) = \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}}\\]\n\n\nHandling the limit by its factors: \\[\\displaystyle\\lim_{n \\to \\infty} {\\color{red}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{-k}} = 1, \\quad \\displaystyle\\lim_{n \\to \\infty} {\\color{blue}\\left(1 - \\displaystyle\\frac{\\lambda t}{n}\\right)^{n}} = e^{-\\lambda t}\\]\n\\[\\begin{array}{rcl}\n  \\displaystyle\\lim_{n \\to \\infty} {\\color{purple}\\binom{n}{k} \\left(\\displaystyle\\frac{\\lambda t}{n}\\right)^{k}} & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\binom{n}{k} \\left(\\displaystyle\\frac{1}{n}\\right)^{k} \\\\\n  ~ & = & (\\lambda t)^{k} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{k!(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n!}{(n-k)!} \\cdot \\displaystyle\\frac{1}{n^{k}} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\displaystyle\\prod_{i = 0}^{k-1} \\displaystyle\\lim_{n \\to \\infty} \\displaystyle\\frac{n - i}{n} \\\\\n  ~ & = & \\displaystyle\\frac{(\\lambda t)^{k}}{k!}  \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#poisson-distribution",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#poisson-distribution",
    "title": "5: Conjugate Families",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nLet discrete random variable \\(Y\\) be the number of independent events that occur in a fixed amount of time or space, where \\(\\lambda&gt;0\\) is the rate at which these events occur. Then the dependence of \\(Y\\) on parameter \\(\\lambda\\) can be modeled by the Poisson.\n\\[Y|\\lambda \\sim \\text{Pois}(\\lambda)\\]\nwith probability mass function\n\\[f(y|\\lambda) = \\displaystyle\\frac{\\lambda^{y}e^{-\\lambda}}{y!} \\text{ for } y \\in \\{0, 1, 2, ...\\}\\]\n\n\\(f(y|\\lambda) \\geq 0\\)\n\\(\\displaystyle\\sum_{y=0}^{\\infty} \\! f(y|\\lambda) = 1\\)\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\nThe Poisson distribution has the curious property where the randomness has equal mean and variance:\n\\[\\text{E}(Y|\\lambda) = \\text{Var}(Y|\\lambda) = \\lambda\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#guidance",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#guidance",
    "title": "5: Conjugate Families",
    "section": "Guidance",
    "text": "Guidance\n\nPlotCodeGuidance\n\n\n\n\n\n\n\n\n\n\ny_i &lt;- 0:10\nf_y &lt;- dpois(y_i, 1)\ndf_for_plots &lt;- data.frame(y_i,f_y)\n\np1 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 1))) + \n  geom_col() + \n  labs(title = \"lambda = 1\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np2 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 2))) + \n  geom_col() + \n  labs(title = \"lambda = 2\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np3 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 3))) + \n  geom_col() + \n  labs(title = \"lambda = 3\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np4 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 4))) + \n  geom_col() + \n  labs(title = \"lambda = 4\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\np5 &lt;- df_for_plots |&gt; \n  ggplot(aes(x = y_i, y = dpois(y_i, 5))) + \n  geom_col() + \n  labs(title = \"lambda = 5\") +\n  scale_x_continuous(name = \"y\", \n                   breaks = 0:10, \n                   labels = as.character(0:10)) +\n  theme_minimal()\n\n# patchwork\np1 + p2 + p3 + p4 + p5\n\n\n\nThe Poisson distribution is a discrete distribution that tends to be used to model rare events."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#joint-pmf",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#joint-pmf",
    "title": "5: Conjugate Families",
    "section": "Joint PMF",
    "text": "Joint PMF\nLet \\((Y_1,Y_2,…,Y_n)\\) be an independent sample of random variables and \\(\\vec{y} = (y_1,y_2,…,y_n)\\) be the corresponding vector of observed values.\n\n\n\n\n\n\nJoint Probability Mass Function\n\n\n\n\n\nFurther, let \\(f(y_i|\\lambda)\\) denote the pmf of an individual observed data point \\(Y_i=y_i\\). Then by the assumption of independence, the following joint pmf specifies the randomness in and plausibility of the collective sample:\n\\[f(\\vec{y}|\\lambda) = \\displaystyle\\prod_{i=1}^{n} f(y_{i}|\\lambda) = f(y_{1}|\\lambda) \\cdot (y_{2}|\\lambda) \\cdots f(y_{n}|\\lambda)\\]\n\n\n\nThe Poisson probability mass function is then\n\\[\\begin{array}{rcl}\n  f(\\vec{y}|\\lambda) & = & \\displaystyle\\prod_{i=1}^{n} f(y_{i}|\\lambda) \\\\\n  ~ & = & \\displaystyle\\prod_{i=1}^{n} \\displaystyle\\frac{\\lambda^{y_{i}}e^{\\lambda}}{y_{i}!} \\\\\n  ~ & = & \\displaystyle\\frac{\\lambda^{y_{1}}e^{\\lambda}}{y_{1}!} \\cdot \\displaystyle\\frac{\\lambda^{y_{2}}e^{\\lambda}}{y_{2}!} \\cdots \\displaystyle\\frac{\\lambda^{y_{n}}e^{\\lambda}}{y_{n}!} \\\\\n  ~ & = & \\displaystyle\\frac{ [\\lambda^{y_{1}}\\lambda^{y_{2}}\\cdots\\lambda^{y_{n}}][e^{-\\lambda}e^{-\\lambda} \\cdots e^{-\\lambda}] }{ y_{1}!y_{2}! \\cdots y_{n}! } \\\\\n  ~ & = & \\displaystyle\\frac{\\lambda^{\\sum y_{i}}e^{-n\\lambda}}{\\prod y_{i}!} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#poisson-likelihood",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#poisson-likelihood",
    "title": "5: Conjugate Families",
    "section": "Poisson Likelihood",
    "text": "Poisson Likelihood\nThe Poisson likelihood function is then\n\\[L(\\lambda|\\vec{y}) = \\displaystyle\\frac{\\lambda^{\\sum y_{i}}e^{-n\\lambda}}{\\prod y_{i}!}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#parameter-selection",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#parameter-selection",
    "title": "5: Conjugate Families",
    "section": "Parameter Selection",
    "text": "Parameter Selection\nHow do we fit a Poisson model with our data? One idea is to seek the maximum likelihood estimate (MLE).\nClaim: The MLE for the \\(\\text{Pois}(\\lambda)\\) distribution is \\[\\lambda^{*} = \\bar{y} = \\displaystyle\\frac{\\sum y_{i}}{n}\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the Poisson distribution’s PMF \\(f(y) = \\displaystyle\\frac{\\lambda^{y}e^{-\\lambda}}{y!}\\), the likelihood function\n\\[L(\\lambda) = \\displaystyle\\frac{\\lambda^{y_{1}}e^{-\\lambda}}{y_{1}!} \\cdot \\displaystyle\\frac{\\lambda^{y_{2}}e^{-\\lambda}}{y_{2}!} \\cdots \\displaystyle\\frac{\\lambda^{y_{n}}e^{-\\lambda}}{y_{n}!} \\]\nTaking the natural logarithm of both sides, we create the log likelihood function \\(\\ell(\\lambda)\\)\n$$\n\\[\\begin{array}{rcl}\n  \\ln L(\\lambda) & = & \\ln \\left(\\displaystyle\\frac{\\lambda^{y_{1}}e^{-\\lambda}}{y_{1}!} \\cdot \\displaystyle\\frac{\\lambda^{y_{2}}e^{-\\lambda}}{y_{2}!} \\cdots \\displaystyle\\frac{\\lambda^{y_{n}}e^{-\\lambda}}{y_{n}!}\\right) \\\\\n  \\ell(\\lambda) & = & \\ln \\displaystyle\\prod_{i=1}^{n} \\displaystyle\\frac{\\lambda^{y_{i}}e^{-\\lambda}}{y_{i}!} \\\\\n  \\ell(\\lambda) & = & \\displaystyle\\sum_{i=1}^{n} \\ln \\displaystyle\\frac{\\lambda^{y_{i}}e^{-\\lambda}}{y_{i}!} \\\\\n  \n  \\ell(\\lambda) & = & \\displaystyle\\sum_{i=1}^{n} \\left( y_{i}\\ln \\lambda + \\ln e^{-\\lambda} - \\ln y_{i}! \\right) \\\\\n  \n  \\ell(\\lambda) & = & (\\ln \\lambda)\\left(\\displaystyle\\sum_{i=1}^{n} y_{i}\\right) -  \\displaystyle\\sum_{i=1}^{n}\\lambda -  \\displaystyle\\sum_{i=1}^{n} \\ln y_{i}! \\\\\n  \n  \\ell(\\lambda) & = &  (\\ln \\lambda)\\left(\\displaystyle\\sum_{i=1}^{n} y_{i}\\right) - n\\lambda - \\displaystyle\\sum_{i=1}^{n} \\ln (y_{i}!) \\\\\n\\end{array}\\]\n$$\nThe motivation for the logarithm usage is to ease the process of taking the derivative. Here, taking the derivative with respect to \\(\\lambda\\),\n\\[0 = \\ell'(\\lambda)  \\quad\\Rightarrow\\quad 0 = -n + \\displaystyle\\frac{ \\sum_{i=1}^{n} y_{i} }{ \\lambda } \\quad\\Rightarrow\\quad \\lambda = \\displaystyle\\frac{ \\sum_{i=1}^{n} y_{i} }{ n } = \\bar{y}\\]\nThat is, the optimal value for parameter \\(\\lambda\\) is the sample mean \\(\\bar{y}\\)."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety",
    "title": "5: Conjugate Families",
    "section": "Example: Campus Safety",
    "text": "Example: Campus Safety\n\nDataLikelihoodCode\n\n\nThe following data on arrests for drug law violations come from the Princeton University Annual Security and Fire Safety Report (in and around the main campus)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n\n\n\n\narrests\n18\n14\n23\n22\n12\n22\n7\n0\n1\n\n\n\nOur maximum likelihood estimate is\n\\[\\lambda^{*} = \\displaystyle\\frac{\\sum y_{i}}{n} = \\displaystyle\\frac{119}{9} \\approx 13.2222 \\text{ arrests per year}\\]\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_poisson_likelihood(\n  y = c(18, 14, 23, 22, 12, 22, 7, 0, 1),\n  lambda_upper_bound = 20\n) +\n  labs(title = \"Likelihood Curve\",\n       subtitle = \"Arrests per year for drug law violations\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#terminology",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#terminology",
    "title": "5: Conjugate Families",
    "section": "Terminology",
    "text": "Terminology\nLet \\(\\lambda &gt; 0\\) be a continuous random variable. For modeling, we might try a Gamma model \\[\\lambda \\sim \\text{Gamma}(s, r)\\]\n\n\\(s\\): shape parameter\n\\(r\\): rate parameter\n\n\n\n\n\n\n\nExplore!\n\n\n\n\n\nMatt Bognar at the University of Iowa created this great webapp to explore the gamma distribution.\n\n\n\n\n\n\n\n\n\nExponential Model\n\n\n\n\n\nThe Gamma model is a generalization of the exponential model. When the shape parameter \\(s = 1\\), then\n\\[\\lambda \\sim \\text{Gamma}(1,r) = \\text{Exp}(r)\\]\nwhere \\(r\\) is once again the rate parameter."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function",
    "title": "5: Conjugate Families",
    "section": "Probablity Density Function",
    "text": "Probablity Density Function\nThe Gamma model has a continuous pdf\n\\[f(\\lambda) = \\displaystyle\\frac{r^{s}}{\\Gamma(s)} \\lambda^{s-1}e^{-r\\lambda} \\text{ for } \\lambda &gt; 0\\]\nwhere the gamma function\n\n\\(\\Gamma(z) = \\displaystyle\\int_{0}^{\\infty} \\! x^{z-1}e^{-x} \\, dx\\)\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\nFormulas for the Gamma model include\n\\[\\begin{array}{rcl}\n  \\text{E}(\\lambda) & = & \\displaystyle\\frac{s}{r} \\\\\n  \\text{Mode}(\\lambda) & = & \\displaystyle\\frac{s-1}{r} \\\\\n  \\text{Var}(\\lambda) & = & \\displaystyle\\frac{s}{r^{2}} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior",
    "title": "5: Conjugate Families",
    "section": "Tuning the Prior",
    "text": "Tuning the Prior\n\nExample: Campus SafetyStatisticsPlotCode\n\n\nSuppose that a parent of an university applicant feels that the university has arrests for drug law violations with counts between 10 and 30 per year. Matching some statistics formulas\n\\[[\\mu - 2\\sigma, \\mu + 2\\sigma] = [10, 30] \\quad\\rightarrow\\quad \\mu = 20, \\quad \\sigma = 5\\]\n\n\n\\[\\text{E}(\\lambda) = \\displaystyle\\frac{s}{r} = 20 \\text{ and } \\text{Var}(\\lambda) = \\displaystyle\\frac{s}{r^{2}} = 5^{2}\\]\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_gamma(16, 0.8, mean = TRUE) +\n  labs(title = \"Gamma(16, 0.8) Prior\",\n       subtitle = \"mean = 20, sd = 5\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#gamma-poisson-bayesian-model",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#gamma-poisson-bayesian-model",
    "title": "5: Conjugate Families",
    "section": "Gamma-Poisson Bayesian Model",
    "text": "Gamma-Poisson Bayesian Model\nLet \\(\\lambda &gt; 0\\) be an unknown rate parameter and let \\(\\{Y_{1}, Y_{2}, ..., Y_{n}\\}\\) be an i.i.d. sample from a \\(\\text{Pois}(\\lambda)\\) distribution. With a setup of a Gamma prior and Poisson likelihood\n\\[\\begin{array}{rcl}\n  \\lambda & \\sim & \\text{Gamma}(s,r) \\\\\n  Y_{i}|\\lambda & \\sim & \\text{Pois}(\\lambda) \\\\\n\\end{array}\\]\nand observing data \\(\\vec{y} = \\{y_{1}, y_{2}, ..., y_{n}\\}\\), the posterior distribution also has a Gamma structure with updated parameters\n\\[\\lambda|\\vec{y} \\sim \\text{Gamma}\\left( s + \\displaystyle\\sum_{i=1}^{n} y_{i}, r + n \\right)\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\\begin{array}{rcl}\n  f(\\lambda|\\vec{y}) & \\propto & f(\\lambda) \\cdot L(\\lambda|\\vec{y}) \\\\\n  ~ & = & \\displaystyle\\frac{r^{s}}{\\Gamma(s)}\\lambda^{s-1}e^{-r\\lambda} \\cdot \\displaystyle\\frac{\\lambda^{\\sum y_{i}}e^{-n\\lambda}}{\\prod y_{i}!} \\\\\n  ~ & \\propto & \\lambda^{s-1}e^{-r\\lambda} \\cdot \\lambda^{\\sum y_{i}}e^{-n\\lambda} \\\\\n  ~ & = & \\lambda^{s+\\sum y_{i} - 1}e^{-(r+n)\\lambda} \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety-2",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#example-campus-safety-2",
    "title": "5: Conjugate Families",
    "section": "Example: Campus Safety",
    "text": "Example: Campus Safety\n\nRecapPlotsCodeStatistics\n\n\n\nwe tuned a \\(\\text{Gamma}(16, 0.8)\\) prior\nwe observed 119 arrests for drug law violations over a \\(n = 9\\) year time span\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_gamma_poisson(shape = 16, rate = 0.8,\n                               sum_y = 119, n = 9) +\n  labs(title = \"Gamma-Poisson Model\",\n       subtitle = \"Drug Law Violations Example\",\n       caption = \"SML 320\",\n       x = \"arrests for drug law violations\") +\n  theme_minimal()\n\n\n\n\nbayesrules::summarize_gamma_poisson(shape = 16, rate = 0.8,\n                                    sum_y = 119, n = 9) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model shape rate    mean    mode     var     sd\n1     prior    16  0.8 20.0000 18.7500 25.0000 5.0000\n2 posterior   135  9.8 13.7755 13.6735  1.4057 1.1856"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#terminology-1",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#terminology-1",
    "title": "5: Conjugate Families",
    "section": "Terminology",
    "text": "Terminology\nLet \\(Y &gt; 0\\) be a continuous random variable over all real numbers \\(())-\\infty, \\infty)\\). For modeling, we might try a normal distribution \\[Y \\sim \\text{N}(\\mu, \\sigma^{2})\\]\n\n\\(\\mu\\): mean\n\\(\\sigma\\): standard deviation"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function-1",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#probablity-density-function-1",
    "title": "5: Conjugate Families",
    "section": "Probablity Density Function",
    "text": "Probablity Density Function\nThe normal distribution has a continuous probability density function\n\\[f(y) = \\displaystyle\\frac{1}{\\sqrt{2\\pi \\sigma^{2}}} \\text{exp}\\left[ -\\displaystyle\\frac{(y-\\mu)^{2}}{2\\sigma^{2}}\\right] \\text{ for } y \\in (-\\infty, \\infty)\\]\n\n\n\n\n\n\nStatistics\n\n\n\n\n\nDescriptions of normal distributions are dictated by their statistics\n\\[\\begin{array}{rcl}\n  \\text{E}(Y) & = & \\mu \\\\\n  \\text{Mode}(Y) & = & \\mu \\\\\n  \\text{Var}(Y) & = & \\sigma^{2} \\\\\n  \\text{SD}(Y) & = & \\sigma \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior-1",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#tuning-the-prior-1",
    "title": "5: Conjugate Families",
    "section": "Tuning the Prior",
    "text": "Tuning the Prior\n\nExample: TipsStatisticsPlotCode\n\n\n\nhead(tips_df)\n\n# A tibble: 6 × 7\n  total_bill   tip sex    smoker day   time    size\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n1       17.0  1.01 Female No     Sun   Dinner     2\n2       10.3  1.66 Male   No     Sun   Dinner     3\n3       21.0  3.5  Male   No     Sun   Dinner     3\n4       23.7  3.31 Male   No     Sun   Dinner     2\n5       24.6  3.61 Female No     Sun   Dinner     4\n6       25.3  4.71 Male   No     Sun   Dinner     4\n\n\n\n\nLet us guess that Americans tend to tip between 5 and 25 percent of the total bill.\n\\[[\\mu - 2\\sigma, \\mu + 2\\sigma] = [5, 25] \\quad\\rightarrow\\quad \\mu = 15, \\quad \\sigma = 5\\]\n\n\n\n\n\n\n\n\n\n\nbayesrules::plot_normal(mean = 15, sd = 5) +\n  labs(title = \"N(15, 25) Prior\",\n       subtitle = \"mean = 15, sd = 5\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#likelihood-2",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#likelihood-2",
    "title": "5: Conjugate Families",
    "section": "Likelihood",
    "text": "Likelihood\nIn this conjugate prior relationship, the likelihood is also modeled as a normal distribution.\n\\[L(\\mu, \\sigma|\\vec{y}) \\propto \\displaystyle\\prod_{i=1}^{n} \\text{exp}\\left[-\\displaystyle\\frac{(y_{i} - \\mu)^{2}}{2\\sigma^{2}}\\right] = \\text{exp}\\left[-\\displaystyle\\frac{\\sum_{i=1}^{n} (y_{i}-\\mu)^{2}}{2\\sigma^{2}}\\right]\\]"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#mles",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#mles",
    "title": "5: Conjugate Families",
    "section": "MLEs",
    "text": "MLEs\nThe likelihood can also be expressed in terms of the sample mean \\(\\bar{y}\\) and the sample size \\(n\\)\n\\[L(\\mu, \\sigma|\\vec{y}) \\propto \\text{exp}\\left[-\\displaystyle\\frac{ (\\bar{y}-\\mu)^{2}}{\\frac{2\\sigma^{2}}{n}}\\right]\\]\nIt follows that the maximum likelihood estimates for the parameters are\n\\[\\begin{array}{rcl}\n  \\mu^{*} & = & \\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} y_{i} \\\\\n  \\sigma^{*} & = & \\sqrt{\\displaystyle\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (y_{i} - \\mu^{2})^{2}} \\\\\n\\end{array}\\]\nwhich are the sample mean and from the not-corrected population variance (source)."
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#dplyr",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#dplyr",
    "title": "5: Conjugate Families",
    "section": "dplyr",
    "text": "dplyr\n\nn &lt;- nrow(tips_df)\ntips_df |&gt;\n  mutate(tips_pct = tip/total_bill * 100) |&gt;\n  summarize(mu = mean(tips_pct, na.rm = TRUE),\n            sigma = sqrt(var(tips_pct, na.rm = TRUE) *(n-1)/(n)))\n\n# A tibble: 1 × 2\n     mu sigma\n  &lt;dbl&gt; &lt;dbl&gt;\n1  16.1  6.09"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#plot-4",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#plot-4",
    "title": "5: Conjugate Families",
    "section": "Plot",
    "text": "Plot"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#code-6",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#code-6",
    "title": "5: Conjugate Families",
    "section": "Code",
    "text": "Code\n\nbayesrules::plot_normal(mean = 16.08026, sd = 6.094693  ) +\n  labs(title = \"Normal \",\n       subtitle = \"MLEs: ybar = 16.08026, sigma = 6.094693\",\n       caption = \"SML 320\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#normal-normal-conjugacy",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#normal-normal-conjugacy",
    "title": "5: Conjugate Families",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\nLet \\(\\mu \\in (-\\infty, \\infty)\\) be an unknown mean parameter and let \\(\\sigma^{2} &gt; 0\\) be an unknown variance parameter and let \\(\\{Y_{1}, Y_{2}, ..., Y_{n}\\}\\) be an i.i.d. sample from a \\(\\text{N}(\\mu, \\sigma^{2})\\) distribution. With a setup of a normal prior and normal likelihood\n\\[\\begin{array}{rcl}\n  \\mu,\\sigma^{2} & \\sim & \\text{N}(\\theta,\\tau^{2}) \\\\\n  Y_{i}|\\mu, \\sigma^{2} & \\sim & \\text{N}(\\mu,\\sigma^{2}) \\\\\n\\end{array}\\]\nand observing data \\(\\vec{y} = \\{y_{1}, y_{2}, ..., y_{n}\\}\\), the posterior distribution also has a normal structure with updated parameters\n\\[\\mu,\\sigma^{2}|\\vec{y} \\sim \\text{N}\\left( \\displaystyle\\frac{\\sigma^{2}}{n\\tau^{2}+\\sigma^{2}} \\cdot \\theta + \\displaystyle\\frac{n\\tau^{2}}{n\\tau^{2}+\\sigma^{2}} \\cdot \\bar{y}, \\quad \\displaystyle\\frac{\\tau^{2}\\sigma^{2}}{n\\tau^{2}+\\sigma^{2}} \\right)\\]\n\nWhat happens if we have relatively small data sets?\nWhat happens if we have relatively large data sets?"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#example",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#example",
    "title": "5: Conjugate Families",
    "section": "Example",
    "text": "Example\n\nCodePlotsStatistics\n\n\n\nbayesrules::plot_normal_normal(\n  \n  # from prior\n  mean = 15, sd = 5,\n  \n  # from observations\n  y_bar = 16.08026, sigma = 6.094693, n = 244\n) +\n  labs(title = \"Normal-Normal Model\",\n       subtitle = \"Restaurant Tips Example\",\n       caption = \"SML 320\",\n       x = \"percent of total food bill\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nbayesrules::summarize_normal_normal(\n  \n  # from prior\n  mean = 15, sd = 5,\n  \n  # from observations\n  y_bar = 16.08026, sigma = 6.094693, n = 244\n) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model    mean    mode     var    sd\n1     prior 15.0000 15.0000 25.0000 5.000\n2 posterior 16.0737 16.0737  0.1513 0.389"
  },
  {
    "objectID": "posts/05_conjugate_families/05_conjugate_families.html#model-selection",
    "href": "posts/05_conjugate_families/05_conjugate_families.html#model-selection",
    "title": "5: Conjugate Families",
    "section": "Model Selection",
    "text": "Model Selection\nWe looked at 3 conjugate families.\n\nBeta-BinomialGamma-PoissonNormal-Normal\n\n\n\nestimate \\(\\pi \\in [0,1]\\)\npro: good for interpretability\ncon: computationally expensive for large \\(n\\)\n\n\n\n\nestimate \\(\\lambda &gt; 0\\)\npro: models rare events and skewed data well\ncon: discussion of rates instead of counts\n\n\n\n\nestimate mean \\(\\mu\\) and variance \\(\\sigma\\)\npro: ubiquitous in scientific communities\ncons:\n\ninfinite support may lead to suboptimal results in larger networks\nwas the data symmetric?"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html",
    "href": "posts/06_approx_posterior/06_approx_posterior.html",
    "title": "6: Approximating the Posterior",
    "section": "",
    "text": "library(\"bayesplot\")\nlibrary(\"ggtext\")\nlibrary(\"rstan\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# tips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#setting-noaa",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#setting-noaa",
    "title": "6: Approximating the Posterior",
    "section": "Setting: NOAA",
    "text": "Setting: NOAA\nI will be studying weather data for a semester theme. The main data will probably come from NOAA (National Oceanic and Atmospheric Administration), where the data consists of several readings (variables) from several research stations over many years.\nLet \\(\\theta\\) represent the high temperature recorded. Across \\(k\\) research stations,\n\\[\\vec{\\theta} = (\\theta_{1}, \\theta_{2}, ..., \\theta_{k})\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#bayesian-approach",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#bayesian-approach",
    "title": "6: Approximating the Posterior",
    "section": "Bayesian Approach",
    "text": "Bayesian Approach\nIf we have a vector of parameters to estimate,\n\\[f(\\vec{\\theta} | \\vec{y} ) \\propto \\text{prior}*\\text{likelihood} = f(\\vec{\\theta}) \\cdot L(\\vec{\\theta} | \\vec{y})\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#normalizing-constant",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#normalizing-constant",
    "title": "6: Approximating the Posterior",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\\[f(y) = \\displaystyle\\int_{\\theta_{1}}\\int_{\\theta_{2}} \\cdots \\int_{\\theta_{k}} \\! f(\\vec{\\theta}) \\cdot L(\\vec{\\theta} | \\vec{y} ) \\, d\\theta_{k} \\cdots d\\theta_{2} \\, d\\theta_{1}\\]\n\nclosed form solution probably does not exist\nvery expensive computationally"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#conjugate-priors",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#conjugate-priors",
    "title": "6: Approximating the Posterior",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\nFor today, we will revisit the conjugate priors where we know a lot about the posterior distributions.\n\nBeta-Binomial\n\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Beta}(\\alpha, \\beta) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Bin}(n, \\pi) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Beta}(\\alpha + y, \\beta + n - y) \\\\\n\\end{array}\\]\n\nGamma-Poisson\n\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Gamma}(s, r) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Pois}(\\lambda) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Gamma}\\left(s + \\displaystyle\\sum_{i=1}^{n} y, r + n\\right) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#broad-idea",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#broad-idea",
    "title": "6: Approximating the Posterior",
    "section": "Broad Idea",
    "text": "Broad Idea\nAs we gather pieces, the overall picture might become clear.\n\n\n\nArt of Ellis Rowan\n\n\n\nimage source: National Museum of Australia"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#math-definitions",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#math-definitions",
    "title": "6: Approximating the Posterior",
    "section": "Math Definitions",
    "text": "Math Definitions\nGrid approximation produces a sample of \\(N\\) independent \\(\\theta\\) values, \\(\\{\\theta^(1),\\theta^(2),…,\\theta^(N)\\}\\), from a discretized approximation of posterior pdf \\(f(\\theta|y)\\). This algorithm evolves in four steps:\n\nDefine a discrete grid of possible \\(\\theta\\) values.\nEvaluate the prior pdf \\(f(\\theta)\\) and likelihood function \\(L(\\theta|y)\\) at each \\(\\theta\\) grid value.\nObtain a discrete approximation of the posterior pdf \\(f(\\theta|y)\\) by:\n\ncalculating the product \\(f(\\theta)L(\\theta|y)\\) at each \\(\\theta\\) grid value; and then\nnormalizing the products so that they sum to 1 across all \\(\\theta\\).\n\nRandomly sample \\(N\\) \\(\\theta\\) grid values with respect to their corresponding normalized posterior probabilities."
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#example-beta-binomial",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#example-beta-binomial",
    "title": "6: Approximating the Posterior",
    "section": "Example: Beta-Binomial",
    "text": "Example: Beta-Binomial\n\nScenario: Smokers in Restaurants\nLet us start with a vague beta prior, use a binomial model to get the likelihood of \\(y = 4\\) smokers among \\(n = 9\\) customers, and then get a beta posterior.\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Beta}(3, 3) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Bin}(9, \\pi) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Beta}(7, 8) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#sparse-grid",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#sparse-grid",
    "title": "6: Approximating the Posterior",
    "section": "Sparse Grid",
    "text": "Sparse Grid\nHere we will try this grid approximation idea over \\(N = 5\\) values\n\\[\\pi \\in \\{0, 0.25, 0.50, 0.75, 1.0\\}\\]\n\nBayesGrid DataGrid ValuesGraph 1 CodePosterior SamplingAlignmentGraph 2 Code\n\n\n\n# Step 1: Define a grid of 6 pi values\ngrid_data &lt;- data.frame(pi_grid = seq(from = 0, to = 1, \n                                      length = 5))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dbeta(pi_grid, 3, 3),\n         likelihood = dbinom(4, 9, pi_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\n\n\nround(grid_data, 4)\n\n  pi_grid  prior likelihood unnormalized posterior\n1    0.00 0.0000     0.0000       0.0000    0.0000\n2    0.25 1.0547     0.1168       0.1232    0.1969\n3    0.50 1.8750     0.2461       0.4614    0.7375\n4    0.75 1.0547     0.0389       0.0411    0.0656\n5    1.00 0.0000     0.0000       0.0000    0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot the grid approximated posterior\nggplot(grid_data, aes(x = pi_grid, y = posterior)) + \n  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior),\n               color = \"gray50\",\n               linewidth = 2) +\n  geom_point(size = 7) + \n  labs(title = \"Sparse Grid\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal()\n\n\n\n\n# Step 4: sample from the discretized posterior\nposterior_sample &lt;- sample_n(grid_data, \n                             size = 10000, \n                             weight = posterior, \n                             replace = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nggplot(posterior_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.1,\n                 fill = \"gray50\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  lims(x = c(0, 1)) +\n  labs(title = \"Sparse Grid: &lt;span style='color:#7F7F7F'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#dense-grid",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#dense-grid",
    "title": "6: Approximating the Posterior",
    "section": "Dense Grid",
    "text": "Dense Grid\nHere we will try this grid approximation idea over \\(N = 101\\) values\n\\[\\pi \\in [0,1]\\]\n\nBayesGrid DataGrid ValuesGraph 1 CodePosterior SamplingAlignmentGraph 2 Code\n\n\n\n# Step 1: Define a grid of 6 pi values\ngrid_data &lt;- data.frame(pi_grid = seq(from = 0, to = 1, \n                                      length = 101))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dbeta(pi_grid, 3, 3),\n         likelihood = dbinom(4, 9, pi_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\n\n\nround(grid_data, 4)\n\n    pi_grid  prior likelihood unnormalized posterior\n1      0.00 0.0000     0.0000       0.0000    0.0000\n2      0.01 0.0029     0.0000       0.0000    0.0000\n3      0.02 0.0115     0.0000       0.0000    0.0000\n4      0.03 0.0254     0.0001       0.0000    0.0000\n5      0.04 0.0442     0.0003       0.0000    0.0000\n6      0.05 0.0677     0.0006       0.0000    0.0000\n7      0.06 0.0954     0.0012       0.0001    0.0000\n8      0.07 0.1271     0.0021       0.0003    0.0000\n9      0.08 0.1625     0.0034       0.0006    0.0000\n10     0.09 0.2012     0.0052       0.0010    0.0001\n11     0.10 0.2430     0.0074       0.0018    0.0001\n12     0.11 0.2875     0.0103       0.0030    0.0002\n13     0.12 0.3345     0.0138       0.0046    0.0003\n14     0.13 0.3837     0.0179       0.0069    0.0004\n15     0.14 0.4349     0.0228       0.0099    0.0006\n16     0.15 0.4877     0.0283       0.0138    0.0009\n17     0.16 0.5419     0.0345       0.0187    0.0012\n18     0.17 0.5973     0.0415       0.0248    0.0016\n19     0.18 0.6536     0.0490       0.0320    0.0020\n20     0.19 0.7106     0.0573       0.0407    0.0026\n21     0.20 0.7680     0.0661       0.0507    0.0032\n22     0.21 0.8257     0.0754       0.0623    0.0040\n23     0.22 0.8834     0.0852       0.0753    0.0048\n24     0.23 0.9409     0.0954       0.0898    0.0057\n25     0.24 0.9981     0.1060       0.1058    0.0067\n26     0.25 1.0547     0.1168       0.1232    0.0078\n27     0.26 1.1105     0.1278       0.1419    0.0090\n28     0.27 1.1655     0.1388       0.1618    0.0103\n29     0.28 1.2193     0.1499       0.1827    0.0116\n30     0.29 1.2718     0.1608       0.2045    0.0130\n31     0.30 1.3230     0.1715       0.2269    0.0144\n32     0.31 1.3726     0.1820       0.2498    0.0159\n33     0.32 1.4205     0.1921       0.2729    0.0173\n34     0.33 1.4666     0.2017       0.2959    0.0188\n35     0.34 1.5107     0.2109       0.3185    0.0202\n36     0.35 1.5527     0.2194       0.3406    0.0216\n37     0.36 1.5925     0.2272       0.3619    0.0230\n38     0.37 1.6301     0.2344       0.3820    0.0243\n39     0.38 1.6652     0.2407       0.4008    0.0255\n40     0.39 1.6979     0.2462       0.4180    0.0266\n41     0.40 1.7280     0.2508       0.4334    0.0275\n42     0.41 1.7555     0.2545       0.4468    0.0284\n43     0.42 1.7802     0.2573       0.4581    0.0291\n44     0.43 1.8022     0.2592       0.4671    0.0297\n45     0.44 1.8214     0.2601       0.4737    0.0301\n46     0.45 1.8377     0.2600       0.4779    0.0304\n47     0.46 1.8511     0.2590       0.4795    0.0305\n48     0.47 1.8615     0.2571       0.4786    0.0304\n49     0.48 1.8690     0.2543       0.4753    0.0302\n50     0.49 1.8735     0.2506       0.4695    0.0298\n51     0.50 1.8750     0.2461       0.4614    0.0293\n52     0.51 1.8735     0.2408       0.4511    0.0287\n53     0.52 1.8690     0.2347       0.4387    0.0279\n54     0.53 1.8615     0.2280       0.4245    0.0270\n55     0.54 1.8511     0.2207       0.4085    0.0260\n56     0.55 1.8377     0.2128       0.3910    0.0248\n57     0.56 1.8214     0.2044       0.3722    0.0237\n58     0.57 1.8022     0.1955       0.3524    0.0224\n59     0.58 1.7802     0.1863       0.3317    0.0211\n60     0.59 1.7555     0.1769       0.3105    0.0197\n61     0.60 1.7280     0.1672       0.2889    0.0184\n62     0.61 1.6979     0.1574       0.2673    0.0170\n63     0.62 1.6652     0.1475       0.2457    0.0156\n64     0.63 1.6301     0.1376       0.2244    0.0143\n65     0.64 1.5925     0.1278       0.2036    0.0129\n66     0.65 1.5527     0.1181       0.1834    0.0117\n67     0.66 1.5107     0.1086       0.1641    0.0104\n68     0.67 1.4666     0.0994       0.1457    0.0093\n69     0.68 1.4205     0.0904       0.1284    0.0082\n70     0.69 1.3726     0.0818       0.1122    0.0071\n71     0.70 1.3230     0.0735       0.0973    0.0062\n72     0.71 1.2718     0.0657       0.0835    0.0053\n73     0.72 1.2193     0.0583       0.0711    0.0045\n74     0.73 1.1655     0.0513       0.0598    0.0038\n75     0.74 1.1105     0.0449       0.0499    0.0032\n76     0.75 1.0547     0.0389       0.0411    0.0026\n77     0.76 0.9981     0.0335       0.0334    0.0021\n78     0.77 0.9409     0.0285       0.0268    0.0017\n79     0.78 0.8834     0.0240       0.0212    0.0013\n80     0.79 0.8257     0.0200       0.0165    0.0011\n81     0.80 0.7680     0.0165       0.0127    0.0008\n82     0.81 0.7106     0.0134       0.0095    0.0006\n83     0.82 0.6536     0.0108       0.0070    0.0004\n84     0.83 0.5973     0.0085       0.0051    0.0003\n85     0.84 0.5419     0.0066       0.0036    0.0002\n86     0.85 0.4877     0.0050       0.0024    0.0002\n87     0.86 0.4349     0.0037       0.0016    0.0001\n88     0.87 0.3837     0.0027       0.0010    0.0001\n89     0.88 0.3345     0.0019       0.0006    0.0000\n90     0.89 0.2875     0.0013       0.0004    0.0000\n91     0.90 0.2430     0.0008       0.0002    0.0000\n92     0.91 0.2012     0.0005       0.0001    0.0000\n93     0.92 0.1625     0.0003       0.0000    0.0000\n94     0.93 0.1271     0.0002       0.0000    0.0000\n95     0.94 0.0954     0.0001       0.0000    0.0000\n96     0.95 0.0677     0.0000       0.0000    0.0000\n97     0.96 0.0442     0.0000       0.0000    0.0000\n98     0.97 0.0254     0.0000       0.0000    0.0000\n99     0.98 0.0115     0.0000       0.0000    0.0000\n100    0.99 0.0029     0.0000       0.0000    0.0000\n101    1.00 0.0000     0.0000       0.0000    0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot the grid approximated posterior\nggplot(grid_data, aes(x = pi_grid, y = posterior)) + \n  geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior),\n               color = \"gray50\",\n               linewidth = 1) +\n  geom_point(size = 2) + \n  labs(title = \"Dense Grid\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal()\n\n\n\n\n# Step 4: sample from the discretized posterior\nposterior_sample &lt;- sample_n(grid_data, \n                             size = 10000, \n                             weight = posterior, \n                             replace = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nggplot(posterior_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 color = \"black\", \n                 binwidth = 0.05,\n                 fill = \"gray50\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  lims(x = c(0, 1)) +\n  labs(title = \"Dense Grid: &lt;span style='color:#7F7F7F'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#example-gamma-poisson",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#example-gamma-poisson",
    "title": "6: Approximating the Posterior",
    "section": "Example: Gamma-Poisson",
    "text": "Example: Gamma-Poisson\n\nScenario: Drug Law Violations\nLet us start with a vague Gamma prior, use a binomial model to get the likelihood of \\(\\sum y = 119\\) drug law violations over \\(n = 9\\) years, and then get a Gamma posterior.\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Gamma}(16, 0.8) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Pois}(119/9) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Gamma}(135, 9.8) \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#sparse-grid-1",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#sparse-grid-1",
    "title": "6: Approximating the Posterior",
    "section": "Sparse Grid",
    "text": "Sparse Grid\nHere we will try this grid approximation idea over \\(N = 11\\) values\n\\[\\lambda \\in \\{0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30\\}\\]\n\nBayesGrid DataGrid ValuesGraph 1 CodePosterior SamplingAlignmentGraph 2 Code\n\n\n\nobs_counts &lt;- c(18, 14, 23, 22, 12, 22, 7, 0, 1)\n\n# Step 1: Define a grid of 11 pi values\ngrid_data &lt;- data.frame(lambda_grid = seq(from = 0, to = 30, \n                                      length = 11))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dgamma(lambda_grid, 16, 0.8),\n         likelihood = dpois(18, lambda_grid)*\n           dpois(14, lambda_grid)*\n           dpois(23, lambda_grid)*\n           dpois(22, lambda_grid)*\n           dpois(12, lambda_grid)*\n           dpois(22, lambda_grid)*\n           dpois(7, lambda_grid)*\n           dpois(0, lambda_grid)*\n           dpois(1, lambda_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\n\n\nround(grid_data, 4)\n\n   lambda_grid  prior likelihood unnormalized posterior\n1            0 0.0000          0            0    0.0000\n2            3 0.0000          0            0    0.0000\n3            6 0.0001          0            0    0.0000\n4            9 0.0033          0            0    0.0000\n5           12 0.0225          0            0    0.3756\n6           15 0.0579          0            0    0.6200\n7           18 0.0809          0            0    0.0043\n8           21 0.0741          0            0    0.0000\n9           24 0.0498          0            0    0.0000\n10          27 0.0265          0            0    0.0000\n11          30 0.0117          0            0    0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot the grid approximated posterior\nggplot(grid_data, aes(x = lambda_grid, y = posterior)) + \n  geom_segment(aes(x = lambda_grid, xend = lambda_grid, y = 0, yend = posterior),\n               color = \"gray50\",\n               linewidth = 2) +\n  geom_point(size = 7) + \n  labs(title = \"Sparse Grid\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal()\n\n\n\n\n# Step 4: sample from the discretized posterior\nposterior_sample &lt;- sample_n(grid_data, \n                             size = 10000, \n                             weight = posterior, \n                             replace = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nggplot(posterior_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.1,\n                 fill = \"gray50\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  lims(x = c(0, 1)) +\n  labs(title = \"Sparse Grid: &lt;span style='color:#7F7F7F'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#dense-grid-1",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#dense-grid-1",
    "title": "6: Approximating the Posterior",
    "section": "Dense Grid",
    "text": "Dense Grid\nHere we will try this grid approximation idea over \\(N = 501\\) values\n\\[\\lambda \\in [0, 30]\\]\n\nBayesGrid DataGrid ValuesGraph 1 CodePosterior SamplingAlignmentGraph 2 Code\n\n\n\nobs_counts &lt;- c(18, 14, 23, 22, 12, 22, 7, 0, 1)\n\n# Step 1: Define a grid of 11 pi values\ngrid_data &lt;- data.frame(lambda_grid = seq(from = 0, to = 30, \n                                      length = 501))\n\n# Step 2: Evaluate the prior & likelihood at each pi\ngrid_data &lt;- grid_data %&gt;% \n  mutate(prior = dgamma(lambda_grid, 16, 0.8),\n         likelihood = dpois(18, lambda_grid)*\n           dpois(14, lambda_grid)*\n           dpois(23, lambda_grid)*\n           dpois(22, lambda_grid)*\n           dpois(12, lambda_grid)*\n           dpois(22, lambda_grid)*\n           dpois(7, lambda_grid)*\n           dpois(0, lambda_grid)*\n           dpois(1, lambda_grid))\n\n# Step 3: Approximate the posterior\ngrid_data &lt;- grid_data %&gt;% \n  mutate(unnormalized = likelihood * prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\n\n\nround(grid_data, 4)\n\n    lambda_grid  prior likelihood unnormalized posterior\n1          0.00 0.0000          0            0    0.0000\n2          0.06 0.0000          0            0    0.0000\n3          0.12 0.0000          0            0    0.0000\n4          0.18 0.0000          0            0    0.0000\n5          0.24 0.0000          0            0    0.0000\n6          0.30 0.0000          0            0    0.0000\n7          0.36 0.0000          0            0    0.0000\n8          0.42 0.0000          0            0    0.0000\n9          0.48 0.0000          0            0    0.0000\n10         0.54 0.0000          0            0    0.0000\n11         0.60 0.0000          0            0    0.0000\n12         0.66 0.0000          0            0    0.0000\n13         0.72 0.0000          0            0    0.0000\n14         0.78 0.0000          0            0    0.0000\n15         0.84 0.0000          0            0    0.0000\n16         0.90 0.0000          0            0    0.0000\n17         0.96 0.0000          0            0    0.0000\n18         1.02 0.0000          0            0    0.0000\n19         1.08 0.0000          0            0    0.0000\n20         1.14 0.0000          0            0    0.0000\n21         1.20 0.0000          0            0    0.0000\n22         1.26 0.0000          0            0    0.0000\n23         1.32 0.0000          0            0    0.0000\n24         1.38 0.0000          0            0    0.0000\n25         1.44 0.0000          0            0    0.0000\n26         1.50 0.0000          0            0    0.0000\n27         1.56 0.0000          0            0    0.0000\n28         1.62 0.0000          0            0    0.0000\n29         1.68 0.0000          0            0    0.0000\n30         1.74 0.0000          0            0    0.0000\n31         1.80 0.0000          0            0    0.0000\n32         1.86 0.0000          0            0    0.0000\n33         1.92 0.0000          0            0    0.0000\n34         1.98 0.0000          0            0    0.0000\n35         2.04 0.0000          0            0    0.0000\n36         2.10 0.0000          0            0    0.0000\n37         2.16 0.0000          0            0    0.0000\n38         2.22 0.0000          0            0    0.0000\n39         2.28 0.0000          0            0    0.0000\n40         2.34 0.0000          0            0    0.0000\n41         2.40 0.0000          0            0    0.0000\n42         2.46 0.0000          0            0    0.0000\n43         2.52 0.0000          0            0    0.0000\n44         2.58 0.0000          0            0    0.0000\n45         2.64 0.0000          0            0    0.0000\n46         2.70 0.0000          0            0    0.0000\n47         2.76 0.0000          0            0    0.0000\n48         2.82 0.0000          0            0    0.0000\n49         2.88 0.0000          0            0    0.0000\n50         2.94 0.0000          0            0    0.0000\n51         3.00 0.0000          0            0    0.0000\n52         3.06 0.0000          0            0    0.0000\n53         3.12 0.0000          0            0    0.0000\n54         3.18 0.0000          0            0    0.0000\n55         3.24 0.0000          0            0    0.0000\n56         3.30 0.0000          0            0    0.0000\n57         3.36 0.0000          0            0    0.0000\n58         3.42 0.0000          0            0    0.0000\n59         3.48 0.0000          0            0    0.0000\n60         3.54 0.0000          0            0    0.0000\n61         3.60 0.0000          0            0    0.0000\n62         3.66 0.0000          0            0    0.0000\n63         3.72 0.0000          0            0    0.0000\n64         3.78 0.0000          0            0    0.0000\n65         3.84 0.0000          0            0    0.0000\n66         3.90 0.0000          0            0    0.0000\n67         3.96 0.0000          0            0    0.0000\n68         4.02 0.0000          0            0    0.0000\n69         4.08 0.0000          0            0    0.0000\n70         4.14 0.0000          0            0    0.0000\n71         4.20 0.0000          0            0    0.0000\n72         4.26 0.0000          0            0    0.0000\n73         4.32 0.0000          0            0    0.0000\n74         4.38 0.0000          0            0    0.0000\n75         4.44 0.0000          0            0    0.0000\n76         4.50 0.0000          0            0    0.0000\n77         4.56 0.0000          0            0    0.0000\n78         4.62 0.0000          0            0    0.0000\n79         4.68 0.0000          0            0    0.0000\n80         4.74 0.0000          0            0    0.0000\n81         4.80 0.0000          0            0    0.0000\n82         4.86 0.0000          0            0    0.0000\n83         4.92 0.0000          0            0    0.0000\n84         4.98 0.0000          0            0    0.0000\n85         5.04 0.0000          0            0    0.0000\n86         5.10 0.0000          0            0    0.0000\n87         5.16 0.0000          0            0    0.0000\n88         5.22 0.0000          0            0    0.0000\n89         5.28 0.0000          0            0    0.0000\n90         5.34 0.0000          0            0    0.0000\n91         5.40 0.0000          0            0    0.0000\n92         5.46 0.0000          0            0    0.0000\n93         5.52 0.0000          0            0    0.0000\n94         5.58 0.0000          0            0    0.0000\n95         5.64 0.0000          0            0    0.0000\n96         5.70 0.0000          0            0    0.0000\n97         5.76 0.0001          0            0    0.0000\n98         5.82 0.0001          0            0    0.0000\n99         5.88 0.0001          0            0    0.0000\n100        5.94 0.0001          0            0    0.0000\n101        6.00 0.0001          0            0    0.0000\n102        6.06 0.0001          0            0    0.0000\n103        6.12 0.0001          0            0    0.0000\n104        6.18 0.0001          0            0    0.0000\n105        6.24 0.0001          0            0    0.0000\n106        6.30 0.0001          0            0    0.0000\n107        6.36 0.0001          0            0    0.0000\n108        6.42 0.0002          0            0    0.0000\n109        6.48 0.0002          0            0    0.0000\n110        6.54 0.0002          0            0    0.0000\n111        6.60 0.0002          0            0    0.0000\n112        6.66 0.0002          0            0    0.0000\n113        6.72 0.0003          0            0    0.0000\n114        6.78 0.0003          0            0    0.0000\n115        6.84 0.0003          0            0    0.0000\n116        6.90 0.0003          0            0    0.0000\n117        6.96 0.0004          0            0    0.0000\n118        7.02 0.0004          0            0    0.0000\n119        7.08 0.0004          0            0    0.0000\n120        7.14 0.0005          0            0    0.0000\n121        7.20 0.0005          0            0    0.0000\n122        7.26 0.0005          0            0    0.0000\n123        7.32 0.0006          0            0    0.0000\n124        7.38 0.0006          0            0    0.0000\n125        7.44 0.0007          0            0    0.0000\n126        7.50 0.0007          0            0    0.0000\n127        7.56 0.0008          0            0    0.0000\n128        7.62 0.0008          0            0    0.0000\n129        7.68 0.0009          0            0    0.0000\n130        7.74 0.0009          0            0    0.0000\n131        7.80 0.0010          0            0    0.0000\n132        7.86 0.0011          0            0    0.0000\n133        7.92 0.0012          0            0    0.0000\n134        7.98 0.0012          0            0    0.0000\n135        8.04 0.0013          0            0    0.0000\n136        8.10 0.0014          0            0    0.0000\n137        8.16 0.0015          0            0    0.0000\n138        8.22 0.0016          0            0    0.0000\n139        8.28 0.0017          0            0    0.0000\n140        8.34 0.0018          0            0    0.0000\n141        8.40 0.0019          0            0    0.0000\n142        8.46 0.0020          0            0    0.0000\n143        8.52 0.0021          0            0    0.0000\n144        8.58 0.0023          0            0    0.0000\n145        8.64 0.0024          0            0    0.0000\n146        8.70 0.0025          0            0    0.0000\n147        8.76 0.0027          0            0    0.0000\n148        8.82 0.0028          0            0    0.0000\n149        8.88 0.0030          0            0    0.0000\n150        8.94 0.0031          0            0    0.0000\n151        9.00 0.0033          0            0    0.0000\n152        9.06 0.0035          0            0    0.0000\n153        9.12 0.0037          0            0    0.0000\n154        9.18 0.0039          0            0    0.0000\n155        9.24 0.0041          0            0    0.0000\n156        9.30 0.0043          0            0    0.0000\n157        9.36 0.0045          0            0    0.0000\n158        9.42 0.0047          0            0    0.0000\n159        9.48 0.0049          0            0    0.0000\n160        9.54 0.0051          0            0    0.0000\n161        9.60 0.0054          0            0    0.0000\n162        9.66 0.0056          0            0    0.0000\n163        9.72 0.0059          0            0    0.0000\n164        9.78 0.0062          0            0    0.0000\n165        9.84 0.0064          0            0    0.0000\n166        9.90 0.0067          0            0    0.0000\n167        9.96 0.0070          0            0    0.0000\n168       10.02 0.0073          0            0    0.0001\n169       10.08 0.0076          0            0    0.0001\n170       10.14 0.0080          0            0    0.0001\n171       10.20 0.0083          0            0    0.0001\n172       10.26 0.0086          0            0    0.0001\n173       10.32 0.0090          0            0    0.0002\n174       10.38 0.0093          0            0    0.0002\n175       10.44 0.0097          0            0    0.0002\n176       10.50 0.0101          0            0    0.0003\n177       10.56 0.0104          0            0    0.0003\n178       10.62 0.0108          0            0    0.0004\n179       10.68 0.0112          0            0    0.0005\n180       10.74 0.0117          0            0    0.0005\n181       10.80 0.0121          0            0    0.0006\n182       10.86 0.0125          0            0    0.0007\n183       10.92 0.0130          0            0    0.0009\n184       10.98 0.0134          0            0    0.0010\n185       11.04 0.0139          0            0    0.0012\n186       11.10 0.0143          0            0    0.0013\n187       11.16 0.0148          0            0    0.0015\n188       11.22 0.0153          0            0    0.0017\n189       11.28 0.0158          0            0    0.0020\n190       11.34 0.0163          0            0    0.0022\n191       11.40 0.0168          0            0    0.0025\n192       11.46 0.0173          0            0    0.0028\n193       11.52 0.0179          0            0    0.0032\n194       11.58 0.0184          0            0    0.0035\n195       11.64 0.0190          0            0    0.0039\n196       11.70 0.0195          0            0    0.0043\n197       11.76 0.0201          0            0    0.0048\n198       11.82 0.0207          0            0    0.0052\n199       11.88 0.0213          0            0    0.0057\n200       11.94 0.0219          0            0    0.0062\n201       12.00 0.0225          0            0    0.0068\n202       12.06 0.0231          0            0    0.0074\n203       12.12 0.0237          0            0    0.0079\n204       12.18 0.0243          0            0    0.0085\n205       12.24 0.0249          0            0    0.0092\n206       12.30 0.0256          0            0    0.0098\n207       12.36 0.0262          0            0    0.0105\n208       12.42 0.0269          0            0    0.0111\n209       12.48 0.0276          0            0    0.0118\n210       12.54 0.0282          0            0    0.0124\n211       12.60 0.0289          0            0    0.0131\n212       12.66 0.0296          0            0    0.0137\n213       12.72 0.0303          0            0    0.0144\n214       12.78 0.0310          0            0    0.0150\n215       12.84 0.0316          0            0    0.0156\n216       12.90 0.0323          0            0    0.0162\n217       12.96 0.0331          0            0    0.0168\n218       13.02 0.0338          0            0    0.0173\n219       13.08 0.0345          0            0    0.0178\n220       13.14 0.0352          0            0    0.0182\n221       13.20 0.0359          0            0    0.0187\n222       13.26 0.0367          0            0    0.0190\n223       13.32 0.0374          0            0    0.0193\n224       13.38 0.0381          0            0    0.0196\n225       13.44 0.0389          0            0    0.0199\n226       13.50 0.0396          0            0    0.0200\n227       13.56 0.0403          0            0    0.0202\n228       13.62 0.0411          0            0    0.0202\n229       13.68 0.0418          0            0    0.0203\n230       13.74 0.0426          0            0    0.0202\n231       13.80 0.0433          0            0    0.0201\n232       13.86 0.0440          0            0    0.0200\n233       13.92 0.0448          0            0    0.0198\n234       13.98 0.0455          0            0    0.0196\n235       14.04 0.0463          0            0    0.0193\n236       14.10 0.0470          0            0    0.0190\n237       14.16 0.0478          0            0    0.0186\n238       14.22 0.0485          0            0    0.0182\n239       14.28 0.0493          0            0    0.0178\n240       14.34 0.0500          0            0    0.0174\n241       14.40 0.0507          0            0    0.0169\n242       14.46 0.0515          0            0    0.0164\n243       14.52 0.0522          0            0    0.0158\n244       14.58 0.0529          0            0    0.0153\n245       14.64 0.0537          0            0    0.0147\n246       14.70 0.0544          0            0    0.0141\n247       14.76 0.0551          0            0    0.0135\n248       14.82 0.0558          0            0    0.0130\n249       14.88 0.0565          0            0    0.0124\n250       14.94 0.0572          0            0    0.0118\n251       15.00 0.0579          0            0    0.0112\n252       15.06 0.0586          0            0    0.0106\n253       15.12 0.0593          0            0    0.0100\n254       15.18 0.0600          0            0    0.0095\n255       15.24 0.0606          0            0    0.0089\n256       15.30 0.0613          0            0    0.0084\n257       15.36 0.0620          0            0    0.0079\n258       15.42 0.0626          0            0    0.0074\n259       15.48 0.0633          0            0    0.0069\n260       15.54 0.0639          0            0    0.0064\n261       15.60 0.0645          0            0    0.0060\n262       15.66 0.0652          0            0    0.0056\n263       15.72 0.0658          0            0    0.0052\n264       15.78 0.0664          0            0    0.0048\n265       15.84 0.0670          0            0    0.0044\n266       15.90 0.0676          0            0    0.0041\n267       15.96 0.0681          0            0    0.0037\n268       16.02 0.0687          0            0    0.0034\n269       16.08 0.0693          0            0    0.0032\n270       16.14 0.0698          0            0    0.0029\n271       16.20 0.0703          0            0    0.0026\n272       16.26 0.0709          0            0    0.0024\n273       16.32 0.0714          0            0    0.0022\n274       16.38 0.0719          0            0    0.0020\n275       16.44 0.0724          0            0    0.0018\n276       16.50 0.0729          0            0    0.0016\n277       16.56 0.0733          0            0    0.0015\n278       16.62 0.0738          0            0    0.0013\n279       16.68 0.0742          0            0    0.0012\n280       16.74 0.0747          0            0    0.0011\n281       16.80 0.0751          0            0    0.0010\n282       16.86 0.0755          0            0    0.0009\n283       16.92 0.0759          0            0    0.0008\n284       16.98 0.0763          0            0    0.0007\n285       17.04 0.0767          0            0    0.0006\n286       17.10 0.0770          0            0    0.0005\n287       17.16 0.0774          0            0    0.0005\n288       17.22 0.0777          0            0    0.0004\n289       17.28 0.0781          0            0    0.0004\n290       17.34 0.0784          0            0    0.0003\n291       17.40 0.0787          0            0    0.0003\n292       17.46 0.0790          0            0    0.0003\n293       17.52 0.0792          0            0    0.0002\n294       17.58 0.0795          0            0    0.0002\n295       17.64 0.0797          0            0    0.0002\n296       17.70 0.0800          0            0    0.0002\n297       17.76 0.0802          0            0    0.0001\n298       17.82 0.0804          0            0    0.0001\n299       17.88 0.0806          0            0    0.0001\n300       17.94 0.0808          0            0    0.0001\n301       18.00 0.0809          0            0    0.0001\n302       18.06 0.0811          0            0    0.0001\n303       18.12 0.0812          0            0    0.0001\n304       18.18 0.0814          0            0    0.0001\n305       18.24 0.0815          0            0    0.0000\n306       18.30 0.0816          0            0    0.0000\n307       18.36 0.0817          0            0    0.0000\n308       18.42 0.0818          0            0    0.0000\n309       18.48 0.0818          0            0    0.0000\n310       18.54 0.0819          0            0    0.0000\n311       18.60 0.0819          0            0    0.0000\n312       18.66 0.0819          0            0    0.0000\n313       18.72 0.0819          0            0    0.0000\n314       18.78 0.0819          0            0    0.0000\n315       18.84 0.0819          0            0    0.0000\n316       18.90 0.0819          0            0    0.0000\n317       18.96 0.0819          0            0    0.0000\n318       19.02 0.0818          0            0    0.0000\n319       19.08 0.0818          0            0    0.0000\n320       19.14 0.0817          0            0    0.0000\n321       19.20 0.0816          0            0    0.0000\n322       19.26 0.0815          0            0    0.0000\n323       19.32 0.0814          0            0    0.0000\n324       19.38 0.0813          0            0    0.0000\n325       19.44 0.0811          0            0    0.0000\n326       19.50 0.0810          0            0    0.0000\n327       19.56 0.0808          0            0    0.0000\n328       19.62 0.0807          0            0    0.0000\n329       19.68 0.0805          0            0    0.0000\n330       19.74 0.0803          0            0    0.0000\n331       19.80 0.0801          0            0    0.0000\n332       19.86 0.0799          0            0    0.0000\n333       19.92 0.0797          0            0    0.0000\n334       19.98 0.0795          0            0    0.0000\n335       20.04 0.0792          0            0    0.0000\n336       20.10 0.0790          0            0    0.0000\n337       20.16 0.0787          0            0    0.0000\n338       20.22 0.0784          0            0    0.0000\n339       20.28 0.0782          0            0    0.0000\n340       20.34 0.0779          0            0    0.0000\n341       20.40 0.0776          0            0    0.0000\n342       20.46 0.0773          0            0    0.0000\n343       20.52 0.0770          0            0    0.0000\n344       20.58 0.0766          0            0    0.0000\n345       20.64 0.0763          0            0    0.0000\n346       20.70 0.0760          0            0    0.0000\n347       20.76 0.0756          0            0    0.0000\n348       20.82 0.0753          0            0    0.0000\n349       20.88 0.0749          0            0    0.0000\n350       20.94 0.0745          0            0    0.0000\n351       21.00 0.0741          0            0    0.0000\n352       21.06 0.0738          0            0    0.0000\n353       21.12 0.0734          0            0    0.0000\n354       21.18 0.0730          0            0    0.0000\n355       21.24 0.0726          0            0    0.0000\n356       21.30 0.0722          0            0    0.0000\n357       21.36 0.0717          0            0    0.0000\n358       21.42 0.0713          0            0    0.0000\n359       21.48 0.0709          0            0    0.0000\n360       21.54 0.0704          0            0    0.0000\n361       21.60 0.0700          0            0    0.0000\n362       21.66 0.0696          0            0    0.0000\n363       21.72 0.0691          0            0    0.0000\n364       21.78 0.0687          0            0    0.0000\n365       21.84 0.0682          0            0    0.0000\n366       21.90 0.0677          0            0    0.0000\n367       21.96 0.0673          0            0    0.0000\n368       22.02 0.0668          0            0    0.0000\n369       22.08 0.0663          0            0    0.0000\n370       22.14 0.0658          0            0    0.0000\n371       22.20 0.0653          0            0    0.0000\n372       22.26 0.0648          0            0    0.0000\n373       22.32 0.0644          0            0    0.0000\n374       22.38 0.0639          0            0    0.0000\n375       22.44 0.0634          0            0    0.0000\n376       22.50 0.0629          0            0    0.0000\n377       22.56 0.0624          0            0    0.0000\n378       22.62 0.0618          0            0    0.0000\n379       22.68 0.0613          0            0    0.0000\n380       22.74 0.0608          0            0    0.0000\n381       22.80 0.0603          0            0    0.0000\n382       22.86 0.0598          0            0    0.0000\n383       22.92 0.0593          0            0    0.0000\n384       22.98 0.0588          0            0    0.0000\n385       23.04 0.0582          0            0    0.0000\n386       23.10 0.0577          0            0    0.0000\n387       23.16 0.0572          0            0    0.0000\n388       23.22 0.0567          0            0    0.0000\n389       23.28 0.0562          0            0    0.0000\n390       23.34 0.0556          0            0    0.0000\n391       23.40 0.0551          0            0    0.0000\n392       23.46 0.0546          0            0    0.0000\n393       23.52 0.0541          0            0    0.0000\n394       23.58 0.0535          0            0    0.0000\n395       23.64 0.0530          0            0    0.0000\n396       23.70 0.0525          0            0    0.0000\n397       23.76 0.0519          0            0    0.0000\n398       23.82 0.0514          0            0    0.0000\n399       23.88 0.0509          0            0    0.0000\n400       23.94 0.0504          0            0    0.0000\n401       24.00 0.0498          0            0    0.0000\n402       24.06 0.0493          0            0    0.0000\n403       24.12 0.0488          0            0    0.0000\n404       24.18 0.0483          0            0    0.0000\n405       24.24 0.0478          0            0    0.0000\n406       24.30 0.0472          0            0    0.0000\n407       24.36 0.0467          0            0    0.0000\n408       24.42 0.0462          0            0    0.0000\n409       24.48 0.0457          0            0    0.0000\n410       24.54 0.0452          0            0    0.0000\n411       24.60 0.0447          0            0    0.0000\n412       24.66 0.0442          0            0    0.0000\n413       24.72 0.0437          0            0    0.0000\n414       24.78 0.0432          0            0    0.0000\n415       24.84 0.0426          0            0    0.0000\n416       24.90 0.0421          0            0    0.0000\n417       24.96 0.0417          0            0    0.0000\n418       25.02 0.0412          0            0    0.0000\n419       25.08 0.0407          0            0    0.0000\n420       25.14 0.0402          0            0    0.0000\n421       25.20 0.0397          0            0    0.0000\n422       25.26 0.0392          0            0    0.0000\n423       25.32 0.0387          0            0    0.0000\n424       25.38 0.0382          0            0    0.0000\n425       25.44 0.0378          0            0    0.0000\n426       25.50 0.0373          0            0    0.0000\n427       25.56 0.0368          0            0    0.0000\n428       25.62 0.0363          0            0    0.0000\n429       25.68 0.0359          0            0    0.0000\n430       25.74 0.0354          0            0    0.0000\n431       25.80 0.0349          0            0    0.0000\n432       25.86 0.0345          0            0    0.0000\n433       25.92 0.0340          0            0    0.0000\n434       25.98 0.0336          0            0    0.0000\n435       26.04 0.0331          0            0    0.0000\n436       26.10 0.0327          0            0    0.0000\n437       26.16 0.0323          0            0    0.0000\n438       26.22 0.0318          0            0    0.0000\n439       26.28 0.0314          0            0    0.0000\n440       26.34 0.0310          0            0    0.0000\n441       26.40 0.0305          0            0    0.0000\n442       26.46 0.0301          0            0    0.0000\n443       26.52 0.0297          0            0    0.0000\n444       26.58 0.0293          0            0    0.0000\n445       26.64 0.0289          0            0    0.0000\n446       26.70 0.0284          0            0    0.0000\n447       26.76 0.0280          0            0    0.0000\n448       26.82 0.0276          0            0    0.0000\n449       26.88 0.0272          0            0    0.0000\n450       26.94 0.0269          0            0    0.0000\n451       27.00 0.0265          0            0    0.0000\n452       27.06 0.0261          0            0    0.0000\n453       27.12 0.0257          0            0    0.0000\n454       27.18 0.0253          0            0    0.0000\n455       27.24 0.0249          0            0    0.0000\n456       27.30 0.0246          0            0    0.0000\n457       27.36 0.0242          0            0    0.0000\n458       27.42 0.0238          0            0    0.0000\n459       27.48 0.0235          0            0    0.0000\n460       27.54 0.0231          0            0    0.0000\n461       27.60 0.0228          0            0    0.0000\n462       27.66 0.0224          0            0    0.0000\n463       27.72 0.0221          0            0    0.0000\n464       27.78 0.0217          0            0    0.0000\n465       27.84 0.0214          0            0    0.0000\n466       27.90 0.0211          0            0    0.0000\n467       27.96 0.0207          0            0    0.0000\n468       28.02 0.0204          0            0    0.0000\n469       28.08 0.0201          0            0    0.0000\n470       28.14 0.0198          0            0    0.0000\n471       28.20 0.0195          0            0    0.0000\n472       28.26 0.0191          0            0    0.0000\n473       28.32 0.0188          0            0    0.0000\n474       28.38 0.0185          0            0    0.0000\n475       28.44 0.0182          0            0    0.0000\n476       28.50 0.0179          0            0    0.0000\n477       28.56 0.0176          0            0    0.0000\n478       28.62 0.0174          0            0    0.0000\n479       28.68 0.0171          0            0    0.0000\n480       28.74 0.0168          0            0    0.0000\n481       28.80 0.0165          0            0    0.0000\n482       28.86 0.0162          0            0    0.0000\n483       28.92 0.0160          0            0    0.0000\n484       28.98 0.0157          0            0    0.0000\n485       29.04 0.0154          0            0    0.0000\n486       29.10 0.0152          0            0    0.0000\n487       29.16 0.0149          0            0    0.0000\n488       29.22 0.0147          0            0    0.0000\n489       29.28 0.0144          0            0    0.0000\n490       29.34 0.0142          0            0    0.0000\n491       29.40 0.0139          0            0    0.0000\n492       29.46 0.0137          0            0    0.0000\n493       29.52 0.0134          0            0    0.0000\n494       29.58 0.0132          0            0    0.0000\n495       29.64 0.0130          0            0    0.0000\n496       29.70 0.0127          0            0    0.0000\n497       29.76 0.0125          0            0    0.0000\n498       29.82 0.0123          0            0    0.0000\n499       29.88 0.0121          0            0    0.0000\n500       29.94 0.0119          0            0    0.0000\n501       30.00 0.0117          0            0    0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n# Plot the grid approximated posterior\nggplot(grid_data, aes(x = lambda_grid, y = posterior)) + \n  geom_segment(aes(x = lambda_grid, xend = lambda_grid, y = 0, yend = posterior),\n               color = \"gray50\",\n               linewidth = 1) +\n  geom_point(size = 2) + \n  labs(title = \"Dense Grid\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal()\n\n\n\n\n# Step 4: sample from the discretized posterior\nposterior_sample &lt;- sample_n(grid_data, \n                             size = 10000, \n                             weight = posterior, \n                             replace = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nggplot(posterior_sample, aes(x = pi_grid)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.5,\n                 color = \"black\",\n                 fill = \"gray50\") + \n  stat_function(fun = dgamma, args = list(135, 9.8),\n                color = \"#E77500\", linewidth = 3) + \n  lims(x = c(0, 1)) +\n  labs(title = \"Dense Grid: &lt;span style='color:#7F7F7F'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#limitations",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#limitations",
    "title": "6: Approximating the Posterior",
    "section": "Limitations",
    "text": "Limitations\n\nHandling larger data sets\nHandling multiple parameters\n\n\\[\\vec{\\theta} = (\\theta_{1}, \\theta_{2}, ..., \\theta_{k})\\]\n\nCurse of dimensionality: As the number of variables increases, “the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality” — Wikipedia\n\nBeta-Binomial example: \\(N = 101\\) grid points\nGamma-Poisson example: \\(N = 501\\) grid points"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#andrey-markov",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#andrey-markov",
    "title": "6: Approximating the Posterior",
    "section": "Andrey Markov",
    "text": "Andrey Markov\n\n\n\n1868 - 1908\nRussian Mathematician\nknown for stochastic processes\nthesis supervisor of Georgy Voronoi\n\n\n\n\n\nAndrey Markov"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#stanislaw-ulam",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#stanislaw-ulam",
    "title": "6: Approximating the Posterior",
    "section": "Stanislaw Ulam",
    "text": "Stanislaw Ulam\n\n\n\n1909 - 1984\nPolish Mathematician\nknown for Monte Carlo methods\nindefinite appointment at IAS\n\n\n\n\n\nJohn von Neumann, Richard Feynman, and Stanislaw Ulam, at Bandelier National Monument near Los Alamos, 1949\n\n\n\nimage source: Institute for Advanced Study"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#mcmc-chains",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#mcmc-chains",
    "title": "6: Approximating the Posterior",
    "section": "MCMC Chains",
    "text": "MCMC Chains\nLet \\(\\{ \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)} \\}\\) be an MCMC chain (Markov Chain Monte Carlo).\n\nMarkov Property:\n\n\\[f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(i)}, y \\right) = f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(i)}, y \\right)\\]\n\nMCMC simulation produces a chain of \\(N\\) dependent values\nThese values are not drawn from the posterior pdf \\(f(\\theta|y)\\)"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#stan",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#stan",
    "title": "6: Approximating the Posterior",
    "section": "Stan",
    "text": "Stan"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#example-beta-binomial-1",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#example-beta-binomial-1",
    "title": "6: Approximating the Posterior",
    "section": "Example: Beta-Binomial",
    "text": "Example: Beta-Binomial\n\nScenario: Smokers in Restaurants\nLet us start with a vague beta prior, use a binomial model to get the likelihood of \\(y = 4\\) smokers among \\(n = 9\\) customers, and then get a beta posterior.\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Beta}(3, 3) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Bin}(9, \\pi) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Beta}(7, 8) \\\\\n\\end{array}\\]\n\nDefine ModelSimulate PosteriorHistogramDensity\n\n\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 9&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(9, pi);\n    pi ~ beta(2, 2);\n  }\n\"\n\n\n\n\nstart_time &lt;- Sys.time()\n\n# STEP 2: SIMULATE the posterior\nbb_sim &lt;- stan(model_code = bb_model, data = list(Y = 4), \n               chains = 4, iter = 5000*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 1:                0.029 seconds (Sampling)\nChain 1:                0.057 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 2:                0.03 seconds (Sampling)\nChain 2:                0.058 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 3:                0.029 seconds (Sampling)\nChain 3:                0.056 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 9e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 4:                0.029 seconds (Sampling)\nChain 4:                0.056 seconds (Total)\nChain 4: \n\nend_time &lt;- Sys.time()\nprint(round(end_time- start_time))\n\nTime difference of 38 secs\n\n\n\n\n\nbayesplot::mcmc_hist(bb_sim, pars = \"pi\")\n\n\n\n\n\n\n\nbayesplot::mcmc_dens(bb_sim, pars = \"pi\") + \n  stat_function(fun = dbeta, args = list(7, 8),\n                color = \"#E77500\", linewidth = 3) + \n  labs(title = \"MCMC: &lt;span style='color:#619CFF'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Beta-Binomial Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/06_approx_posterior/06_approx_posterior.html#example-gamma-poisson-1",
    "href": "posts/06_approx_posterior/06_approx_posterior.html#example-gamma-poisson-1",
    "title": "6: Approximating the Posterior",
    "section": "Example: Gamma-Poisson",
    "text": "Example: Gamma-Poisson\n\nScenario: Drug Law Violations\nLet us start with a vague Gamma prior, use a binomial model to get the likelihood of \\(\\sum y = 119\\) drug law violations over \\(n = 9\\) years, and then get a Gamma posterior.\n\\[\\begin{array}{rrcl}\n  \\text{prior: } & \\pi & \\sim & \\text{Gamma}(16, 0.8) \\\\\n  \\text{likelihood: } & Y|\\pi & \\sim & \\text{Pois}(119/9) \\\\\n  \\text{posterior: } & \\pi|Y & \\sim & \\text{Gamma}(135, 9.8) \\\\\n\\end{array}\\]\n\nDefine ModelSimulate PosteriorHistogramDensity\n\n\n\n# STEP 1: DEFINE the model\ngp_model &lt;- \"\n  data {\n    int&lt;lower = 0&gt; Y[9];\n  }\n  parameters {\n    real&lt;lower = 0&gt; lambda;\n  }\n  model {\n    Y ~ poisson(lambda);\n    lambda ~ gamma(16, 0.8);\n  }\n\"\n\n\n\n\nstart_time &lt;- Sys.time()\n\n# STEP 2: SIMULATE the posterior\nobs_counts &lt;- c(18, 14, 23, 22, 12, 22, 7, 0, 1)\ngp_sim &lt;- stan(model_code = gp_model, \n               data = list(Y = obs_counts), \n               chains = 4, iter = 5000*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 1:                0.032 seconds (Sampling)\nChain 1:                0.063 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 2:                0.028 seconds (Sampling)\nChain 2:                0.066 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.032 seconds (Warm-up)\nChain 3:                0.034 seconds (Sampling)\nChain 3:                0.066 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 4:                0.028 seconds (Sampling)\nChain 4:                0.062 seconds (Total)\nChain 4: \n\nend_time &lt;- Sys.time()\nprint(round(end_time- start_time))\n\nTime difference of 38 secs\n\n\n\n\n\nbayesplot::mcmc_hist(gp_sim, pars = \"lambda\")\n\n\n\n\n\n\n\nbayesplot::mcmc_dens(gp_sim, pars = \"lambda\") + \n  stat_function(fun = dgamma, args = list(135, 9.8),\n                color = \"#E77500\", linewidth = 3) + \n  labs(title = \"MCMC: &lt;span style='color:#619CFF'&gt;simulation&lt;/span&gt; versus &lt;span style='color:#E77500'&gt;theoretical&lt;/span&gt;\",\n         subtitle = \"Gamma-Poisson Example\",\n         caption = \"SML 320\") +\n  theme_minimal() +\n  theme(plot.title = element_markdown())"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html",
    "href": "posts/07_mcmc_1/07_mcmc_1.html",
    "title": "7: MCMC",
    "section": "",
    "text": "library(\"bayesplot\")\nlibrary(\"ggtext\")\nlibrary(\"rstan\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# tips_df &lt;- readr::read_csv(\"tips.csv\")"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#metropolis-hastings",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#metropolis-hastings",
    "title": "7: MCMC",
    "section": "Metropolis-Hastings",
    "text": "Metropolis-Hastings\nUsing the analogy from the Bayes Rules! textbook, “As tour manager, you can automate the tour route using the Metropolis-Hastings algorithm. This algorithm iterates through a two-step process. Assuming the Markov chain is at location \\(\\mu(i)=\\mu\\) at iteration or “tour stop” \\(i\\), the next tour stop \\(\\mu(i+1)\\) is selected as follows:\n\nStep 1: Propose a random location, \\(\\mu^{′}\\), for the next tour stop.\nStep 2: Decide whether to\n\nto to the proposed location \\[\\mu(i+1)=\\mu^{′}\\]\nor to stay at the current location for another iteration \\[\\mu(i+1)=\\mu\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#monte-carlo",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#monte-carlo",
    "title": "7: MCMC",
    "section": "Monte Carlo",
    "text": "Monte Carlo\nIf we know the posterior distribution, this special case of the Metropolis-Hastings algorithm has a special name\n\n\n\n\n\n\nMonte Carlo algorithm\n\n\n\n\n\nTo construct an independent Monte Carlo sample directly from posterior pdf \\(f(\\mu|y)\\), \\[\\{\\mu^{(1)},\\mu^{(2)},...,\\mu^{(N)}\\}\\]\nselect each tour stop \\(\\mu^{(i)}=\\mu\\)\nas follows:\n\nStep 1: Propose a location. Draw a location μ from the posterior model with pdf \\(f(\\mu|y)\\)\nStep 2: Go there."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#generalizing",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#generalizing",
    "title": "7: MCMC",
    "section": "Generalizing",
    "text": "Generalizing\n\nwe only need MCMC to approximate a Bayesian posterior when that posterior is too complicated to specify\nif a posterior is too complicated to specify, it’s typically too complicated to directly sample or draw from as we did in our Monte Carlo tour above\nMetropolis-Hastings relies on the fact that, even if we don’t know the posterior model, we do know that the posterior pdf is proportional to the product of the known prior pdf and likelihood function\n\n\\[f(\\mu|y) \\propto f(\\mu) \\cdot L(\\mu|y)\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#nicholas-metropolis",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#nicholas-metropolis",
    "title": "7: MCMC",
    "section": "Nicholas Metropolis",
    "text": "Nicholas Metropolis\n\n\n\nPhD: University of Chicago\nRecruited by Oppenheimer\nTeam included von Neumann and Ulam\n\n\n\n\n\nNicholas Metropolis\n\n\nImage Source: Nuclear Museum"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#wilfred-hastings",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#wilfred-hastings",
    "title": "7: MCMC",
    "section": "Wilfred Hastings",
    "text": "Wilfred Hastings\n\n\nPhD: University of Toronto\nBell Labs (New Jersey)\ngeneralized Metropolis algorithm and MCMC\n\n\n\n\n\n\nWilfred Hastings\n\n\nImage Source: McCall Gardens"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#rosenbluths",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#rosenbluths",
    "title": "7: MCMC",
    "section": "Rosenbluths",
    "text": "Rosenbluths\n\n\nArianna Rosenbluth\n\nPhD: Harvard University\nqualified for Olympics (fencing)\nwrote first MCMC algorithm\n\n\n\nMarshall Rosenbluth\n\nPhD: University of Chicago\nWWII veteran (navy)\n\n\n\n\n\n\n\nMarshall and Arianna Rosenbluth\n\n\nImage Source: Los Alamos National Laboratories"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#uniform-proposal-model",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#uniform-proposal-model",
    "title": "7: MCMC",
    "section": "Uniform proposal model",
    "text": "Uniform proposal model\nLet \\(\\mu^{(i)} = \\mu\\) denote the current tour location with \\(w\\) being a half-width distance:\n\nrandom draw: \\(\\mu^{'}|\\mu \\sim \\text{Unif}(\\mu - w, \\mu + w)\\)\npdf: \\(q(\\mu^{'}|\\mu) = \\displaystyle\\frac{1}{2w} \\text{ for } \\mu^{'} \\in [(\\mu - w, \\mu + w]\\)"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#rejected-ideas",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#rejected-ideas",
    "title": "7: MCMC",
    "section": "Rejected Ideas",
    "text": "Rejected Ideas\n\nNever accept the proposed location.\nAlways accept the proposed location.\nOnly accept the proposed location if its (unnormalized) posterior plausibility is greater than that of the current location."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#main-idea",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#main-idea",
    "title": "7: MCMC",
    "section": "Main Idea",
    "text": "Main Idea\n\nStep 1: Propose a location, \\(\\mu^{'}\\), for the next tour stop by taking a draw from a proposal model.\nStep 2: Decide whether to go to the proposed location (\\(μ^{(i+1)}=\\mu^{'}\\)) or to stay at the current location for another iteration (\\(μ^{(i+1)}=\\mu\\)) as follows.\n\nIf the (unnormalized) posterior plausibility of the proposed location \\(\\mu^{'}\\) is greater than that of the current location \\(\\mu\\), \\[f(\\mu^{'})L(\\mu^{'}|y)&gt;f(\\mu)L(\\mu|y)\\] definitely go there.\nOtherwise, maybe go there."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#definition-1",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#definition-1",
    "title": "7: MCMC",
    "section": "Definition",
    "text": "Definition\n\n\n\n\n\n\nMetropolis-Hastings algorithm\n\n\n\n\n\nConditioned on data \\(y\\), let parameter \\(\\mu\\) have posterior pdf \\[f(\\mu|y)\\propto f(\\mu) \\cdot L(\\mu|y)\\] A Metropolis-Hastings Markov chain for \\(f(\\mu|y)\\), \\(\\{\\mu^{(1)},\\mu^{(2)},...,\\mu^{(N)}\\}\\), evolves as follows. Let \\(\\mu^{(i)}=\\mu\\) be the chain’s location at iteration \\(i\\in\\{1,2,...,N−1\\}\\) and identify the next location \\(\\mu^{(i+1)}\\) through a two-step process:\n\nStep 1: Propose a new location. Conditioned on the current location \\(\\mu\\), draw a location \\(\\mu^{′}\\) from a proposal model with pdf \\(q(\\mu^{′}|\\mu)\\).\nStep 2: Decide whether or not to go there.\n\nCalculate the acceptance probability (i.e., the probability of accepting the proposal \\(\\mu^{′}\\)): \\[\\alpha = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y)}{f(\\mu) \\cdot L(\\mu|y)} \\cdot \\displaystyle\\frac{q(\\mu^{′}|\\mu)}{q(\\mu|\\mu^{′})} \\right\\}\\]\nFiguratively, flip a weighted coin. If it’s Heads, with probability \\(\\alpha\\), go to the proposed location \\(\\mu^{′}\\). If it’s Tails, with probability \\(1−\\alpha\\), stay at \\(\\mu\\): \\[\\mu^{(i+1)} = \\begin{cases}\n\\mu^{'} & \\text{with probability } \\alpha \\\\\n\\mu & \\text{with probability } 1-\\alpha \\\\\n  \\end{cases}\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#symmetric-proposal",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#symmetric-proposal",
    "title": "7: MCMC",
    "section": "Symmetric Proposal",
    "text": "Symmetric Proposal\nWhat happens if we have a symmetric proposal model? For instance,\n\\[\\mu^{'}|\\mu \\sim \\text{Unif}(\\mu - w, \\mu + w)\\]\nleads to the probability density function\n\\[q(\\mu^{′}|\\mu) = q(\\mu|\\mu^{′}) = \\begin{cases}\n  \\frac{1}{2w}, & |\\mu - \\mu^{'}| &lt; w \\\\\n  0, & \\text{otherwise} \\\\\n\\end{cases}\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#metropolis-algorithm",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#metropolis-algorithm",
    "title": "7: MCMC",
    "section": "Metropolis Algorithm",
    "text": "Metropolis Algorithm\n\n\n\n\n\n\nMetropolis Algorithm\n\n\n\n\n\nThe Metropolis algorithm is a special case of the Metropolis-Hastings in which the proposal model is symmetric. That is, the chance of proposing a move to \\(\\mu^{′}\\) from \\(\\mu\\) is equal to that of proposing a move to \\(\\mu\\) from \\(\\mu^{′}\\): \\[q(\\mu^{′}|\\mu) = q(\\mu|\\mu^{′})\\]\nThe acceptance probability simplifies to \\[\\alpha = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y)}{f(\\mu) \\cdot L(\\mu|y)} \\right\\}\\]"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#bayesian-ratio",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#bayesian-ratio",
    "title": "7: MCMC",
    "section": "Bayesian Ratio",
    "text": "Bayesian Ratio\nBy dividing both the numerator and denominator by \\(f(y)\\)\n\\[\\alpha = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y) / f(y)}{f(\\mu)  \\cdot L(\\mu|y) / f(y)} \\right\\} = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{'}|y)}{f(\\mu|y)} \\right\\}\\]\nThis rewrite emphasizes that, though we can’t calculate the posterior pdfs of \\(\\mu^{′}\\) and \\(\\mu\\), \\(f(\\mu^{'}|y)\\) and \\(f(\\mu|y)\\), their ratio is equivalent to that of the unnormalized posterior pdfs (which we can calculate)"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#tour-decisions",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#tour-decisions",
    "title": "7: MCMC",
    "section": "Tour Decisions",
    "text": "Tour Decisions\nThus, the probability of accepting a move from a current location \\(\\mu\\) to a proposed location \\(\\mu^{′}\\) comes down to a comparison of their posterior plausibility: \\(f(\\mu^{'}|y)\\) versus \\(f(\\mu|y)\\). There are two possible scenarios here:\n\nScenario 1: \\(f(\\mu^{'}|y) \\geq f(\\mu|y)\\). When the posterior plausibility of \\(\\mu^{′}\\) is at least as great as that of \\(\\mu\\), \\(\\alpha=1\\). Thus, we’ll definitely move there.\nScenario 2: \\(f(\\mu^{'}|y) &lt; f(\\mu|y)\\). If the posterior plausibility of \\(\\mu^{′}\\) is less than that of \\(\\mu\\), then\n\n\\[α=\\displaystyle\\frac{f(\\mu^{′}|y)}{f(\\mu|y)}&lt;1\\]\nThus, we might move there.\n\n\n\n\n\n\nNear\n\n\n\n\n\nFurther, \\(\\alpha\\) approaches 1 as \\(f(\\mu^{'}|y)\\) nears \\(f(\\mu|y)\\). That is, the probability of accepting the proposal increases with the plausibility of \\(\\mu^{′}\\) relative to \\(\\mu\\)."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#detailed-account",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#detailed-account",
    "title": "7: MCMC",
    "section": "Detailed Account",
    "text": "Detailed Account\n\nstart location: \\(\\mu^{(0)} = 3\\)\nhalf-width: \\(w = 0.5\\)\nnormal-normal model:\n\nprior: \\(\\mu \\sim \\text{N}(3, 0.50^2)\\)\nlikelihood: \\(Y|\\mu \\sim \\text{N}(\\mu, 0.75^2)\\)\n\nobserved value: \\(y = 3.20\\)\n\n\nset.seed(20240220)\ncurrent  &lt;- 3\nproposal &lt;- runif(1, min = current - 1, max = current + 1)\nprint(paste0(\"The proposed value is: \", proposal))\n\n[1] \"The proposed value is: 2.8043764475733\"\n\nproposal_plausibility &lt;- dnorm(proposal, 3, 0.5) * dnorm(3.20, proposal, 0.75)\nprint(paste0(\"The proposed plausibility is: \", proposal_plausibility))\n\n[1] \"The proposed plausibility is: 0.342079515048734\"\n\ncurrent_plausibility &lt;- dnorm(current, 3, 0.5) * dnorm(3.20, current, 0.75)\nprint(paste0(\"The current plausibility is: \", current_plausibility))\n\n[1] \"The current plausibility is: 0.409588054724166\"\n\nalpha &lt;- min(1, proposal_plausibility / current_plausibility)\nprint(paste0(\"The acceptance probability is: \", alpha))\n\n[1] \"The acceptance probability is: 0.835179422600849\""
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#helper-function",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#helper-function",
    "title": "7: MCMC",
    "section": "Helper Function",
    "text": "Helper Function\n\none_mh_iteration &lt;- function(current, w, obs_value, tau, sigma){\n  # Step 1: propose next location in chain\n  proposal &lt;- runif(1, min = current - w, current + w)\n  \n  # Step 2: decide whether or not to go there\n  proposal_plausibility &lt;- dnorm(proposal, 3, tau) * dnorm(obs_value, proposal, sigma)\n  current_plausibility &lt;- dnorm(current, 3, tau) * dnorm(obs_value, current, sigma)\n  alpha &lt;- min(1, proposal_plausibility / current_plausibility)\n  next_stop &lt;- sample(c(proposal, current), \n                      size = 1, prob = c(alpha, 1 - alpha))\n  \n  # Return the results as a data frame\n  return(data.frame(proposal, alpha, next_stop))\n}\n\n\nset.seed(1)\none_mh_iteration(current = 3, w = 0.5, \n                 obs_value = 3.20, tau = 0.75, sigma = 0.50)\n\n  proposal     alpha next_stop\n1 2.765509 0.7071998  2.765509\n\n\n\nset.seed(4)\none_mh_iteration(current = 3, w = 0.5, \n                 obs_value = 3.20, tau = 0.75, sigma = 0.50)\n\n  proposal alpha next_stop\n1   3.0858     1    3.0858\n\n\n\nset.seed(5)\none_mh_iteration(current = 3, w = 0.5, \n                 obs_value = 3.20, tau = 0.75, sigma = 0.50)\n\n  proposal     alpha next_stop\n1 2.700214 0.6068602         3"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#for-loop",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#for-loop",
    "title": "7: MCMC",
    "section": "For Loop",
    "text": "For Loop\n\nmh_tour &lt;- function(N, current, w, obs_value, tau, sigma){\n  # N: chain length\n  # initialize vector\n  mu &lt;- rep(0, N)\n  \n  # simulate N Markov chain stops\n  for(i in 1:N){\n    # simulate one iteration\n    this_iteration &lt;- one_mh_iteration(current, w, obs_value, tau, sigma)\n    \n    # record next location\n    mu[i] &lt;- this_iteration$next_stop\n    \n    # update current location\n    current &lt;- this_iteration$next_stop\n  }\n  \n  # return the chain locations\n  return(data.frame(iteration = c(1:N), mu))\n}"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#go-on-tour",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#go-on-tour",
    "title": "7: MCMC",
    "section": "Go on Tour!",
    "text": "Go on Tour!\n\nour_mh_tour &lt;- mh_tour(N = 5000, current = 3, w = 1, \n                 obs_value = 3.20, tau = 0.50, sigma = 0.75)"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#traces",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#traces",
    "title": "7: MCMC",
    "section": "Traces",
    "text": "Traces\n\nOur Metropolis-Hastings Tour\nFrom our knowledge of the Normal-Normal model,\n\nbayesrules::summarize_normal_normal(\n  # from prior\n  mean = 3, sd = 0.50,\n  \n  # from observations\n  y_bar = 3.20, sigma = 0.75, n = 1\n) |&gt;\n  mutate_if(is.numeric, round, digits = 4)\n\n      model   mean   mode    var    sd\n1     prior 3.0000 3.0000 0.2500 0.500\n2 posterior 3.0615 3.0615 0.1731 0.416\n\n\nwe should have a \\(\\text{N}(3.0615, 0.4160^2)\\) posterior distribution\n\np1 &lt;- ggplot(our_mh_tour, aes(x = iteration, y = mu)) + \n  geom_line() +\n  labs(title = \"Our Metropolis-Hastings Tour\",\n       subtitle = \"Posterior: N(3.0615, 0.4160^2)\",\n       caption = \"SML 320\") +\n  theme_minimal()\n\np2 &lt;- ggplot(our_mh_tour, aes(x = mu)) + \n  geom_histogram(aes(y = after_stat(density)), \n                 binwidth = 0.1,\n                 color = \"black\", fill = \"gray50\") + \n  stat_function(fun = dnorm, args = list(3.0615, 0.4160), \n                color = \"#E77500\",\n                linewidth = 2) +\n  theme_minimal()\n\n# patchwork\np1 + p2"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#bad-examples",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#bad-examples",
    "title": "7: MCMC",
    "section": "Bad Examples",
    "text": "Bad Examples\n\n\n\nbad examples\n\n\nImage Source: [Bayes Rules!](https://www.bayesrulesbook.com/chapter-6#diagnostics"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#density-overlay",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#density-overlay",
    "title": "7: MCMC",
    "section": "Density Overlay",
    "text": "Density Overlay\n\nPlotsCode\n\n\n\n\n\n\n\n\n\n\np1 &lt;- mcmc_dens_overlay(bad_simulation, pars = \"pi\") + \n  labs(title = \"Density Overlay\",\n       subtitle = \"bad simulation\",\n       caption = \"SML 320\",\n       y = \"density\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np2 &lt;- mcmc_dens_overlay(good_simulation, pars = \"pi\") + \n  labs(title = \"Density Overlay\",\n       subtitle = \"good simulation\",\n       caption = \"SML 320\",\n       y = \"density\") +\n  theme_minimal()\n\n# patchwork\np1 + p2"
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#legacy",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#legacy",
    "title": "7: MCMC",
    "section": "Legacy",
    "text": "Legacy\nThe Metropolis-Hastings algorithm appeared in a top-ten list called ““the greatest influence on the development and practice of science and engineering in the 20th century”."
  },
  {
    "objectID": "posts/07_mcmc_1/07_mcmc_1.html#music-recommendation",
    "href": "posts/07_mcmc_1/07_mcmc_1.html#music-recommendation",
    "title": "7: MCMC",
    "section": "Music Recommendation",
    "text": "Music Recommendation\nShould I Stay or Should I Go by The Clash\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.2    forcats_1.0.0      stringr_1.5.0      dplyr_1.1.3       \n [5] purrr_1.0.2        readr_2.1.4        tidyr_1.3.0        tibble_3.2.1      \n [9] ggplot2_3.4.3      tidyverse_2.0.0    patchwork_1.1.2    rstan_2.32.5      \n[13] StanHeaders_2.32.5 ggtext_0.1.2       bayesplot_1.10.0  \n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3        inline_0.3.19        rlang_1.1.1         \n  [4] magrittr_2.0.3       snakecase_0.11.0     matrixStats_1.0.0   \n  [7] e1071_1.7-13         compiler_4.3.0       loo_2.6.0           \n [10] callr_3.7.3          vctrs_0.6.3          reshape2_1.4.4      \n [13] pkgconfig_2.0.3      crayon_1.5.2         fastmap_1.1.1       \n [16] backports_1.4.1      ellipsis_0.3.2       labeling_0.4.3      \n [19] utf8_1.2.3           threejs_0.3.3        promises_1.2.1      \n [22] rmarkdown_2.24       markdown_1.8         tzdb_0.4.0          \n [25] nloptr_2.0.3         ps_1.7.5             xfun_0.40           \n [28] jsonlite_1.8.7       later_1.3.1          parallel_4.3.0      \n [31] prettyunits_1.1.1    R6_2.5.1             dygraphs_1.1.1.6    \n [34] stringi_1.7.12       boot_1.3-28.1        Rcpp_1.0.11         \n [37] knitr_1.43           zoo_1.8-12           base64enc_0.1-3     \n [40] splines_4.3.0        Matrix_1.5-4         igraph_1.4.3        \n [43] httpuv_1.6.11        timechange_0.2.0     tidyselect_1.2.0    \n [46] rstudioapi_0.15.0    abind_1.4-5          yaml_2.3.7          \n [49] miniUI_0.1.1.1       codetools_0.2-19     curl_5.0.2          \n [52] processx_3.8.1       pkgbuild_1.4.0       lattice_0.21-8      \n [55] plyr_1.8.8           shiny_1.7.5          withr_2.5.2         \n [58] groupdata2_2.0.2     posterior_1.4.1      evaluate_0.21       \n [61] survival_3.5-5       proxy_0.4-27         RcppParallel_5.1.7  \n [64] xts_0.13.1           xml2_1.3.5           pillar_1.9.0        \n [67] tensorA_0.36.2       DT_0.28              checkmate_2.2.0     \n [70] stats4_4.3.0         shinyjs_2.1.0        distributional_0.3.2\n [73] generics_0.1.3       hms_1.1.3            rstantools_2.3.1    \n [76] munsell_0.5.0        commonmark_1.9.0     scales_1.2.1        \n [79] minqa_1.2.5          gtools_3.9.4         xtable_1.8-4        \n [82] class_7.3-21         glue_1.6.2           janitor_2.2.0       \n [85] tools_4.3.0          shinystan_2.6.0      lme4_1.1-33         \n [88] colourpicker_1.2.0   bayesrules_0.0.2     grid_4.3.0          \n [91] crosstalk_1.2.0      QuickJSR_1.1.3       colorspace_2.1-0    \n [94] nlme_3.1-162         cli_3.6.1            fansi_1.0.4         \n [97] V8_4.3.0             gtable_0.3.4         digest_0.6.33       \n[100] htmlwidgets_1.6.2    farver_2.1.1         htmltools_0.5.6     \n[103] lifecycle_1.0.4      mime_0.12            rstanarm_2.21.4     \n[106] MASS_7.3-58.4        shinythemes_1.2.0    gridtext_0.1.5"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html",
    "href": "posts/08_mcmc_2/08_mcmc_2.html",
    "title": "8: MCMC",
    "section": "",
    "text": "library(\"bayesplot\")\nlibrary(\"ggtext\")\nlibrary(\"rstan\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#autocorrelation",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#autocorrelation",
    "title": "8: MCMC",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nMarkov PropertyPlotsCode\n\n\nIf we are assuming the Markov property\n\\[f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(i)}, y \\right) = f\\left( \\theta^{(i+1)} \\bigg| \\theta^{(i)}, y \\right)\\]\nthen autocorrelation measurements should only be “large” with a lag of one.\n\n\n\n\n\n\n\n\n\n\np1 &lt;- bayesplot::mcmc_acf(good_simulation, pars = \"pi\") +\n  labs(title = \"Good Simulation\",\n       subtitle = \"10000 iterations\",\n       caption = \"SML 320\")\n\np2 &lt;- bayesplot::mcmc_acf(bad_simulation, pars = \"pi\") +\n  labs(title = \"Bad Simulation\",\n       subtitle = \"100 iterations\",\n       caption = \"SML 320\")\n\np3 &lt;- bayesplot::mcmc_acf(thin_simulation, pars = \"pi\") +\n  labs(title = \"Thinned Simulation\",\n       subtitle = \"Retained every 10 values\",\n       caption = \"SML 320\")\n\n# patchwork\np1 + p2 + p3"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#split-r-metric",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#split-r-metric",
    "title": "8: MCMC",
    "section": "Split R Metric",
    "text": "Split R Metric\n\nAnalysis of VarianceDefinitionGuidanceExamples\n\n\n\n\n\nBayes Rules! Figure 6.19\n\n\n\n\n\n\n\n\n\n\nR-Hat\n\n\n\n\n\nConsider a Markov chain simulation of parameter \\(\\theta\\) which utilizes four parallel chains. Let \\(\\text{Var}_{\\text{combined}}\\) denote the variability in \\(\\theta\\) across all four chains combined and \\(\\text{Var}_{\\text{within}}\\) denote the typical variability within any individual chain. The R-hat metric calculates the ratio between these two sources of variability:\n\\[\\text{R-hat} \\approx \\sqrt{\\displaystyle\\frac{\\text{Var}_{\\text{combined}}}{\\text{Var}_{\\text{within}}}}\\]\n\n\n\n\n\n\nIdeally, \\(\\text{R-hat}\\approx 1\\), reflecting stability across the parallel chains.\nIn contrast, \\(\\text{R-hat} &gt; 1\\) indicates instability, with the variability in the combined chains exceeding that within the chains.\nThough no golden rule exists, an R-hat ratio greater than 1.05 raises some red flags about the stability of the simulation.\n\n\n\n\nbayesplot::rhat(good_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 1.0003\n\n\n\nbayesplot::rhat(bad_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 1.052\n\n\n\nbayesplot::rhat(thin_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 0.9983"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#effective-sample-size",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#effective-sample-size",
    "title": "8: MCMC",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\nMotivationDescriptionGuidanceExamples\n\n\nLoosely speaking, how many independent sample values would it take to produce an equivalently accurate posterior approximation?\n\n\n\n\n\n\n\n\nEffective Sample Size Ratio\n\n\n\n\n\nLet \\(N\\) denote the actual sample size or length of a dependent Markov chain. The effective sample size of this chain, \\(N_{\\text{eff}\\), quantifies the number of independent samples it would take to produce an equivalently accurate posterior approximation. The greater the \\(N_{\\text{eff}\\) the better, yet it’s typically true that the accuracy of a Markov chain approximation is only as good as that of a smaller independent sample. That is, it’s typically true that \\(N_{\\text{eff} &lt; N\\), thus the effective sample size ratio is less than 1: \\[\\displaystyle\\frac{N_{\\text{eff}}{N} &lt; 1 \\quad\\text{(usually)}\\]\n\n\n\n\n\nThere’s no magic rule for interpreting this ratio, and it should be utilized alongside other diagnostics such as the trace plot. That said, we might be suspicious of a Markov chain for which the effective sample size ratio is less than 0.1, i.e., the effective sample size \\(N_{\\text{eff}\\) is less than 10% of the actual sample size N.\n\n\n\nbayesplot::neff_ratio(good_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 0.3873\n\n\n\nbayesplot::neff_ratio(bad_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 0.3434\n\n\n\nbayesplot::neff_ratio(thin_simulation, pars = \"pi\") |&gt; round(digits = 4)\n\n[1] 1.0543"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#metropolis-algorithm",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#metropolis-algorithm",
    "title": "8: MCMC",
    "section": "Metropolis Algorithm",
    "text": "Metropolis Algorithm\nIf we have a symmetric proposal model, the probability of accepting a move from a current location \\(\\mu\\) to a proposed location \\(\\mu^{′}\\) comes down to a comparison of their posterior plausibility: \\(f(\\mu^{'}|y)\\) versus \\(f(\\mu|y)\\). There are two possible scenarios here:\n\nScenario 1: \\(f(\\mu^{'}|y) \\geq f(\\mu|y)\\). When the posterior plausibility of \\(\\mu^{′}\\) is at least as great as that of \\(\\mu\\), \\(\\alpha=1\\). Thus, we’ll definitely move there.\nScenario 2: \\(f(\\mu^{'}|y) &lt; f(\\mu|y)\\). If the posterior plausibility of \\(\\mu^{′}\\) is less than that of \\(\\mu\\), then\n\n\\[α=\\displaystyle\\frac{f(\\mu^{′}|y)}{f(\\mu|y)}&lt;1\\]\nThus, we might move there."
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#metropolis-hastings-algorithm",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#metropolis-hastings-algorithm",
    "title": "8: MCMC",
    "section": "Metropolis-Hastings Algorithm",
    "text": "Metropolis-Hastings Algorithm\nRemoving the assumption of a symmetric proposal model, we have to convey \\(q(\\mu^{'}|\\mu)\\), the probability density function of the proposal model.\nConditioned on data \\(y\\), let parameter \\(\\mu\\) have posterior pdf \\[f(\\mu|y)\\propto f(\\mu) \\cdot L(\\mu|y)\\] A Metropolis-Hastings Markov chain for \\(f(\\mu|y)\\), \\(\\{\\mu^{(1)},\\mu^{(2)},...,\\mu^{(N)}\\}\\), evolves as follows. Let \\(\\mu^{(i)}=\\mu\\) be the chain’s location at iteration \\(i\\in\\{1,2,...,N−1\\}\\) and identify the next location \\(\\mu^{(i+1)}\\) through a two-step process:\n\nStep 1: Propose a new location. Conditioned on the current location \\(\\mu\\), draw a location \\(\\mu^{′}\\) from a proposal model with pdf \\(q(\\mu^{′}|\\mu)\\).\nStep 2: Decide whether or not to go there.\n\nCalculate the acceptance probability (i.e., the probability of accepting the proposal \\(\\mu^{′}\\)): \\[\\alpha = \\text{min}\\left\\{1, \\displaystyle\\frac{f(\\mu^{′}) \\cdot L(\\mu^{′}|y)}{f(\\mu) \\cdot L(\\mu|y)} \\cdot \\displaystyle\\frac{q(\\mu^{′}|\\mu)}{q(\\mu|\\mu^{′})} \\right\\}\\]\nFiguratively, flip a weighted coin. If it’s Heads, with probability \\(\\alpha\\), go to the proposed location \\(\\mu^{′}\\). If it’s Tails, with probability \\(1−\\alpha\\), stay at \\(\\mu\\): \\[\\mu^{(i+1)} = \\begin{cases}\n\\mu^{'} & \\text{with probability } \\alpha \\\\\n\\mu & \\text{with probability } 1-\\alpha \\\\\n  \\end{cases}\\]"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#motivation-1",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#motivation-1",
    "title": "8: MCMC",
    "section": "Motivation",
    "text": "Motivation\nWhy generalize to a non-symmetric proposal model?\n\nflexibility to estimate a variety of parameters, such as standard deviations (or other nonnegative values)\nleads to more clever searches"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#gibbs-sampling",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#gibbs-sampling",
    "title": "8: MCMC",
    "section": "Gibbs Sampling",
    "text": "Gibbs Sampling\n\nadaptive algorithm (especially with conjugate pairs)\ndescribed in 1984 by Stuart Geman and Donald Geman\nnamed after Josiah Willard Gibbs (for Gibbs’ work in statistical physics)\ngood for conditional distributions and marginal distributions"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#interlude-mountains-of-laos",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#interlude-mountains-of-laos",
    "title": "8: MCMC",
    "section": "Interlude: Mountains of Laos",
    "text": "Interlude: Mountains of Laos\n\n\n\nVang Pao"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#hamiltonian-monte-carlo",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#hamiltonian-monte-carlo",
    "title": "8: MCMC",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\nToward simulating a posterior distribution, Hamiltonian Monte Carlo (HMC) uses the topology by seeking out the gradient of maximum ascent\n\\[\\displaystyle\\text{max}_{\\vec{h}} \\lim_{|h| \\to 0} \\displaystyle\\frac{f(\\vec{x} + \\vec{h}) - f(\\vec{x})}{|h|}\\]\n“Path of least resistance”"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#app",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#app",
    "title": "8: MCMC",
    "section": "App",
    "text": "App\nTry out some MCMC simulations!\n\nlink: MCMC Demo by Chi Feng\nalgorithms:\n\nRandomWalkMH\nGibbsSampling\nHamiltonianMC\nEfficientNUTS\n\nTarget distributions:\n\nstandard\nbanana\ndonut\nmultimodal"
  },
  {
    "objectID": "posts/08_mcmc_2/08_mcmc_2.html#tuning",
    "href": "posts/08_mcmc_2/08_mcmc_2.html#tuning",
    "title": "8: MCMC",
    "section": "Tuning",
    "text": "Tuning\n\ngood_simulation &lt;- stan(model_code = bb_model, data = list(Y = 4), \n                        chains = 4, iter = 5000*2, seed = 84735)\n\nWhere did the simulation parameters, such as half-width and step-size, go?"
  }
]